[
  {
    "objectID": "syllabus.html#class-description",
    "href": "syllabus.html#class-description",
    "title": "LIB 201 - Intro to Digital Research",
    "section": "Class Description:",
    "text": "Class Description:\nFor this class, we will be using the corpus of letters in the Smith Papers to interrogate various conversations about race, gender, culture, economics, politics, and interpersonal relationships in rural, 20th century Mississippi. Using various digital humanities methods and tools, we will research the impact of these topics in the Smith Papers in a broader context, and we will display our findings in an online space that is visible to the public. In addition to sharing the findings of our research, we will make our research process visible, sharing raw data files, documenting the steps of our process, and detailing our collaborations.\nThis class will make visible the “invisible labor” of digital scholarship, will teach you persistence and flexibility with technology, will give you practice collaborating on several mini hands-on projects, and it will give you a cool project and some technical skills to put on your résumé."
  },
  {
    "objectID": "syllabus.html#course-objectives",
    "href": "syllabus.html#course-objectives",
    "title": "LIB 201 - Intro to Digital Research",
    "section": "Course Objectives:",
    "text": "Course Objectives:\n\nDefine, digitize, assemble, and create metadata for a circumscribed dataset\nDesign several small research projects that experiment with various digital tools and methods, creating a strategy for the project and documenting progress toward defined goals\nDemystify underpinnings of digital research projects, such as algorithms, formulas, and metadata\nBecome familiar with various ethical, sociocultural, and technological issues associated with digital research and publishing.\nSynthesize the process of a digital project into a reflective paper that pinpoints the goals of the project, the challenges faced, and strategies used to complete those goals."
  },
  {
    "objectID": "syllabus.html#grades",
    "href": "syllabus.html#grades",
    "title": "LIB 201 - Intro to Digital Research",
    "section": "Grades",
    "text": "Grades\n\n\n\nAssignments\n% of Grade\n\n\n\n\nWeekly Writings, check-ins, and discussion posts\n30%\n\n\nDigital Prepwork (metadata, transcriptions)\n30%\n\n\nMini-projects\n30%\n\n\nPortfolio/Reflection Essay\n10%"
  },
  {
    "objectID": "syllabus.html#course-requirements",
    "href": "syllabus.html#course-requirements",
    "title": "LIB 201 - Intro to Digital Research",
    "section": "Course Requirements",
    "text": "Course Requirements\n\nAccess to a laptop or desktop computer (either a personal laptop, desktop, or use of one of the library lab computers. Tablets and mobile devices are insufficient for this level of computing).\nA notebook/writing utensil\nSoftware/applications: (all are free and online)\n\nGoogle Drive (use your myapps email)\nMicrosoft Teams (use your app in Office 365, via WConnect)\nAsana (This is a free, browser application that you’ll receive an invite to)\nTranskribus (Installation instructions found here)\nVisual Studio Code (Installation instructions found here)\nPalladio (No installation necessary; browser application)\nOther applications TBA"
  },
  {
    "objectID": "syllabus.html#course-expectations",
    "href": "syllabus.html#course-expectations",
    "title": "LIB 201 - Intro to Digital Research",
    "section": "Course expectations:",
    "text": "Course expectations:\n\nReadings/Lessons: it’s important to keep up with course reading and video material, as you will be discussing them with your classmates and in your reading responses each week. They will provide a valuable theoretical and practical framework as you begin to work with digital studies tools.\nCreativity: Even though a good bit of the work assigned is technical in nature, there’s a lot of room for curiosity and creativity in digital studies, and there is no one “right” answer. This course is a place to explore connections between content and technology.\nFailure: You will be working with content and technology that is unfamiliar to you, and at times you may struggle with analysis tools or your research material. Often the most valuable learning happens during this time! I will score your work in this course primarily on process rather than on the final product. Showing us something that doesn’t work quite like you want/expect, and explaining your steps and what your goal is, indicates a level of engagement and curiosity we are all striving towards.\nRespect: You’ll be interacting with your classmates in person and in the online environment, on the discussion board in posts and replies, and in the zoom classroom. The expectation is that interactions will be respectful, kind, and constructive at all times.\nCommunication: I expect to hear from you if something is going well, or less than well, at any point in the course. You can expect me to provide feedback on submitted work in a timely manner (within two weeks of submission). I am available to talk via Zoom/phone throughout the week, including weekends outside of class, so feel free to make an appointment to chat about course content, processes, concerns etc.\nPrecarity: Any student facing housing, food, or health challenges which they believe will affect their performance in this course is urged to contact me or the Student Emergency Fund for support and accommodation. One of the best strategies you can have is to tackle issues before they become a crisis: it’s OK to ask for assistance!"
  },
  {
    "objectID": "syllabus.html#university-policies",
    "href": "syllabus.html#university-policies",
    "title": "LIB 201 - Intro to Digital Research",
    "section": "University Policies",
    "text": "University Policies\n\nAttendance: Unless otherwise noted in the syllabus schedule, attendance is required. With the exception of readings and discussion posts, the majority of the work for the course will be done in class, and in a group setting, so it is important to be present and alert for class. That said, absences are bound to happen, so you may accrue 3 absences (excused or unexcused) without penalty. Excused absences, as listed in the undergraduate bulletin, will require documentation, and students are responsible for making up missed work as a result of any absence. Students missing more than 50% of meetings will not receive credit.\nTitle IX: Mississippi University for Women recognizes the inherent dignity of all individuals and promotes respect for all people. The University is committed to creating an educational and learning environment that is free from discrimination based on sex, including sexual violence (assault, domestic violence, dating violence and gender-based stalking). To learn more about the University’s policy on sexual misconduct, how to make a report, or confidential resources, go to www.muw.edu/titleix. The Title IX Coordinator is located in Cochran Hall, Room 405, and may be contacted by phone at 662-241-6083 or email at titleix@muw.edu.\nAmericans with Disabilities Act: The University is committed to providing equitable access to learning for all students. The Student Success Center is the campus office that collaborates with students who have disabilities (e.g. physical, sensory, chronic health, learning, attentional, mental health) and arranges for reasonable accommodations to be implemented. It is the responsibility of students requesting accommodations to make an appointment with the Student Support Specialist to review specific needs, participate in the development of an Accommodation Plan by providing appropriate documentation, and discuss with the instructor how the Accommodation Plan will be applied in the course. Accommodations are not retroactive and a new Accommodation Plan must be reviewed, signed and presented to instructors each semester. The Student Support Specialist is located in Reneau Hall, Room 101(B), and may be contacted by phone at 662.329.7138 or email at ada@muw.edu.\nAcademic Integrity: All Mississippi University for Women students are expected to engage honestly and responsibly in their academic work and to refrain from any dishonest academic behavior. Violations of Academic Integrity include cheating, plagiarism, fabrication, falsification, or other actions that violate commonly accepted intellectual and ethical standards within academic and scientific communities. Violations of Academic Integrity can lead to severe penalties, from a zero grade for a test or assignment to expulsion from the University. Academic Integrity applies to work in progress as well as completed work. If you are uncertain about the proper procedure to follow when citing a source, working in a team with other students, or any other coursework situation please ask your instructor, a librarian, or a resource like the Writing Center for help. To learn more about the university’s standards of Academic Integrity, including what happens if your instructor believes that you have engaged in dishonest academic behavior and your rights to appeal such a charge, please consult section 7.2 of the Undergraduate Bulletin."
  },
  {
    "objectID": "syllabus.html#schedule-outline",
    "href": "syllabus.html#schedule-outline",
    "title": "LIB 201 - Intro to Digital Research",
    "section": "Schedule Outline",
    "text": "Schedule Outline\n**Note - dates will be updated throughout the semester as we progress toward certain goals. I will email updates to our schedule, but you can also check Canvas for assignment deadlines, etc.*\n\n\n\n\nModule\nDeliverable\n\n\n\n\n1: Evaluating Digital Scholarship\nQuiz\n\n\n2: Metadata\nMetadata for new letters\n\n\n3: Transcription\nTranscripts and tag metadata\n\n\n4: Network Analysis\nNetwork Analysis Graph\n\n\n5: Spatial Analysis\nMap or Timeline (TBD)\n\n\n6: Textual Analysis\nText analysis visualization\n\n\n7: Narrative Exhibit\nDigital Exhibit"
  },
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": "7-21-23",
    "section": "",
    "text": "7-21-23\n\nAdded all modules to module folder\nDeleted Readings folder, created folders for module images\nIs there a way to remotely import my folders from github of images for each module without downloading each individual file?\nrethinking how to turn self-guided modules into slide-friendly format\n\nchunk paragraphs into smaller slide-sized bits\nremove readings and chapter-heading sections; are the linked sections automatically a part of the html page?\nhow do I structure these sections?\naccessibility options?\n\nalt text\nhyperlinked text w/i the slides themselves\n\n\nCompleted metadata and transcribing modules. Still need to pull images to corresponding folders.\n\n\n\n7-24-23\nalt text, etc. https://quarto.org/docs/authoring/figures.html\ndiv with ::: [content here] ::: https://quarto.org/docs/presentation/revealjs/\nchange heading structure for metadata.md and transcribing.md add raw links to github images for modules\n\n\n7-25-23\n\nadded network analysis lesson, edited formatting for quarto\nupdated schedule\n\nwhy are the headings in the schedule listed with double brackets? [[xyz]]\n\nre-formatted headings in metadata.md and transcribing.md\n\nNeed to check if images will work since they are the link, but doesn’t include ?raw=true in the url\n\n\n7-27-23\n\nreworking mapping.md and text-analysis.md lessons\nupdating schedule.md\nhow to format/organize acknowledgements for lessons?\n\norganization of checkins - return to this (numbers or more headings)?\n\n\n7-28-23\n\nadded narrative.md and reformatted\nWith a quarto site, can students use f11 to see the raw markdown file? (see commented text in section 2)\nadding assignment handouts and rubrics to Dig-prep\n\nhow to fix broken image of metadata-tags in assignments folder?\nhow to link to heading section within module? as in tags defined section of transcribing.md network analysis\n\n\n\n\n7-31-23\n\nadding rest of assignment details\n\nTA.md\nmap-rubric.md\nnarrative-rubric.md Do I need to do additional stylizing for these pages? They are fairly simple!\n\n\n\n\n8-4-23\nMeeting with Toneisha Taylor\n\nHer Zotero library: https://www.zotero.org/drtonieshat/items/3GHUDKA6/item-list\n\n\n\n10-2-23\nquarto preview to run preivew…how to publish? - error message: assignments.md unable to resolve link target - home is still Ryan’s original page; can’t find where it’s mapping from in dir - schedule is right, but formatting looks off - need to clean up files from Ryan’s original dir.\n\n\n10-10-23\n\nfixed home page (index.md) and copied from syllabus. Do I need to have this info twice?\nsyllabus slideshow thing. Where to edit again?\nadded modules.md to index"
  },
  {
    "objectID": "Modules/text-analysis.html",
    "href": "Modules/text-analysis.html",
    "title": "Introduction to Text Analysis",
    "section": "",
    "text": "In addition to the important things to analyze in the letters, like places, people, and themes, there’s also the text. In this lesson, we will use our transcriptions, some textual cleaning methods, and the digital tools that have been developed to analyze the content of a text (or texts) to explore the Smith Papers en masse. In this lesson, we will:\n\nUnderstand the purpose of distant reading when analyzing a text or texts\nUnderstand the processes by which computers “read” text (specifically stemming, lemmatization, stop words, and tokenization)\nDefine text analysis methods, specifically n-gram analysis, topic modeling, and sentiment analysis\nSelect a data set from Smith Papers to clean, perform select text analysis methods on, and analyze.\n\n\n\n\n\n\n\n\n\nIf you have been in an English class before, you are familiar with the phrase “close reading.” If you have not, close reading refers to choosing part of a text - a word, a symbol, an image - to closely analyze the meaning of the whole work. In other words, why did the author make these word choices? What factors (e.g. historical, intellectual, subliminal, etc.) have gone into influencing the text that show up in this text over and over again? Close reading is one method used analyze a text. Distant reading, which is founded on some of the same principles as close reading, uses computer assistance to process large amounts of text, and it is the method we are going to use!\nDistant reading with computers is not a way to replace the levels of interpreting that humans are able to do. The work of the computer and the work of the text are often at odds, and while our brains will operate differently than a computer’s, we are going to explore using the 2 together. In fact, distant reading is possible without a computer–the work of the computer just saves time! Additionally, computationally distant reading loses its impact without the context and interpretation from human eyes. (Here’s a controversial opinion that even says it is a failure in the humanities! Gasp!) Countless distant reading projects have enhanced, adjusted, or even changed the ways we look at entire works of a specific collection, an author, or even genre.  Like close reading, though, distant reading will allow us to address a hypothesis about a text, and through analysis and interpretation, address that hypothesis with textual evidence.\nUnlike close reading, distant reading allows us to: * Link specific words to the context in which they were used throughout a text (as in a concordance or scholarly digital edition of an author’s entire works or just a single short story * Visualize a single text (or a collection of texts), allowing us to - interact with it graphically and dynamically - compare patterns up close and zoomed out - gain new insight - See patterns or changes in texts over time * Compare one author’s body of work to another, as in to identify distinctive vocabulary or stylistic patterns (i.e. This person used this word more than that person) * Structure a text with - hyperlinks - other metadata   * and more!\n\n\n\n\"The difference between the almost right word and the right word is the difference between lightening and a lightening bug\" -Mark Twain\nWords are so messy! You and I can understand them only after growing up and speaking a language for many years, and even then, we all have different ways of saying and understanding things! So how does a computer begin to “understand” words? With most tools, this is something that goes on automatically in the background, often using a pre-defined list of rules or dictionaries. Let’s take a peek behind the curtain!\n\n\nComputers can recognize characters in sequence. But what words say, and what they mean, aren’t always so clear! So in order to have a computer “read” something and pull information from it, you have to “tokenize” a text. Tokens can be separated by spaces, but they can also be separated by punctuation (hyphens, commas, parentheses), they could be differently capitalized, and they can have different meanings, even though they’re spelled the same way. Tokenizing splits words in a sentence into their individual units. \nHello, I am the all-knowing magician!\nTo a computer, this is just a series of characters. A computer might separate all from knowing when tokenized, simply because they are joined by a hyphen. But that changes the meaning of the sentence. By tokenizing, we can tell the computer what and how we want it to read.\n\nHello\n,\nI\nam\nthe\nall-knowing\nmagician\n!\n\nIf this sentence were a part of a larger corpus, we might also want to make everything the same case and take out punctuation so all linguistic units (aka words) are treated similarly.\n\n\n\nLemmas are root words, or common base words for different variations of a word. Lemmatizing a word is a Natural Language Processing (NLP) process, which groups words by their roots and parts of speech for analysis. Stemming a word removes part of words (plurals, suffixes, alternate endings, etc.) to collapse them into a common word. For example:\n\n\n\nWord\nLemma/Stem\n\n\n\n\ngood, better, best\ngood\n\n\nlibrary, librarian, libraries\nlibrar\n\n\nsounding (v.), sounds (n.)\nsound\n\n\n\nYou can look at all of the instances of the sentiment “good” by lemmatizing the words that mean “good” (i.e. better and best) at their root. And you can stem all of the variations of library by shortening them to “librar.”\n\n\n\nIn a large text, stop words are words like “the,” “some,” “of,” “these,” etc., that you can exclude from analysis because they are determined to be of less value than the other words. But be warned! Sometimes these little words mean a lot! See this blog post about “the creepiest word in Macbeth” to see why!\nThese methods - tokenizing, lemmatizing, stop words (there are more) - are still not perfect! Think of these methods as formulas that you are performing on a text en masse, and the English language often resists a formula. Think of all of the “exceptions to the rule” you have had to learn about. Furthermore, consider texts that are not in English. We use these formulas to help us read a corpus distantly, but that won’t replace our abilities to read closely.\nThese methods are automatic for many tools that we will use, but let’s not forget them! When we encounter errors or strange findings, it might be that we need to tweak these settings.\n\n\n\n\nComplete the 1.Text Analysis check-in before moving on to the next section.\n\n\n\n\nIn this class, we’re going to use specific text analysis methods that are available in the Digital Scholar Lab. (Hint: click Databases from the MUW Library Homepage, and find it under “D.”). You’ll need to link your Google MyApps account to this database to get started. These are the tools we’ll explore…\n\n\n\n\n\nScreenshot of a word cloud of William Wordsworth’s distinctive vocabulary words in his poems, from Ted Underwood\n\n\nN-grams are more commonly known as word clouds. You see words of different sizes, indicating the number of times that word is used throughout the text. The “N” in n-gram stands for number of words you are counting or highlighting in a corpus, so you can also do this with phrases.\nN-grams also show the patterns of those words as they’re used throughout the text over time (time can be literal - as in, throughout the years, as seen in the screenshot below, or it can be relative to the text itself, as in how the word is used throughout the course of the corpus.)\n\n\n\nScreenshot of Robots Reading Vogue, showing the uses of corset, girdle, bra, bustier, and hosiery over time\n\n\nN-grams can also be used to collocate words or phrases and to determine relationships between word pairs. The example below shows how words in Shakespeare’s plays with positive connotations are more often associated with male-gendered words.\n\n\n\nScreenshot of Heather Froehlich’s analysis of Shakespeare texts, where male-gendered words have more positive connotations (i.e. Man - young) than female-gendered words (i.e. Woman - wretched)\n\n\nYou’ll use an n-gram analysis to explore word pairs, frequencies, and their use over time.\n\n\n\nTopic Modeling is one of the more difficult text analysis methods because it is based on probability. Topic Modeling uses an algorithm Latent Direchlet Allocation to find probabilities of words that co-occur. When you run this algorithm on a body of text, it puts the words that are more likely to occur together in categories, or “bags of words.” The idea is that each category represents a topic of discussion in the text.\n\n\n\nScreenshot of Lincoln Sermons Topic keywords split into 10 columns labeled “topic 1,” “topic 2,” and so on\n\n\nJust like other text analysis methods, topic modeling requires some inference on behalf of the person running the algorithm - how are these groups of words related? How are they used in the text? Do they signify a distinct theme? For instance, all of the words in topic #1 in the image above are probabistically related to each other. At first glance, it looks like they are related to law and policy. It is not always clear what distinct topics are, but knowing about the contents of the text helps!\nIn another example, the column labels below were chosen to reflect the commonalities among the words below them.\n\n\n\nScreenshot of topic modeling results with columns labeled Arts, Budgets, Children, and Education, in colors corresponding to highlighted words within a paragraph\n\n\n\n\n\n\n\n\nScreenshot of sentiment analysis of tweets using #nationalsonsday\n\n\nSentiment analysis of tweets using #nationalsonsday from https://www.csc2.ncsu.edu/faculty/healey/tweet_viz/tweet_app/\n\n\n\nScreenshot of sentiment analysis of tweets using #nationaldaughtersday\n\n\nSentiment analysis of tweets using #nationaldaughtersday from https://www.csc2.ncsu.edu/faculty/healey/tweet_viz/tweet_app/.\nA sentiment analysis uses natural language processing to “score” words according to a positive or negative scale. Sentiment analyses are trained on a pre-determined set of words (such as the AFINN lexicon), and show (or attempt to show) the overall positive or negative emotions at any given moment within a corpus.\n\n\n\nScreenshot of the plot of James Joyce’s Portrait of the Artist as a Young Man in a sentiment analysis\n\n\nNote: Sentiment analyses have limits! Like toddlers, sentiment analyses do not detect sarcasm! :laughing: They also commonly do not account for the change in meanings of words over time (i.e. sick = ill v. sick = slang for cool).\nSee an example LIB 201 project by Beckie Fuller using content from 3 20th century letter collections to see if positive and negative emotions could be detected!\n\n\n\nNamed Entity Recognition (or NER) is similar to the previous methods, in that it uses a pre-determined list of words and algorithms to classify words into categories. These categories, or “named entities” helps readers identify key people, places, and things (aka “proper nouns,” though it’s not always limited to these) within a key text. For instance, if you are trying to identify locations in a text, you can use NER to help you identify named cities, counties, etc. Think about the tags you manually created in the letters. This is like an automated version of that!\nIn the Digital Scholar Lab, the following categories are used, based on a commonly used NER module:\n\n\n\nCategory\nDescription\n\n\n\n\nNumber\nNumerals that do not fall under another type\n\n\nDate\nAbsolute or relative dates or periods\n\n\nEvent\nNamed hurricanes, battles, wars, sports events, etc.\n\n\nPlace\nBuildings, airports, highways, bridges, etc.\n\n\nGeo-Political Entity\nCountries, cities, states\n\n\nLanguage\nAny named language\n\n\nLaw\nNamed documents made into laws\n\n\nGeography\nNon-GPE locations, mountain ranges, bodies of water\n\n\nMoney\nMonetary values, including unit\n\n\nCultural Group\nNationalities or religious or political groups\n\n\nPosition\n“first”, “second”, etc.\n\n\nOrganization\nCompanies, agencies, institutions, etc.\n\n\nPercentage\nPercentage, including “%”\n\n\nPerson\nPeople, including fictional\n\n\nProduct\nObjects, vehicles, foods, etc. (Not services)\n\n\nMeasurement\nMeasurements, as of weight or distance\n\n\nTime\nA period of time, smaller than a day or 24 hours.\n\n\nArtwork\nIncludes titles of books, songs, etc.\n\n\n\n\n\nComplete the 2.Text Analysis check-in here before moving on to the cleaning methods section.\n\n\n\n\n\nRemember our discussion of tokenization, lemmas, and stop words? In order to remove the inconsistencies and “noise” for a computer to analyze texts, you will judicially employ and tinker with some of these processes. Also remember the words of advice from Muñoz and Rawson in “Against Cleaning when employing these methods!\nBefore you even begin to clean individual texts, remember that you are also likely going to select groups of texts from within our collected transcriptions. Depending on what question you’ll pursue, you might limit your letters to just be from a certain decade, or from (or to!) a particular individual, etc. Try to aim for enough letters to render your reading a “distant” one! (In other words, 10 1-page letters probably won’t be enough! Aim higher!)\n \n\n\nIn some cases, a text analysis tool will treat words differently if they are in different cases. Remember, computers see strings of characters, and are as frustratingly literal as possible! This cleaning method eliminates the possibility of the computer treating the words Mother and mother differently. There could be a scenario when you’d like them to be different (i.e. maybe they say “Mother” as her name, but refer to her as “mother” to each other), so keep that in mind!\n\n\n\nThis method allows you to programmatically remove or replace characters, words, or lemmas. This is common for a text that is read through OCR recognition (when you scan a document instead of transcribe it, and a computer tries to recognize the characters). Often OCR will substitute characters or images with non-letter symbols, like Ê, §, ¶, ¥, or %. This is less likely to happen with transcriptions.\nHowever, removing punctuation might be helpful, especially since letter writers use punctuation differently! Furthermore, if you are analyzing the body of the letter, you will want to remove the part of the text that includes the envelope and letter head. Consider the different ways to do this (stop words, cutting the beginning of all the letters, removing line breaks, etc.) \n\n\n\nLike the ctrl+F function, replacing items across the board is another way to clean a text. (Remember doing this for the network analysis?) This could be helpful with things like abbreviations, nicknames, inconsistencies in spelling, etc.\n\n\n\nIn the previous section, we talked about removing words with “little value” (poor little words!), which you can alter according to the specifications of the text you’re dealing with. This also applies to words that are endemic to the colleciton. For instance, you might see the word “Mississippi” over and over again, and think, well duh! If your analysis reveals “Mississippi” as a significant word that you aren’t really interested in, add it to your stop words.\nIn different tools, you will separate these words differently (spaces, commas, or line breaks).\n\n\nUse this check-in to sketch out your main point of inquiry using text analysis. This will require more thinking through your text analysis assignment, so be sure to think through possible options, and you might even read through more letters before completing this. Complete the 3.Text Analysis check-in to complete this lesson."
  },
  {
    "objectID": "Modules/text-analysis.html#distant-reading",
    "href": "Modules/text-analysis.html#distant-reading",
    "title": "Introduction to Text Analysis",
    "section": "",
    "text": "If you have been in an English class before, you are familiar with the phrase “close reading.” If you have not, close reading refers to choosing part of a text - a word, a symbol, an image - to closely analyze the meaning of the whole work. In other words, why did the author make these word choices? What factors (e.g. historical, intellectual, subliminal, etc.) have gone into influencing the text that show up in this text over and over again? Close reading is one method used analyze a text. Distant reading, which is founded on some of the same principles as close reading, uses computer assistance to process large amounts of text, and it is the method we are going to use!\nDistant reading with computers is not a way to replace the levels of interpreting that humans are able to do. The work of the computer and the work of the text are often at odds, and while our brains will operate differently than a computer’s, we are going to explore using the 2 together. In fact, distant reading is possible without a computer–the work of the computer just saves time! Additionally, computationally distant reading loses its impact without the context and interpretation from human eyes. (Here’s a controversial opinion that even says it is a failure in the humanities! Gasp!) Countless distant reading projects have enhanced, adjusted, or even changed the ways we look at entire works of a specific collection, an author, or even genre.  Like close reading, though, distant reading will allow us to address a hypothesis about a text, and through analysis and interpretation, address that hypothesis with textual evidence.\nUnlike close reading, distant reading allows us to: * Link specific words to the context in which they were used throughout a text (as in a concordance or scholarly digital edition of an author’s entire works or just a single short story * Visualize a single text (or a collection of texts), allowing us to - interact with it graphically and dynamically - compare patterns up close and zoomed out - gain new insight - See patterns or changes in texts over time * Compare one author’s body of work to another, as in to identify distinctive vocabulary or stylistic patterns (i.e. This person used this word more than that person) * Structure a text with - hyperlinks - other metadata   * and more!\n\n\n\n\"The difference between the almost right word and the right word is the difference between lightening and a lightening bug\" -Mark Twain\nWords are so messy! You and I can understand them only after growing up and speaking a language for many years, and even then, we all have different ways of saying and understanding things! So how does a computer begin to “understand” words? With most tools, this is something that goes on automatically in the background, often using a pre-defined list of rules or dictionaries. Let’s take a peek behind the curtain!\n\n\nComputers can recognize characters in sequence. But what words say, and what they mean, aren’t always so clear! So in order to have a computer “read” something and pull information from it, you have to “tokenize” a text. Tokens can be separated by spaces, but they can also be separated by punctuation (hyphens, commas, parentheses), they could be differently capitalized, and they can have different meanings, even though they’re spelled the same way. Tokenizing splits words in a sentence into their individual units. \nHello, I am the all-knowing magician!\nTo a computer, this is just a series of characters. A computer might separate all from knowing when tokenized, simply because they are joined by a hyphen. But that changes the meaning of the sentence. By tokenizing, we can tell the computer what and how we want it to read.\n\nHello\n,\nI\nam\nthe\nall-knowing\nmagician\n!\n\nIf this sentence were a part of a larger corpus, we might also want to make everything the same case and take out punctuation so all linguistic units (aka words) are treated similarly.\n\n\n\nLemmas are root words, or common base words for different variations of a word. Lemmatizing a word is a Natural Language Processing (NLP) process, which groups words by their roots and parts of speech for analysis. Stemming a word removes part of words (plurals, suffixes, alternate endings, etc.) to collapse them into a common word. For example:\n\n\n\nWord\nLemma/Stem\n\n\n\n\ngood, better, best\ngood\n\n\nlibrary, librarian, libraries\nlibrar\n\n\nsounding (v.), sounds (n.)\nsound\n\n\n\nYou can look at all of the instances of the sentiment “good” by lemmatizing the words that mean “good” (i.e. better and best) at their root. And you can stem all of the variations of library by shortening them to “librar.”\n\n\n\nIn a large text, stop words are words like “the,” “some,” “of,” “these,” etc., that you can exclude from analysis because they are determined to be of less value than the other words. But be warned! Sometimes these little words mean a lot! See this blog post about “the creepiest word in Macbeth” to see why!\nThese methods - tokenizing, lemmatizing, stop words (there are more) - are still not perfect! Think of these methods as formulas that you are performing on a text en masse, and the English language often resists a formula. Think of all of the “exceptions to the rule” you have had to learn about. Furthermore, consider texts that are not in English. We use these formulas to help us read a corpus distantly, but that won’t replace our abilities to read closely.\nThese methods are automatic for many tools that we will use, but let’s not forget them! When we encounter errors or strange findings, it might be that we need to tweak these settings.\n\n\n\n\nComplete the 1.Text Analysis check-in before moving on to the next section."
  },
  {
    "objectID": "Modules/text-analysis.html#select-distant-reading-methods",
    "href": "Modules/text-analysis.html#select-distant-reading-methods",
    "title": "Introduction to Text Analysis",
    "section": "",
    "text": "In this class, we’re going to use specific text analysis methods that are available in the Digital Scholar Lab. (Hint: click Databases from the MUW Library Homepage, and find it under “D.”). You’ll need to link your Google MyApps account to this database to get started. These are the tools we’ll explore…\n\n\n\n\n\nScreenshot of a word cloud of William Wordsworth’s distinctive vocabulary words in his poems, from Ted Underwood\n\n\nN-grams are more commonly known as word clouds. You see words of different sizes, indicating the number of times that word is used throughout the text. The “N” in n-gram stands for number of words you are counting or highlighting in a corpus, so you can also do this with phrases.\nN-grams also show the patterns of those words as they’re used throughout the text over time (time can be literal - as in, throughout the years, as seen in the screenshot below, or it can be relative to the text itself, as in how the word is used throughout the course of the corpus.)\n\n\n\nScreenshot of Robots Reading Vogue, showing the uses of corset, girdle, bra, bustier, and hosiery over time\n\n\nN-grams can also be used to collocate words or phrases and to determine relationships between word pairs. The example below shows how words in Shakespeare’s plays with positive connotations are more often associated with male-gendered words.\n\n\n\nScreenshot of Heather Froehlich’s analysis of Shakespeare texts, where male-gendered words have more positive connotations (i.e. Man - young) than female-gendered words (i.e. Woman - wretched)\n\n\nYou’ll use an n-gram analysis to explore word pairs, frequencies, and their use over time.\n\n\n\nTopic Modeling is one of the more difficult text analysis methods because it is based on probability. Topic Modeling uses an algorithm Latent Direchlet Allocation to find probabilities of words that co-occur. When you run this algorithm on a body of text, it puts the words that are more likely to occur together in categories, or “bags of words.” The idea is that each category represents a topic of discussion in the text.\n\n\n\nScreenshot of Lincoln Sermons Topic keywords split into 10 columns labeled “topic 1,” “topic 2,” and so on\n\n\nJust like other text analysis methods, topic modeling requires some inference on behalf of the person running the algorithm - how are these groups of words related? How are they used in the text? Do they signify a distinct theme? For instance, all of the words in topic #1 in the image above are probabistically related to each other. At first glance, it looks like they are related to law and policy. It is not always clear what distinct topics are, but knowing about the contents of the text helps!\nIn another example, the column labels below were chosen to reflect the commonalities among the words below them.\n\n\n\nScreenshot of topic modeling results with columns labeled Arts, Budgets, Children, and Education, in colors corresponding to highlighted words within a paragraph\n\n\n\n\n\n\n\n\nScreenshot of sentiment analysis of tweets using #nationalsonsday\n\n\nSentiment analysis of tweets using #nationalsonsday from https://www.csc2.ncsu.edu/faculty/healey/tweet_viz/tweet_app/\n\n\n\nScreenshot of sentiment analysis of tweets using #nationaldaughtersday\n\n\nSentiment analysis of tweets using #nationaldaughtersday from https://www.csc2.ncsu.edu/faculty/healey/tweet_viz/tweet_app/.\nA sentiment analysis uses natural language processing to “score” words according to a positive or negative scale. Sentiment analyses are trained on a pre-determined set of words (such as the AFINN lexicon), and show (or attempt to show) the overall positive or negative emotions at any given moment within a corpus.\n\n\n\nScreenshot of the plot of James Joyce’s Portrait of the Artist as a Young Man in a sentiment analysis\n\n\nNote: Sentiment analyses have limits! Like toddlers, sentiment analyses do not detect sarcasm! :laughing: They also commonly do not account for the change in meanings of words over time (i.e. sick = ill v. sick = slang for cool).\nSee an example LIB 201 project by Beckie Fuller using content from 3 20th century letter collections to see if positive and negative emotions could be detected!\n\n\n\nNamed Entity Recognition (or NER) is similar to the previous methods, in that it uses a pre-determined list of words and algorithms to classify words into categories. These categories, or “named entities” helps readers identify key people, places, and things (aka “proper nouns,” though it’s not always limited to these) within a key text. For instance, if you are trying to identify locations in a text, you can use NER to help you identify named cities, counties, etc. Think about the tags you manually created in the letters. This is like an automated version of that!\nIn the Digital Scholar Lab, the following categories are used, based on a commonly used NER module:\n\n\n\nCategory\nDescription\n\n\n\n\nNumber\nNumerals that do not fall under another type\n\n\nDate\nAbsolute or relative dates or periods\n\n\nEvent\nNamed hurricanes, battles, wars, sports events, etc.\n\n\nPlace\nBuildings, airports, highways, bridges, etc.\n\n\nGeo-Political Entity\nCountries, cities, states\n\n\nLanguage\nAny named language\n\n\nLaw\nNamed documents made into laws\n\n\nGeography\nNon-GPE locations, mountain ranges, bodies of water\n\n\nMoney\nMonetary values, including unit\n\n\nCultural Group\nNationalities or religious or political groups\n\n\nPosition\n“first”, “second”, etc.\n\n\nOrganization\nCompanies, agencies, institutions, etc.\n\n\nPercentage\nPercentage, including “%”\n\n\nPerson\nPeople, including fictional\n\n\nProduct\nObjects, vehicles, foods, etc. (Not services)\n\n\nMeasurement\nMeasurements, as of weight or distance\n\n\nTime\nA period of time, smaller than a day or 24 hours.\n\n\nArtwork\nIncludes titles of books, songs, etc.\n\n\n\n\n\nComplete the 2.Text Analysis check-in here before moving on to the cleaning methods section."
  },
  {
    "objectID": "Modules/text-analysis.html#text-cleaning-methods",
    "href": "Modules/text-analysis.html#text-cleaning-methods",
    "title": "Introduction to Text Analysis",
    "section": "",
    "text": "Remember our discussion of tokenization, lemmas, and stop words? In order to remove the inconsistencies and “noise” for a computer to analyze texts, you will judicially employ and tinker with some of these processes. Also remember the words of advice from Muñoz and Rawson in “Against Cleaning when employing these methods!\nBefore you even begin to clean individual texts, remember that you are also likely going to select groups of texts from within our collected transcriptions. Depending on what question you’ll pursue, you might limit your letters to just be from a certain decade, or from (or to!) a particular individual, etc. Try to aim for enough letters to render your reading a “distant” one! (In other words, 10 1-page letters probably won’t be enough! Aim higher!)\n \n\n\nIn some cases, a text analysis tool will treat words differently if they are in different cases. Remember, computers see strings of characters, and are as frustratingly literal as possible! This cleaning method eliminates the possibility of the computer treating the words Mother and mother differently. There could be a scenario when you’d like them to be different (i.e. maybe they say “Mother” as her name, but refer to her as “mother” to each other), so keep that in mind!\n\n\n\nThis method allows you to programmatically remove or replace characters, words, or lemmas. This is common for a text that is read through OCR recognition (when you scan a document instead of transcribe it, and a computer tries to recognize the characters). Often OCR will substitute characters or images with non-letter symbols, like Ê, §, ¶, ¥, or %. This is less likely to happen with transcriptions.\nHowever, removing punctuation might be helpful, especially since letter writers use punctuation differently! Furthermore, if you are analyzing the body of the letter, you will want to remove the part of the text that includes the envelope and letter head. Consider the different ways to do this (stop words, cutting the beginning of all the letters, removing line breaks, etc.) \n\n\n\nLike the ctrl+F function, replacing items across the board is another way to clean a text. (Remember doing this for the network analysis?) This could be helpful with things like abbreviations, nicknames, inconsistencies in spelling, etc.\n\n\n\nIn the previous section, we talked about removing words with “little value” (poor little words!), which you can alter according to the specifications of the text you’re dealing with. This also applies to words that are endemic to the colleciton. For instance, you might see the word “Mississippi” over and over again, and think, well duh! If your analysis reveals “Mississippi” as a significant word that you aren’t really interested in, add it to your stop words.\nIn different tools, you will separate these words differently (spaces, commas, or line breaks).\n\n\nUse this check-in to sketch out your main point of inquiry using text analysis. This will require more thinking through your text analysis assignment, so be sure to think through possible options, and you might even read through more letters before completing this. Complete the 3.Text Analysis check-in to complete this lesson."
  },
  {
    "objectID": "Modules/narrative.html",
    "href": "Modules/narrative.html",
    "title": "Introduction to Digital Exhibits",
    "section": "",
    "text": "We have been focusing on “telling the story” of a collection thus far through mini-projects like network analyses, timelines, maps, and textual analyses. Now it is time to use a story to tell the story! Thus, we will create a narrative, online project to weave together the items in the collection - and exhibit the artifacts we’ve created with them - with what we have learned through our prodding, research, and exploration.\n\nIdentify the qualities of an accessible web narrative\nIdentify elements that comprise Markdown formatting, and design a simple document with those elements\nDesign a digital exhibit with a logical outline and course artifacts\nCreate a digital exhibit portfolio that compiles primary sources, secondary sources, and derived artifacts from previous course modules.\n\n\n\n\n\nWeb-based narratives are great for sharing a variety of content in an open, public space. Let’s remember some important things when writing for an online environment.\n\nWeb-based writing should be concise. Say what you’re going to say with simple, direct sentences. This doesn’t mean it has to be short (although that’s preferred), but it does warrant a more economic attitude toward words.\nWeb-based writing should be accessible. Web-based writing should strive to be accessible for any and all users, no matter their intellectual levels or physical abilities. In other words, you’re writing for a diverse public, and not a singular or specifically-trained audience. Remember the POUR acronym when you’re writing your narrative:\n\n\nPerceivable - the user can engage with elements through various senses (primarily sound and sight, or touch for haptic screens). Example: If you include an image, thoroughly describe that image with alternative text. If you include a video, make sure it has captions. The image below contains a good example of thorough alt text:\n\n\n\n\nScreenshot of a letter from “Women’s Stories, W.E.B. Du Bois Papers Data” via https://sway.office.com/jnwIZkrNCACbONhA with alternative text highlighted\n\n\n\nOperable - a user can successfully use controls, buttons, navigation, and other interactive elements. For many, this means identifying something visually, and then clicking, tapping, or swiping. For others, using a computer keyboard or voice commands may be the only way they can operate and control the page.\nLook at the differences in navigation between this site’s accessibility view and it’s default view.\nUnderstandable - content is consistent in its presentation and format, predictable in its design and organizing patterns, is concise, multimodal, and appropriate to the audience in its voice and tone. Anyone should be able to comprehend the content and use the page with relative ease.\nExample: a website for children that uses complex, higher-level words without defining them is not very accessible. The website would need to introduce these words with pictures and definitions (or uses less complex synonyms).\nRobust - Users should be able to choose the technology they use to interact with websites, online documents, multimedia, and other information formats. If something only works in one browser or with a proprietary format, it is not very accessible.\nExample: a website with a plug-in that is outdated or not supported on all devices or browsers isn’t very accessible.\n\n\n\nBroken image icon - a page with an invisible line through it\n\n\n\n\nWeb-based writing should use web elements. Purposefully break up your text with formatting elements (see next section), use hyperlinks where appropriate (to cite, to refer to other content, etc.), and include media to enhance and solidify the meaning of something.\n\n\n\nCheck your accesibility comprehension with the 1.Narrative check-in before moving on to the next section.\n\n\n\n\nMarkdown is a very simple markup language (ha! did you catch that pun?!) used for writing for the web. Put simply, Markdown allows us to format our text using simple cues. All of our course lessons are written using Markdown, with some HTML here and there. Most websites use HTML (Hyper Text Markup Language), another markup language, to structure web documents that use nested labels to tell a browser how you want something displayed. Markdown and HTML work together in many cases, so while we’ll mostly use Markdown, you may see (or even use) some HTML elements while formatting our narrative project.\n\n\n\n\n\nHeadings are marked with 1) the #, 2) a space, 3) the text, and 4) a hard return.\nIn markdown, you would type # Heading 1, but the computer would read: &gt; # Heading 1\nExamples of more headings and subheadings: \n\n\n\nStyle your text to create emphasis with: &gt; **bold** –&gt; bold or *italics* –&gt; italics\nCreate an ordered or unordered list:\n1. Ordered list item\n* unordered list item 1\n- unordered list item 2\n\n\nordered list item\n\n\nunordered list item 1\nunordered list item 2\n\n\nHighlight quotes with a blockquote:\n&gt; Let's highlight this quote turns into &gt; Let’s highlight this quote\n\n\n\nYou can instantly hyperlink something with angle brackets, like this: &gt; &lt;hyperlinked-URL&gt;\nTo hyperlink text, put the text in square brackets [], immediately followed by the URL in parentheses. Like this: &gt; [This is the linked text](hyperlink-URL)\nAlternatively, you can use a bit of HTML to hyperlink text. It is one of the most common and useful bits of code to learn! HTML uses the anchor element and the hypertext reference attribute, or &lt;a href&gt;. A full link would look like this: &gt; &lt;a href=\"example.linkhere\"&gt;Linked text here.&lt;/a&gt;\nInserting an image is similar, but add an exclamation point to the beginning, then include square brackets containing the alt text describing it, and parentheses with the image URL: &gt; ![alternative text describing the image](Image URL)\nTo hyperlink an image, put square brackets around the Markdown for the image, and immediately follow that with the URL in parentheses, like this:\n\n[![alternative text](image-url)](hyperlinked-url)\n\nFor more help with markdown elements, there are free, online cheat sheets, like Markdownguide.org. Bonus: You can insert emoji with Markdown by putting the alt text in between two colons, like this: :rofl:. Here’s a full cheat sheet. Have fun! :wink:\n\n\n\nCreate a markdown file (save it as .md) in your text editor that uses elements above for the 2.Narrative check-in.\n\n\n\n\n\nThis is the tricky part! We’re going to weave the writing and artifacts we’ve already created into a digital exhibit. Here are some examples to help you think about an end product:\n\nHumboldt Redwoods Project - exhibit highlighting\nStarkville Civil Rights Project\nMapping Marronage\nPerforming Archive: Curtis + “the vanishing race”\nDASH Amerikan\n\n\n\nWhat we are essentially doing through our narrative project is creating an online exhibit using the sources we’ve gathered (both primary and secondary) and the digital artifacts we’ve created in the class. In order to understand the fundamentals of online exhibits, we need to know what goes into the planning and design of one.\nAs you think about taking your online audience through a tour of the collection, consider the following:\n\nWho is this audience? - Who are you writing this for? What do you want to say to them and how will you communicate? What is your tone?\nWhat is the purpose of your narrative? Think about the items you have, the ones you want to highlight, and any themes that you want to use to guide your audience through the story. In other words, what is the thesis of this story? What is the central idea of the narrative?\nWhat pieces will you use? We have amassed quite a lot of data over the course of the semester. Think about all you have collected, and how it will help you tell the story you’ve sketched. You may curate anything you’ve created/collected so far, including:\n\nLetters from the Smith Project\nExternal primary sources (photographs, etc.)\nSecondary research (book chapters, articles, websites, etc.)\nMetadata (&lt;– You will need this, regardless!)\nProjects you’ve already created\nText you’ve already written\n\nHow will you organize your layout? What system are you using to organize your layout? Is it going to be chronological? Will it be organized by topic? This will depend on the story you want to tell, and the artifacts you have to tell it.\n\n\n\n\nexamples of a chronologically organized and topically organized exhibit\n\n\n\n\n\nDesigning digital exhibits and writing narratives are recursive acts, meaning they may be constantly redesigned from the beginning of the project to the end (just like a research paper!). You may not end up with what you started in your outline. Keeping an open mind, being flexible with your plan, and keeping (somewhat) organized notes will help you throughout this process!\n\n\n\nExample of an exhibit outline\n\n\n\n\n\n\nIn addition to the story you’re telling and the artifacts you’re using, you’ll want to give more context about yourself, the project, AND give your audience options for further exploration. (LIB 201 students will be primarily responsible for collecting and writing this.) Include the following sections in your narrative:\n\nHome: This is the first thing your audience will see, so this is where you introduce your project. This is where you will explain the “so what” of the project, or the research question(s) that guided your inquiry, and give any other information that will help your audience decide how to interact with the narrative.\nAbout: This includes information about YOU, about the collection you used, why you did the project, and your process. Sometimes, the about section can include navigation or “how to” help, if there are more complex interactive portions or if there are several ways to navigate the page.\nAcknowledgements: This of this as your works cited page. This will include hyperlinks to the sources you cited, but it can also include links to related archives or projects that gave you inspiration, and it can link to other archives or sources for further research.\n\n\n\nFor this check-in, you will need to do some brainstorming. Be ready to: - write the central idea/thesis/purpose statement of your narrative in 1-2 sentences. - Brainstorm a list of the items (letters, metadata, projects, research, etc.) you’d like to focus on - sketch an outline of the narrative itself in 3-5 bullet points\nAfter you’ve considered these things, complete the 3. Narrative check-in. This is the final check-in! After you complete this sketch, you’ll work toward completing the final narrative portfolio assignment."
  },
  {
    "objectID": "Modules/narrative.html#writing-for-the-web",
    "href": "Modules/narrative.html#writing-for-the-web",
    "title": "Introduction to Digital Exhibits",
    "section": "",
    "text": "Web-based narratives are great for sharing a variety of content in an open, public space. Let’s remember some important things when writing for an online environment.\n\nWeb-based writing should be concise. Say what you’re going to say with simple, direct sentences. This doesn’t mean it has to be short (although that’s preferred), but it does warrant a more economic attitude toward words.\nWeb-based writing should be accessible. Web-based writing should strive to be accessible for any and all users, no matter their intellectual levels or physical abilities. In other words, you’re writing for a diverse public, and not a singular or specifically-trained audience. Remember the POUR acronym when you’re writing your narrative:\n\n\nPerceivable - the user can engage with elements through various senses (primarily sound and sight, or touch for haptic screens). Example: If you include an image, thoroughly describe that image with alternative text. If you include a video, make sure it has captions. The image below contains a good example of thorough alt text:\n\n\n\n\nScreenshot of a letter from “Women’s Stories, W.E.B. Du Bois Papers Data” via https://sway.office.com/jnwIZkrNCACbONhA with alternative text highlighted\n\n\n\nOperable - a user can successfully use controls, buttons, navigation, and other interactive elements. For many, this means identifying something visually, and then clicking, tapping, or swiping. For others, using a computer keyboard or voice commands may be the only way they can operate and control the page.\nLook at the differences in navigation between this site’s accessibility view and it’s default view.\nUnderstandable - content is consistent in its presentation and format, predictable in its design and organizing patterns, is concise, multimodal, and appropriate to the audience in its voice and tone. Anyone should be able to comprehend the content and use the page with relative ease.\nExample: a website for children that uses complex, higher-level words without defining them is not very accessible. The website would need to introduce these words with pictures and definitions (or uses less complex synonyms).\nRobust - Users should be able to choose the technology they use to interact with websites, online documents, multimedia, and other information formats. If something only works in one browser or with a proprietary format, it is not very accessible.\nExample: a website with a plug-in that is outdated or not supported on all devices or browsers isn’t very accessible.\n\n\n\nBroken image icon - a page with an invisible line through it\n\n\n\n\nWeb-based writing should use web elements. Purposefully break up your text with formatting elements (see next section), use hyperlinks where appropriate (to cite, to refer to other content, etc.), and include media to enhance and solidify the meaning of something.\n\n\n\nCheck your accesibility comprehension with the 1.Narrative check-in before moving on to the next section."
  },
  {
    "objectID": "Modules/narrative.html#markdown",
    "href": "Modules/narrative.html#markdown",
    "title": "Introduction to Digital Exhibits",
    "section": "",
    "text": "Markdown is a very simple markup language (ha! did you catch that pun?!) used for writing for the web. Put simply, Markdown allows us to format our text using simple cues. All of our course lessons are written using Markdown, with some HTML here and there. Most websites use HTML (Hyper Text Markup Language), another markup language, to structure web documents that use nested labels to tell a browser how you want something displayed. Markdown and HTML work together in many cases, so while we’ll mostly use Markdown, you may see (or even use) some HTML elements while formatting our narrative project."
  },
  {
    "objectID": "Modules/narrative.html#common-markdown-elements",
    "href": "Modules/narrative.html#common-markdown-elements",
    "title": "Introduction to Digital Exhibits",
    "section": "",
    "text": "Headings are marked with 1) the #, 2) a space, 3) the text, and 4) a hard return.\nIn markdown, you would type # Heading 1, but the computer would read: &gt; # Heading 1\nExamples of more headings and subheadings: \n\n\n\nStyle your text to create emphasis with: &gt; **bold** –&gt; bold or *italics* –&gt; italics\nCreate an ordered or unordered list:\n1. Ordered list item\n* unordered list item 1\n- unordered list item 2\n\n\nordered list item\n\n\nunordered list item 1\nunordered list item 2\n\n\nHighlight quotes with a blockquote:\n&gt; Let's highlight this quote turns into &gt; Let’s highlight this quote\n\n\n\nYou can instantly hyperlink something with angle brackets, like this: &gt; &lt;hyperlinked-URL&gt;\nTo hyperlink text, put the text in square brackets [], immediately followed by the URL in parentheses. Like this: &gt; [This is the linked text](hyperlink-URL)\nAlternatively, you can use a bit of HTML to hyperlink text. It is one of the most common and useful bits of code to learn! HTML uses the anchor element and the hypertext reference attribute, or &lt;a href&gt;. A full link would look like this: &gt; &lt;a href=\"example.linkhere\"&gt;Linked text here.&lt;/a&gt;\nInserting an image is similar, but add an exclamation point to the beginning, then include square brackets containing the alt text describing it, and parentheses with the image URL: &gt; ![alternative text describing the image](Image URL)\nTo hyperlink an image, put square brackets around the Markdown for the image, and immediately follow that with the URL in parentheses, like this:\n\n[![alternative text](image-url)](hyperlinked-url)\n\nFor more help with markdown elements, there are free, online cheat sheets, like Markdownguide.org. Bonus: You can insert emoji with Markdown by putting the alt text in between two colons, like this: :rofl:. Here’s a full cheat sheet. Have fun! :wink:\n\n\n\nCreate a markdown file (save it as .md) in your text editor that uses elements above for the 2.Narrative check-in."
  },
  {
    "objectID": "Modules/narrative.html#designing-and-creating-a-digital-exhibit",
    "href": "Modules/narrative.html#designing-and-creating-a-digital-exhibit",
    "title": "Introduction to Digital Exhibits",
    "section": "",
    "text": "This is the tricky part! We’re going to weave the writing and artifacts we’ve already created into a digital exhibit. Here are some examples to help you think about an end product:\n\nHumboldt Redwoods Project - exhibit highlighting\nStarkville Civil Rights Project\nMapping Marronage\nPerforming Archive: Curtis + “the vanishing race”\nDASH Amerikan\n\n\n\nWhat we are essentially doing through our narrative project is creating an online exhibit using the sources we’ve gathered (both primary and secondary) and the digital artifacts we’ve created in the class. In order to understand the fundamentals of online exhibits, we need to know what goes into the planning and design of one.\nAs you think about taking your online audience through a tour of the collection, consider the following:\n\nWho is this audience? - Who are you writing this for? What do you want to say to them and how will you communicate? What is your tone?\nWhat is the purpose of your narrative? Think about the items you have, the ones you want to highlight, and any themes that you want to use to guide your audience through the story. In other words, what is the thesis of this story? What is the central idea of the narrative?\nWhat pieces will you use? We have amassed quite a lot of data over the course of the semester. Think about all you have collected, and how it will help you tell the story you’ve sketched. You may curate anything you’ve created/collected so far, including:\n\nLetters from the Smith Project\nExternal primary sources (photographs, etc.)\nSecondary research (book chapters, articles, websites, etc.)\nMetadata (&lt;– You will need this, regardless!)\nProjects you’ve already created\nText you’ve already written\n\nHow will you organize your layout? What system are you using to organize your layout? Is it going to be chronological? Will it be organized by topic? This will depend on the story you want to tell, and the artifacts you have to tell it.\n\n\n\n\nexamples of a chronologically organized and topically organized exhibit\n\n\n\n\n\nDesigning digital exhibits and writing narratives are recursive acts, meaning they may be constantly redesigned from the beginning of the project to the end (just like a research paper!). You may not end up with what you started in your outline. Keeping an open mind, being flexible with your plan, and keeping (somewhat) organized notes will help you throughout this process!\n\n\n\nExample of an exhibit outline"
  },
  {
    "objectID": "Modules/narrative.html#additional-sections-to-include",
    "href": "Modules/narrative.html#additional-sections-to-include",
    "title": "Introduction to Digital Exhibits",
    "section": "",
    "text": "In addition to the story you’re telling and the artifacts you’re using, you’ll want to give more context about yourself, the project, AND give your audience options for further exploration. (LIB 201 students will be primarily responsible for collecting and writing this.) Include the following sections in your narrative:\n\nHome: This is the first thing your audience will see, so this is where you introduce your project. This is where you will explain the “so what” of the project, or the research question(s) that guided your inquiry, and give any other information that will help your audience decide how to interact with the narrative.\nAbout: This includes information about YOU, about the collection you used, why you did the project, and your process. Sometimes, the about section can include navigation or “how to” help, if there are more complex interactive portions or if there are several ways to navigate the page.\nAcknowledgements: This of this as your works cited page. This will include hyperlinks to the sources you cited, but it can also include links to related archives or projects that gave you inspiration, and it can link to other archives or sources for further research.\n\n\n\nFor this check-in, you will need to do some brainstorming. Be ready to: - write the central idea/thesis/purpose statement of your narrative in 1-2 sentences. - Brainstorm a list of the items (letters, metadata, projects, research, etc.) you’d like to focus on - sketch an outline of the narrative itself in 3-5 bullet points\nAfter you’ve considered these things, complete the 3. Narrative check-in. This is the final check-in! After you complete this sketch, you’ll work toward completing the final narrative portfolio assignment."
  },
  {
    "objectID": "Modules/mapping.html",
    "href": "Modules/mapping.html",
    "title": "Introduction to Spatial and Temporal Data",
    "section": "",
    "text": "Identify further resources (primary and/or secondary sources) as context to support a spatial or temporal analysis\nAddress a research question using an original map or timeline\nUnderstand what geospatial data is, how it is organized, and where to find it.\nIdentify and manage files, including external and internal images, for creating a timeline or mapping visualization\n\n(Tweet from @TerribleMaps - https://twitter.com/TerribleMaps/status/1442190976525635593)\nWhy make a map or a timeline? What’s the point? What kinds of questions do these answer?\nThink of the data we’ve already gathered in the Smith letters. Which data allow themselves to tell a geographical (i.e. a story across locations) or temporal (i.e. a story across time) story? More importantly (because the map and the timeline aren’t the point in and of themselves!), wow can we supplement those data with contextual information that we can incorporate to tell a bigger story?\n\n\n\n\n\nBefore we get into figuring out what all goes into a map or a timeline, the most important thing is to figure out what questions to ask of our data. In order to do that, we will be gathering research for this project to give context to the letters – events, themes, and people in the letters – to tell a bigger story. Our timelines and our maps will explore something in the letters, and they will likely contain other primary and secondary sources to explain the significance of an event.\n\n\nPrimary sources are often considered more “direct” pieces of evidence that are from someone’s individual point of view, or they report on what is happening in the moment. These could include things like: * letters (ahem!) * newspapers * interviews * autobiographies or diaries * photographs * original observations or experiments * original creative works\n…and more!\n\n\n\nJust because a primary source is considered direct or in “real time,” it is still from the point of view of someone who (like all of us!) has opinions, perspectives, and bias. All historical documents are written by an author with a specific point of view, and no matter how objective the author may try to appear, their unique perspectives and inclinations influence the document. Keep this in mind!\n\n\n\nSecondary sources are usually interpretive, aren’t usually published, posted, or shared in the moment something is occurring, and they often include primary sources in discussion with ideas or hypotheses. This could include things like * scholarly articles * books and book chapters * websites (blogs, articles, reviews, etc.) * …and more!\n\n\nComplete the 1a.Research data check-in on primary and secondary sources before moving on to the next section.\n\n\n\n\nNow that we’ve figured out the differences between primary and secondary sources, you will use this information to choose sources in different places. For our upcoming projects, we will look in several places, including, but not limited to: * Websites (i.e. blogs, news sites, .orgs, .govs, etc.). * Blogs can show you someone else’s perspective on a related topic, link to other sources, etc. * You can find reports and data on government or organizational websites like the National Archives and Records Administration or Mississippi History Now, from the Mississippi Department of Archives and History. * Wikipedia (gasp!) can even get you started with background information on a topic you’re unfamiliar with! The external and reference links at the bottom of the page are also usually good breadcrubms to follow. * Library books. Books (or book chapters) are a great way to take a deep dive into a topic. For establishing a foundation for a research topic that involves culture, history, major events…books are a great place to start. * Books in the library are great, but you can easily get a book outside of the library! When searching in the catalog, change the drop-down menu from “MUW Library” to “Everything,” and if you find a book outside of MUW, click the Place Hold button. * To broaden the possibilities even further, look for books outside of this partnership in the WorldCat database. Request what you find through Interlibrary Loan! * Library databases. Search library databases for things like scholarly articles, news, reviews, and more. * Digital collections or archives * Our own AthenaCommons contains digitized letters from the Smith Papers Collection. * The University of Mississippi’s eGrove repository has a digitized collection of MS Blue Books, which contain historical legislative information and other primary sources relevant to our collection.\nTo help with research topics, resources, and relevant websites, MUW faculty and staff have started the Smith Papers Research Guide. Use this to help get started, or to gather more information on a topic.\n\n\nComplete the Research data check-in on finding resources before moving on to the next section.\n\n\n\n\nBy now, you know that not everything you find on the internet is worth using! As you’re looking up resources, keep these things in mind:\n\nRelevance. Determine a source’s relevance to your topic by asking if it will help you get to where you need to go. It might help influence the direction of your research, but that’s ok!\nOrigin of the source. Where is this coming from? What is the domain name? Does the resource cite any references?\nPurpose of the source. What is this source trying to accomplish? Is it selling a product? Is it to share information/educate? Is it to persuade? Consider if there is an ulterior motive (especially if you’re searching a company or corporation’s website).\n\n\n\n\n\nWe are always taught to cite our sources, so expect the sources you use to do the same! While we are not adopting a specific style for this class (e.g. MLA, APA, CMoS, etc.), look for citation information within a source, and cite the sources you use by hyperlinking pertinent info (e.g. author, title, etc.).\nWhen you do find media you want to reuse, make sure you have the permissions to do so. Since we are creating maps and timelines using open, online tools, ask yourself if the artist/creator would mind you posting their stuff, and if so, ALWAYS give them recognition!\nWhen reusing images, video, etc., be sure the links you use are supported by the platform you’re sharing them on. Not all image and video files will work in all places (for example, see the supported media types used in TimelineJS)! Additionally, be sure you’re copying the correct URL if using an external image.\n\nTip: permissions to share images can come in the form of usage rights licenses, creative commons licenses, or public domain designations. Look for these notes next to an image in its repository. You can search for images with usage rights in many places, like * Wikimedia Commons * Creative Commons * Flickr * Library digital repositories or archives (like MS Digital Library, NYPL, and more!) * Google Images (use the usage rights search feature to filter the results) * Getty Images\n\n\nComplete the final 1c.Research data check-in of this section on choosing resources for research before moving to lesson 2.\n\n\n\n\n\nWhy create a timeline? If you would like to show the progression of or change in data over time, a timeline will visualize this for you. Both maps and timelines use spreadsheets to associate information with a spot on a “map.” Timelines associate information with dates and ranges of time. Put differently, by pairing references in the letters with other sources along a linear progression, we are putting the letters in a larger context of a year, a few years, a decade, etc.\n\n\n\nScreenshot of a timeline from Starkville Civil Rights project, featuring a letter from Douglas Connor to found the Oktibbeha Co. NAACP\n\n\nBefore we get into tools, let’s remember that we are telling a story about a progression of thoughts and events over time. To help center the story (and not the website we’re posting it on), let’s sketch an outline of the timeline. In your sketch, you’ll want to consider: * What are the boundaries of my timeline (start date, end date)? * What information from my primary sources do I want to highlight? * What information from my secondary sources will help establish context? * Do I have permission to share images/media from these secondary sources? * Are there themes, ranges of time, or other patterns that I want to highlight among my sources? (Example: you can group things by local v. national event, or tag certain events with a theme, like “desegregation” or “New Deal.”)\n\n\nAnswer the questions above with a sketch of your timeline in the 2.Timeline check-in before moving on.\n\n\n\nLike any project, there are a myriad of tools that you can use. Palladio (our tool for the Network Analysis), for instance, has timeline options! For this lesson, we are going to use TimelineJS, a template-based story-telling tool developed by Northwestern University’s Knight Lab.\nTimelineJS uses Google Sheets to organize dates, add context, include media (photos, videos, sound, etc.), and groups things according to time and tags (See more on this here.) So, sign in to your Google MyApps Account before getting started.\nOnce you have signed in, click Get the Spreadsheet Template, and click “Make a Copy.”\n\n\n\nScreenshot of Knightlab website’s step 1 of making a timeline\n\n\nThe first row contains the column headings and does not need editing. The 2nd row (the blue one) is going to be your title slide in the timeline, and doesn’t need date metadata. You will see sample information in rows 2-3 that show you what is possible to add.\nBegin creating your timeline using the assignment guidelines and filling out your story with metadata (dates, images, captions, etc.)! To publish (i.e. when you’re done), follow steps 2-3 here.\n\n\n\n\nMaps are also storytelling (or story showing) devices that use location as a basis.\n\n\n\nScreenshot of a map of MS with counties outlined in red, and with 3 points in Tupelo, Pittsboro, and Meridian\n\n\n\n\nMaps are layers of data represented by shapes, lines, and points. Layers usually include a: * Base map layer - something you put all of your layers on top of. This could be a historic map, a map with specific boundaries, a topographical map, etc., etc.! * Shapefiles - these are files that contain shapes, lines, and points that correspond to places, and include information on them. An example would be a point (like a city) and its geographical location (latitude, longitude). Other examples can include county boundaries, rivers, buildings, population (census) data, and routes in between locations. These are just a few possibilities! There are 2 kinds of shapefile data: * Vector Data - this is discrete data, like the examples mentioned above. * Raster Data (we will not use raster data in this class) - this includes data that is continuous, like weather patterns or elevation.\nShape files can come in a variety of formats. Most of the time, they will include several files compressed, or zipped (.zip), together. We will see shapefiles in various formats, including: * .kml (or .kmz, if several are compressed in a folder) * .sph * .gpx * .csv\n\n\nComplete 3a.Mapping checkin before going to the next section.\n\n\n\n\n\n\n\nImage of a map showing its shapefile’s data\n\n\nMaps are often built with tables of attributes (or, METADATA!) that correspond to locations–like a spreadsheet that corresponds to geographic plots. It can be challenging to find data sets that: * are relevant to your research. For example, if you wanted to show the proximity of locations to waterways, you’d want to find a map of MS rivers, and not dam locations. Sometimes it’s easy to get distracted by different data sets. * fit within the limits of your software, time, and size requirements. We are using free mapping software, which comes with limits for file types, size, and amount of expertise needed. Some data sets will surpass all of these! * exist! Just like any other type of data, gathering data for a map can be time consuming and specific to the needs of a researcher, so not everything you are looking for will exist. You will have to decide if you need to create a data set from scratch, find one, or pivot your question to accommodate a lack in data.\nSome places you might find map data include: * ArcGIS Hub - contains open data shapefiles, including government data (city infrastructures, etc.), election data, and more. * US Census Mapping Files - contains regional and geographic boundaries, demographic data, and more. * Local and state entities, like MARIS - serves as the statewide GIS clearinghouse for Mississippi. * Google - search for the data (e.g. “TVA power lines”) and the filetype (e.g. “shapefile”).\nFor this map, we are going to use a combination of data that exists, and data that we create using location information from the Smith papers and secondary sources.\n\n\n\nMany maps, especially digital mapping projects, use a 2-dimensional representation (i.e. flat) of the earth, which is just a part of the inaccuracies of our world that maps represent. Like historical records, maps and timelines are incomplete choices that represent human experience of the world. As you create maps and/or timelines, consider these questions, adapted from DH Institutes Curriculum, that critically examine the process of map-making: * How are you collecting data? Are you gathering data, or using existing data? What are the limitations of either, and what choices lead to this decision (convenience? access? most appropriate?) * What is missing from your data? If you collect population information, which populations are you including/excluding? What boundaries (county? state? neighborhood?) define the map? To give you an idea of what it might take to gather the data you’d like to examine, here’s a snapshot of what you could encounter.\nExample research question: How many men registered for the WWII draft in Mississippi, and where did they go to basic training? (Note: this research question is not perfect and contains some holes, like there is not a database that shows where MS men went to basic training, and it assumes that most MS residents did basic training in MS, which is not true. But for the sake of demonstration…!)\nData obstacles: There are a couple of major databases that contain the number of WWII enlistees (e.g. Fold3.com, The National Archives and Records Administration, and Ancestry.com), but to digitally access this data, there are obstacles: * Proprietary data - in order to access these records at the level we need them for this question (the enlisted’s place of residence), you have to have an Ancestry.com account, which is not free, unless you can get to a place with the type of subscription that includes this data (not all subscriptions do! :frowning_face:). * Amount of data/labor - In Mississippi alone, there were over 725,000 men between 21-36 years old that registered for the draft between 1940-1947. (That doesn’t count the additional registrations that happened between 1941 and 1943.) Ancestry allows a maximum of 50 records to view per page, and doesn’t give an option to export the entire contents of the collection into a file like a CSV or XLSX. The options are to * scrape the data through code (which requires permissions and has a high learning curve) * copy and paste the data into a spreadsheet, page by page. With over 725,000 drafted at 50 per page, that is 14,500 pages of data. And it takes 1.5 hours to copy and paste 100 pages’ worth. That would be over 200 hours of copying and pasting…and that doesn’t count the time it would take to clean it! Which brings us to… * Cleaning the data - Copying and pasting 5,000 entries took around 2 hours, but the data copied did not separate the residential information into usable columns, and the data itself had several misspellings and inconsistencies. To clean this, you’d have to separate the location information into separate columns with a split formula, then sort, filter, and remove duplicates to find the inconsistencies. This took about 2 more hours of labor!\n\n\n\nScreenshot of the cleaning process to split columns and clean names\n\n\n\nTechnological obstacles:\n\nAfter page 100 of the results, Ancestry stopped allowing me to go through page-by-page. (Likely a result of the system preventing me from copying proprietary information.)\nAs I was searching Google for county information of registrants with missing locations of residence, I tripped an alarm from Google, which thought I was behaving too much like a bot :laughing:\n\n\n\n\nScreenshot of a message from Google verifying I’m not a robot\n\n\n\n\n\n\nComplete the Mapping check-in before completing the last section in this lesson.\n\n\n\n\nThere are a range of different mapping tools that require a range of experience and can cost $0 or require a monthly subscription! We are going to use ArcGIS online, which is free (with some limits), and doesn’t require any additional downloads. Before we begin, create a free, public account. (Instructions for signing up are here.)\n\n\n\nFor our map, we are going to use a shapefile of the electric lines owned by the TVA. You can find and download this zipped folder to get started.\nIn ArcGIS online, you can choose any basemap you’d like, but let’s look at what goes into a shapefile by adding the zipped folder to the map.\n\nClick Add, then add layer from file.\n\n\n\n\nScreenshot of instructions for how to add a layer from file\n\n\n\nChoose the compressed folder, and click Import Layer. What we’re doing is importing a table that recognizes geocoded information.\nTo see what information is included, click on content, and the table icon underneath the shapefile’s title.\n\n\n\n\nScreenshot of map layer contents\n\n\nTake a minute to explore the shapefile’s contents! What can you discern from each column heading, or contents within?\n\nNow it’s your turn to add data from the Smith Papers and from secondary sources.\n\nTo create your own data set, think about the data you’ll be collecting, and what all you want to be visible on your map. Each location will have accompanying metadata, which will likely include: * Date * Location  Media * Captions * Details*\n*Location details may differ. Maps often use latitude/longitude for specific locations (which you can easily Google. Pittsboro, MS’s long/lat, for instance, is 33.9400608,-89.3417058). Some maps can translate things like address (street address + city + state) into a point on a map.\n\n\n\nScreenshot of Pittsboro, MS on a map, showing the lat/long in the URL on Google Maps’ website\n\n\n**This will most likely be a URL of an image, video, or other media type. Remember to be sure you can re-share images, and ALWAYS give credit to the original source.\n***This will be a good place to provide context and even interpretation of certain events or locations."
  },
  {
    "objectID": "Modules/mapping.html#primary-and-secondary-source-research",
    "href": "Modules/mapping.html#primary-and-secondary-source-research",
    "title": "Introduction to Spatial and Temporal Data",
    "section": "",
    "text": "Before we get into figuring out what all goes into a map or a timeline, the most important thing is to figure out what questions to ask of our data. In order to do that, we will be gathering research for this project to give context to the letters – events, themes, and people in the letters – to tell a bigger story. Our timelines and our maps will explore something in the letters, and they will likely contain other primary and secondary sources to explain the significance of an event.\n\n\nPrimary sources are often considered more “direct” pieces of evidence that are from someone’s individual point of view, or they report on what is happening in the moment. These could include things like: * letters (ahem!) * newspapers * interviews * autobiographies or diaries * photographs * original observations or experiments * original creative works\n…and more!\n\n\n\nJust because a primary source is considered direct or in “real time,” it is still from the point of view of someone who (like all of us!) has opinions, perspectives, and bias. All historical documents are written by an author with a specific point of view, and no matter how objective the author may try to appear, their unique perspectives and inclinations influence the document. Keep this in mind!\n\n\n\nSecondary sources are usually interpretive, aren’t usually published, posted, or shared in the moment something is occurring, and they often include primary sources in discussion with ideas or hypotheses. This could include things like * scholarly articles * books and book chapters * websites (blogs, articles, reviews, etc.) * …and more!\n\n\nComplete the 1a.Research data check-in on primary and secondary sources before moving on to the next section.\n\n\n\n\nNow that we’ve figured out the differences between primary and secondary sources, you will use this information to choose sources in different places. For our upcoming projects, we will look in several places, including, but not limited to: * Websites (i.e. blogs, news sites, .orgs, .govs, etc.). * Blogs can show you someone else’s perspective on a related topic, link to other sources, etc. * You can find reports and data on government or organizational websites like the National Archives and Records Administration or Mississippi History Now, from the Mississippi Department of Archives and History. * Wikipedia (gasp!) can even get you started with background information on a topic you’re unfamiliar with! The external and reference links at the bottom of the page are also usually good breadcrubms to follow. * Library books. Books (or book chapters) are a great way to take a deep dive into a topic. For establishing a foundation for a research topic that involves culture, history, major events…books are a great place to start. * Books in the library are great, but you can easily get a book outside of the library! When searching in the catalog, change the drop-down menu from “MUW Library” to “Everything,” and if you find a book outside of MUW, click the Place Hold button. * To broaden the possibilities even further, look for books outside of this partnership in the WorldCat database. Request what you find through Interlibrary Loan! * Library databases. Search library databases for things like scholarly articles, news, reviews, and more. * Digital collections or archives * Our own AthenaCommons contains digitized letters from the Smith Papers Collection. * The University of Mississippi’s eGrove repository has a digitized collection of MS Blue Books, which contain historical legislative information and other primary sources relevant to our collection.\nTo help with research topics, resources, and relevant websites, MUW faculty and staff have started the Smith Papers Research Guide. Use this to help get started, or to gather more information on a topic.\n\n\nComplete the Research data check-in on finding resources before moving on to the next section.\n\n\n\n\nBy now, you know that not everything you find on the internet is worth using! As you’re looking up resources, keep these things in mind:\n\nRelevance. Determine a source’s relevance to your topic by asking if it will help you get to where you need to go. It might help influence the direction of your research, but that’s ok!\nOrigin of the source. Where is this coming from? What is the domain name? Does the resource cite any references?\nPurpose of the source. What is this source trying to accomplish? Is it selling a product? Is it to share information/educate? Is it to persuade? Consider if there is an ulterior motive (especially if you’re searching a company or corporation’s website).\n\n\n\n\n\nWe are always taught to cite our sources, so expect the sources you use to do the same! While we are not adopting a specific style for this class (e.g. MLA, APA, CMoS, etc.), look for citation information within a source, and cite the sources you use by hyperlinking pertinent info (e.g. author, title, etc.).\nWhen you do find media you want to reuse, make sure you have the permissions to do so. Since we are creating maps and timelines using open, online tools, ask yourself if the artist/creator would mind you posting their stuff, and if so, ALWAYS give them recognition!\nWhen reusing images, video, etc., be sure the links you use are supported by the platform you’re sharing them on. Not all image and video files will work in all places (for example, see the supported media types used in TimelineJS)! Additionally, be sure you’re copying the correct URL if using an external image.\n\nTip: permissions to share images can come in the form of usage rights licenses, creative commons licenses, or public domain designations. Look for these notes next to an image in its repository. You can search for images with usage rights in many places, like * Wikimedia Commons * Creative Commons * Flickr * Library digital repositories or archives (like MS Digital Library, NYPL, and more!) * Google Images (use the usage rights search feature to filter the results) * Getty Images\n\n\nComplete the final 1c.Research data check-in of this section on choosing resources for research before moving to lesson 2."
  },
  {
    "objectID": "Modules/mapping.html#building-a-timeline",
    "href": "Modules/mapping.html#building-a-timeline",
    "title": "Introduction to Spatial and Temporal Data",
    "section": "",
    "text": "Why create a timeline? If you would like to show the progression of or change in data over time, a timeline will visualize this for you. Both maps and timelines use spreadsheets to associate information with a spot on a “map.” Timelines associate information with dates and ranges of time. Put differently, by pairing references in the letters with other sources along a linear progression, we are putting the letters in a larger context of a year, a few years, a decade, etc.\n\n\n\nScreenshot of a timeline from Starkville Civil Rights project, featuring a letter from Douglas Connor to found the Oktibbeha Co. NAACP\n\n\nBefore we get into tools, let’s remember that we are telling a story about a progression of thoughts and events over time. To help center the story (and not the website we’re posting it on), let’s sketch an outline of the timeline. In your sketch, you’ll want to consider: * What are the boundaries of my timeline (start date, end date)? * What information from my primary sources do I want to highlight? * What information from my secondary sources will help establish context? * Do I have permission to share images/media from these secondary sources? * Are there themes, ranges of time, or other patterns that I want to highlight among my sources? (Example: you can group things by local v. national event, or tag certain events with a theme, like “desegregation” or “New Deal.”)\n\n\nAnswer the questions above with a sketch of your timeline in the 2.Timeline check-in before moving on.\n\n\n\nLike any project, there are a myriad of tools that you can use. Palladio (our tool for the Network Analysis), for instance, has timeline options! For this lesson, we are going to use TimelineJS, a template-based story-telling tool developed by Northwestern University’s Knight Lab.\nTimelineJS uses Google Sheets to organize dates, add context, include media (photos, videos, sound, etc.), and groups things according to time and tags (See more on this here.) So, sign in to your Google MyApps Account before getting started.\nOnce you have signed in, click Get the Spreadsheet Template, and click “Make a Copy.”\n\n\n\nScreenshot of Knightlab website’s step 1 of making a timeline\n\n\nThe first row contains the column headings and does not need editing. The 2nd row (the blue one) is going to be your title slide in the timeline, and doesn’t need date metadata. You will see sample information in rows 2-3 that show you what is possible to add.\nBegin creating your timeline using the assignment guidelines and filling out your story with metadata (dates, images, captions, etc.)! To publish (i.e. when you’re done), follow steps 2-3 here."
  },
  {
    "objectID": "Modules/mapping.html#geospatial-data",
    "href": "Modules/mapping.html#geospatial-data",
    "title": "Introduction to Spatial and Temporal Data",
    "section": "",
    "text": "Maps are also storytelling (or story showing) devices that use location as a basis.\n\n\n\nScreenshot of a map of MS with counties outlined in red, and with 3 points in Tupelo, Pittsboro, and Meridian\n\n\n\n\nMaps are layers of data represented by shapes, lines, and points. Layers usually include a: * Base map layer - something you put all of your layers on top of. This could be a historic map, a map with specific boundaries, a topographical map, etc., etc.! * Shapefiles - these are files that contain shapes, lines, and points that correspond to places, and include information on them. An example would be a point (like a city) and its geographical location (latitude, longitude). Other examples can include county boundaries, rivers, buildings, population (census) data, and routes in between locations. These are just a few possibilities! There are 2 kinds of shapefile data: * Vector Data - this is discrete data, like the examples mentioned above. * Raster Data (we will not use raster data in this class) - this includes data that is continuous, like weather patterns or elevation.\nShape files can come in a variety of formats. Most of the time, they will include several files compressed, or zipped (.zip), together. We will see shapefiles in various formats, including: * .kml (or .kmz, if several are compressed in a folder) * .sph * .gpx * .csv\n\n\nComplete 3a.Mapping checkin before going to the next section.\n\n\n\n\n\n\n\nImage of a map showing its shapefile’s data\n\n\nMaps are often built with tables of attributes (or, METADATA!) that correspond to locations–like a spreadsheet that corresponds to geographic plots. It can be challenging to find data sets that: * are relevant to your research. For example, if you wanted to show the proximity of locations to waterways, you’d want to find a map of MS rivers, and not dam locations. Sometimes it’s easy to get distracted by different data sets. * fit within the limits of your software, time, and size requirements. We are using free mapping software, which comes with limits for file types, size, and amount of expertise needed. Some data sets will surpass all of these! * exist! Just like any other type of data, gathering data for a map can be time consuming and specific to the needs of a researcher, so not everything you are looking for will exist. You will have to decide if you need to create a data set from scratch, find one, or pivot your question to accommodate a lack in data.\nSome places you might find map data include: * ArcGIS Hub - contains open data shapefiles, including government data (city infrastructures, etc.), election data, and more. * US Census Mapping Files - contains regional and geographic boundaries, demographic data, and more. * Local and state entities, like MARIS - serves as the statewide GIS clearinghouse for Mississippi. * Google - search for the data (e.g. “TVA power lines”) and the filetype (e.g. “shapefile”).\nFor this map, we are going to use a combination of data that exists, and data that we create using location information from the Smith papers and secondary sources.\n\n\n\nMany maps, especially digital mapping projects, use a 2-dimensional representation (i.e. flat) of the earth, which is just a part of the inaccuracies of our world that maps represent. Like historical records, maps and timelines are incomplete choices that represent human experience of the world. As you create maps and/or timelines, consider these questions, adapted from DH Institutes Curriculum, that critically examine the process of map-making: * How are you collecting data? Are you gathering data, or using existing data? What are the limitations of either, and what choices lead to this decision (convenience? access? most appropriate?) * What is missing from your data? If you collect population information, which populations are you including/excluding? What boundaries (county? state? neighborhood?) define the map? To give you an idea of what it might take to gather the data you’d like to examine, here’s a snapshot of what you could encounter.\nExample research question: How many men registered for the WWII draft in Mississippi, and where did they go to basic training? (Note: this research question is not perfect and contains some holes, like there is not a database that shows where MS men went to basic training, and it assumes that most MS residents did basic training in MS, which is not true. But for the sake of demonstration…!)\nData obstacles: There are a couple of major databases that contain the number of WWII enlistees (e.g. Fold3.com, The National Archives and Records Administration, and Ancestry.com), but to digitally access this data, there are obstacles: * Proprietary data - in order to access these records at the level we need them for this question (the enlisted’s place of residence), you have to have an Ancestry.com account, which is not free, unless you can get to a place with the type of subscription that includes this data (not all subscriptions do! :frowning_face:). * Amount of data/labor - In Mississippi alone, there were over 725,000 men between 21-36 years old that registered for the draft between 1940-1947. (That doesn’t count the additional registrations that happened between 1941 and 1943.) Ancestry allows a maximum of 50 records to view per page, and doesn’t give an option to export the entire contents of the collection into a file like a CSV or XLSX. The options are to * scrape the data through code (which requires permissions and has a high learning curve) * copy and paste the data into a spreadsheet, page by page. With over 725,000 drafted at 50 per page, that is 14,500 pages of data. And it takes 1.5 hours to copy and paste 100 pages’ worth. That would be over 200 hours of copying and pasting…and that doesn’t count the time it would take to clean it! Which brings us to… * Cleaning the data - Copying and pasting 5,000 entries took around 2 hours, but the data copied did not separate the residential information into usable columns, and the data itself had several misspellings and inconsistencies. To clean this, you’d have to separate the location information into separate columns with a split formula, then sort, filter, and remove duplicates to find the inconsistencies. This took about 2 more hours of labor!\n\n\n\nScreenshot of the cleaning process to split columns and clean names\n\n\n\nTechnological obstacles:\n\nAfter page 100 of the results, Ancestry stopped allowing me to go through page-by-page. (Likely a result of the system preventing me from copying proprietary information.)\nAs I was searching Google for county information of registrants with missing locations of residence, I tripped an alarm from Google, which thought I was behaving too much like a bot :laughing:\n\n\n\n\nScreenshot of a message from Google verifying I’m not a robot\n\n\n\n\n\n\nComplete the Mapping check-in before completing the last section in this lesson.\n\n\n\n\nThere are a range of different mapping tools that require a range of experience and can cost $0 or require a monthly subscription! We are going to use ArcGIS online, which is free (with some limits), and doesn’t require any additional downloads. Before we begin, create a free, public account. (Instructions for signing up are here.)\n\n\n\nFor our map, we are going to use a shapefile of the electric lines owned by the TVA. You can find and download this zipped folder to get started.\nIn ArcGIS online, you can choose any basemap you’d like, but let’s look at what goes into a shapefile by adding the zipped folder to the map.\n\nClick Add, then add layer from file.\n\n\n\n\nScreenshot of instructions for how to add a layer from file\n\n\n\nChoose the compressed folder, and click Import Layer. What we’re doing is importing a table that recognizes geocoded information.\nTo see what information is included, click on content, and the table icon underneath the shapefile’s title.\n\n\n\n\nScreenshot of map layer contents\n\n\nTake a minute to explore the shapefile’s contents! What can you discern from each column heading, or contents within?\n\nNow it’s your turn to add data from the Smith Papers and from secondary sources.\n\nTo create your own data set, think about the data you’ll be collecting, and what all you want to be visible on your map. Each location will have accompanying metadata, which will likely include: * Date * Location  Media * Captions * Details*\n*Location details may differ. Maps often use latitude/longitude for specific locations (which you can easily Google. Pittsboro, MS’s long/lat, for instance, is 33.9400608,-89.3417058). Some maps can translate things like address (street address + city + state) into a point on a map.\n\n\n\nScreenshot of Pittsboro, MS on a map, showing the lat/long in the URL on Google Maps’ website\n\n\n**This will most likely be a URL of an image, video, or other media type. Remember to be sure you can re-share images, and ALWAYS give credit to the original source.\n***This will be a good place to provide context and even interpretation of certain events or locations."
  },
  {
    "objectID": "Modules/Class 2.html#overview",
    "href": "Modules/Class 2.html#overview",
    "title": "First Steps into GIS",
    "section": "Overview",
    "text": "Overview\n\nBrief History of Spatial Analysis\nGIS / QGIS\nSpatial Data Formats\n\nRaster, Vector\n\nEnding : Data!"
  },
  {
    "objectID": "Modules/Class 2.html#what-is-a-map",
    "href": "Modules/Class 2.html#what-is-a-map",
    "title": "First Steps into GIS",
    "section": "What is a Map?",
    "text": "What is a Map?"
  },
  {
    "objectID": "Modules/Class 2.html#çatalhöyük",
    "href": "Modules/Class 2.html#çatalhöyük",
    "title": "First Steps into GIS",
    "section": "Çatalhöyük",
    "text": "Çatalhöyük"
  },
  {
    "objectID": "Modules/Class 2.html#çatalhöyük-1",
    "href": "Modules/Class 2.html#çatalhöyük-1",
    "title": "First Steps into GIS",
    "section": "Çatalhöyük",
    "text": "Çatalhöyük"
  },
  {
    "objectID": "Modules/Class 2.html#section",
    "href": "Modules/Class 2.html#section",
    "title": "First Steps into GIS",
    "section": "",
    "text": "6790 - 6430 BCE\n\nÇatalhöyük, Turkey\nSquare-shaped patterns, interpreted as plan of a village\nDouble-peaked motif that might have figured a volcanic eruption\nMap, picture, picture map, both, neither?"
  },
  {
    "objectID": "Modules/Class 2.html#saint-bélec-slab",
    "href": "Modules/Class 2.html#saint-bélec-slab",
    "title": "First Steps into GIS",
    "section": "Saint-Bélec slab",
    "text": "Saint-Bélec slab"
  },
  {
    "objectID": "Modules/Class 2.html#saint-bélec-slab-1",
    "href": "Modules/Class 2.html#saint-bélec-slab-1",
    "title": "First Steps into GIS",
    "section": "Saint-Bélec slab",
    "text": "Saint-Bélec slab\n\n\n2150–1600 BCE\n\nFinistère, France\nShows settlements, rivers, etc\nSurface carved to mimic land\nPossibly symbolic?"
  },
  {
    "objectID": "Modules/Class 2.html#tabula-peutingeriana",
    "href": "Modules/Class 2.html#tabula-peutingeriana",
    "title": "First Steps into GIS",
    "section": "Tabula Peutingeriana",
    "text": "Tabula Peutingeriana"
  },
  {
    "objectID": "Modules/Class 2.html#tabula-peutingeriana-1",
    "href": "Modules/Class 2.html#tabula-peutingeriana-1",
    "title": "First Steps into GIS",
    "section": "Tabula Peutingeriana",
    "text": "Tabula Peutingeriana\n\nParchment copy ~ 1200 CE from Late Antiquity?\n4th / 5th century CE original?\nItineraries - distances between places\nWhat are your impressions of this?"
  },
  {
    "objectID": "Modules/Class 2.html#frontispiece-codex-mendoza",
    "href": "Modules/Class 2.html#frontispiece-codex-mendoza",
    "title": "First Steps into GIS",
    "section": "Frontispiece Codex Mendoza",
    "text": "Frontispiece Codex Mendoza"
  },
  {
    "objectID": "Modules/Class 2.html#frontispiece-codex-mendoza-1",
    "href": "Modules/Class 2.html#frontispiece-codex-mendoza-1",
    "title": "First Steps into GIS",
    "section": "Frontispiece Codex Mendoza",
    "text": "Frontispiece Codex Mendoza\n\n\n1541 CE, commissioned by viceroy of New Spain, Antonio de Mendoza\n\nIntended to record information about the Aztec empire\nArtist(s) were indigenous\nImages were often annotated in Spanish by a priest that spoke Nahuatl\nSchematic diagram of Tenochtitlan\n\nDivided into four parts via canals\nMirrored the organization of the universe\n\n\nSymbolic eagle and snake\n\nLots of other symbols!\n\nMix of different types of spaces / places - can you think of some?"
  },
  {
    "objectID": "Modules/Class 2.html#marshall-islands-stick-charts",
    "href": "Modules/Class 2.html#marshall-islands-stick-charts",
    "title": "First Steps into GIS",
    "section": "Marshall Islands Stick Charts",
    "text": "Marshall Islands Stick Charts"
  },
  {
    "objectID": "Modules/Class 2.html#marshall-islands-stick-charts-1",
    "href": "Modules/Class 2.html#marshall-islands-stick-charts-1",
    "title": "First Steps into GIS",
    "section": "Marshall Islands Stick Charts",
    "text": "Marshall Islands Stick Charts\n\nPossibly for thousands of years\nMarshall Islands\nCurved sticks represented ocean swells\nStraight sticks represented the currents and waves around the islands\nSeashells represented the locations of the islands\nMemorized by navigators; unique to them"
  },
  {
    "objectID": "Modules/Class 2.html#what-is-spatial-analysis",
    "href": "Modules/Class 2.html#what-is-spatial-analysis",
    "title": "First Steps into GIS",
    "section": "What is Spatial Analysis?",
    "text": "What is Spatial Analysis?\n\n\n1,400 years ago in Egypt - for land surveys\n\nAncient Rome, centuriation for land alotments\nPractical reasons: Divide up the land!"
  },
  {
    "objectID": "Modules/Class 2.html#spatial-analysis",
    "href": "Modules/Class 2.html#spatial-analysis",
    "title": "First Steps into GIS",
    "section": "Spatial Analysis",
    "text": "Spatial Analysis\n\nWhat do we think spatial analysis is now?"
  },
  {
    "objectID": "Modules/Class 2.html#baron-pierre-charles-dupin",
    "href": "Modules/Class 2.html#baron-pierre-charles-dupin",
    "title": "First Steps into GIS",
    "section": "Baron Pierre Charles Dupin",
    "text": "Baron Pierre Charles Dupin\n\n\n\n\n\n6 October 1784 – 18 January 1873\nmathematician, engineer, economist, politician, mapmaker\n1826 first known choropleth map, Carte figurative de l’instruction populaire de la France, par Charles Dupin\n\nShows availability of education"
  },
  {
    "objectID": "Modules/Class 2.html#carte-figurative-de-linstruction-populaire-de-la-france",
    "href": "Modules/Class 2.html#carte-figurative-de-linstruction-populaire-de-la-france",
    "title": "First Steps into GIS",
    "section": "Carte figurative de l’instruction populaire de la France",
    "text": "Carte figurative de l’instruction populaire de la France"
  },
  {
    "objectID": "Modules/Class 2.html#charles-picquet",
    "href": "Modules/Class 2.html#charles-picquet",
    "title": "First Steps into GIS",
    "section": "Charles Picquet",
    "text": "Charles Picquet\n\nFrench geographer and cartographer\n1832, fighting Cholera in Paris\nUsing shading to represent deaths in Rapport sur la marche et les effets du choléra-morbus dans ParisOffsite Link et les communes rurales du département de la Seine / par la commission nommée, avec l’approbation de M. le ministre du commerce et des travaux publics, par MM. les préfets de la Seine et de police ; année 1832.\nSpatial analysis!"
  },
  {
    "objectID": "Modules/Class 2.html#jon-snow",
    "href": "Modules/Class 2.html#jon-snow",
    "title": "First Steps into GIS",
    "section": "Jon Snow",
    "text": "Jon Snow\n\n\n\n\n\nEarned his spot on the Night’s Watch\nFought white walkers\nSaved lots of people\nWorked for the Queen of the Seven Kingdoms"
  },
  {
    "objectID": "Modules/Class 2.html#john-snow",
    "href": "Modules/Class 2.html#john-snow",
    "title": "First Steps into GIS",
    "section": "John Snow",
    "text": "John Snow\n\n\n\n\n\nEarned his MD\nFought Cholera\nSaved lots of people\nWorked for the Queen of England"
  },
  {
    "objectID": "Modules/Class 2.html#cholera-outbreak-1854",
    "href": "Modules/Class 2.html#cholera-outbreak-1854",
    "title": "First Steps into GIS",
    "section": "Cholera Outbreak 1854",
    "text": "Cholera Outbreak 1854"
  },
  {
    "objectID": "Modules/Class 2.html#charles-joseph-minard",
    "href": "Modules/Class 2.html#charles-joseph-minard",
    "title": "First Steps into GIS",
    "section": "Charles Joseph Minard",
    "text": "Charles Joseph Minard\n\n\n\n\n\n27 March 1781 – 24 October 1870\nFrench  civil engineer\nFlow maps, numerical data on geographic maps\nCreated what some call the best statistical graph of all time, produced in 1869"
  },
  {
    "objectID": "Modules/Class 2.html#carte-figurative-des-pertes-successives-en-hommes-de-larmée-française-dans-la-campagne-de-russie-18121813",
    "href": "Modules/Class 2.html#carte-figurative-des-pertes-successives-en-hommes-de-larmée-française-dans-la-campagne-de-russie-18121813",
    "title": "First Steps into GIS",
    "section": "Carte figurative des pertes successives en hommes de l’Armée Française dans la campagne de Russie 1812–1813",
    "text": "Carte figurative des pertes successives en hommes de l’Armée Française dans la campagne de Russie 1812–1813"
  },
  {
    "objectID": "Modules/Class 2.html#start-of-gis",
    "href": "Modules/Class 2.html#start-of-gis",
    "title": "First Steps into GIS",
    "section": "Start of GIS",
    "text": "Start of GIS\n\n\n\n\n\nRoger Tomlinson (with others!)\nCoined the term Geographic Information System\nHis team created the first computerized GIS for Canadian government\nIdea: Traditional mylar sheets just did not cut it\nSolution: Electronic layers of information"
  },
  {
    "objectID": "Modules/Class 2.html#what-is-a-gis",
    "href": "Modules/Class 2.html#what-is-a-gis",
    "title": "First Steps into GIS",
    "section": "What is a GIS?",
    "text": "What is a GIS?\n\nA geographic information system (GIS) is a type of database containing geographic data (that is, descriptions of phenomena for which location is relevant), combined with software tools for managing, analyzing, and visualizing those data. \n\n– Wikipedia, Geographic Information System"
  },
  {
    "objectID": "Modules/Class 2.html#different-gis-tools",
    "href": "Modules/Class 2.html#different-gis-tools",
    "title": "First Steps into GIS",
    "section": "Different GIS tools",
    "text": "Different GIS tools\n\n\n\nCommercial\n\n\nArcGIS (ESRI)\n\nIndustry leader\nEntire ecosystem\nArcGIS Storymap\nArcGIS online\n\nTableau\n\n\n\nOpen Source\n\n\nQGIS\n\nGRASS\nOmeka\nStoryMap JS\nApache Superset\n\n\n\nOther Tools\n\n\nGoogle Earth\n\nPython\n\nR\nWebsites / WebGIS"
  },
  {
    "objectID": "Modules/Class 2.html#qgis",
    "href": "Modules/Class 2.html#qgis",
    "title": "First Steps into GIS",
    "section": "QGIS",
    "text": "QGIS\n https://www.qgis.org\n\nA Free and Open Source Geographic Information System"
  },
  {
    "objectID": "Modules/Class 2.html#qgis-1",
    "href": "Modules/Class 2.html#qgis-1",
    "title": "First Steps into GIS",
    "section": "QGIS",
    "text": "QGIS\n\n\n\nCapabilities\n\n\nFree\n\nDisplay, analyze, and edit spatial information\n\nCreate digital maps\nSupports common geospatial features\n\nSupports different spatial file formats\n\nCross-platform\n\n\n\n\nDrawbacks\n\n\nNo dedicated free support\n\nNot “industry standard”\nNot part of the ESRI ecosystem\nLacks some of the commercial tools and features"
  },
  {
    "objectID": "Modules/Class 2.html#commonalities",
    "href": "Modules/Class 2.html#commonalities",
    "title": "First Steps into GIS",
    "section": "Commonalities",
    "text": "Commonalities\n\nAll GIS systems use GIS Data\nNeed to project this data onto a representation of the Earth\nConcerned with geographic space\n\nFood for thought: What about place?\n\nNot very great at representing time"
  },
  {
    "objectID": "Modules/Class 2.html#gis-data",
    "href": "Modules/Class 2.html#gis-data",
    "title": "First Steps into GIS",
    "section": "GIS Data",
    "text": "GIS Data\nData for GIS systems is primarily in two forms:\n\nRaster\nVector"
  },
  {
    "objectID": "Modules/Class 2.html#raster-overview",
    "href": "Modules/Class 2.html#raster-overview",
    "title": "First Steps into GIS",
    "section": "Raster Overview",
    "text": "Raster Overview\n\n\n\nGreat For:\n\n\nLarge, continuous data sets\n\nTerrain / Elevation\n\nImages / Photographs\nApplications where attribute data is not extensive\nWhere it is too computationally expensive to model the data\n\n\n\nNot so Great For:\n\n\nFeatures (buildings, peaks, etc)\nExtensive attribute data\nGranular detail\nQuerying for information"
  },
  {
    "objectID": "Modules/Class 2.html#section-2",
    "href": "Modules/Class 2.html#section-2",
    "title": "First Steps into GIS",
    "section": "",
    "text": "Vector\n\n\n\n\n\nWay to represent features\n\nThese can be anything that has a geospatial component\n\nTrace rivers, lakes, buildings, roads, add points, etc\n\nX, Y, and sometimes Z axis\n\nVertices and paths\n\nThink of svg vs png files\n\nPoints, lines, polygons"
  },
  {
    "objectID": "Modules/Class 2.html#vector-file-formats",
    "href": "Modules/Class 2.html#vector-file-formats",
    "title": "First Steps into GIS",
    "section": "Vector file formats",
    "text": "Vector file formats\n\n\n\nShapefile\n\n\nCreated by ESRI\n\nProprietary\n\nDe facto industry standard\nMultiple files\nLimitations on attribute name size\nSpatially indexed\n\n\n\nGeoJSON\n\n\nSubset of the JSON format\nHuman readable\nDesigned for the web\nEssentially a text file\nNo spatial index\nCan have slower performance\n\n\n\nGeoPackage\n\n\nNew kid on the block\nBuilt on SQLlite (essentially a database file)\nSpatially indexed\nFaster performance\nNot as widely supported as the other formats"
  },
  {
    "objectID": "Modules/Class 2.html#section-3",
    "href": "Modules/Class 2.html#section-3",
    "title": "First Steps into GIS",
    "section": "",
    "text": "Shapefile\n\n\n\n\nFile Extension\nFeatures\n\n\n\n\n.shp (required)\nfeature geometry\n\n\n.shx (required)\npositional index\n\n\n.dbf (required)\nattribute information\n\n\n.prj\nprojection description\n\n\n.sbn / .sbx\nspatial index of the features\n\n\n.fbn/.fbx\nspatial index of read only features\n\n\n.ain/.aih\nattribute index\n\n\n.ixs\ngeocoding index for read-write datasets\n\n\n.mxs\ngeocoding index for read-write OBD datasets\n\n\n.atx\ndifferently formatted attribute index\n\n\n.shp.xml\nmetadata in xml format\n\n\n.cpg\nspecifies character encoding\n\n\n.qix\nalternative spatial index"
  },
  {
    "objectID": "Modules/Class 2.html#section-4",
    "href": "Modules/Class 2.html#section-4",
    "title": "First Steps into GIS",
    "section": "",
    "text": "GeoJSON\n\n{\n  \"type\": \"FeatureCollection\",\n  \"features\": [\n    {\n      \"type\": \"Feature\",\n      \"geometry\": {\n        \"type\": \"Point\",\n        \"coordinates\": [102.0, 0.5]\n      },\n      \"properties\": {\n        \"prop0\": \"value0\"\n      }\n    },\n    {\n      \"type\": \"Feature\",\n      \"geometry\": {\n        \"type\": \"LineString\",\n        \"coordinates\": [\n          [102.0, 0.0],\n          [103.0, 1.0],\n          [104.0, 0.0],\n          [105.0, 1.0]\n        ]\n      },\n      \"properties\": {\n        \"prop0\": \"value0\",\n        \"prop1\": 0.0\n      }\n    },\n    {\n      \"type\": \"Feature\",\n      \"geometry\": {\n        \"type\": \"Polygon\",\n        \"coordinates\": [\n          [\n            [100.0, 0.0],\n            [101.0, 0.0],\n            [101.0, 1.0],\n            [100.0, 1.0],\n            [100.0, 0.0]\n          ]\n        ]\n      },\n      \"properties\": {\n        \"prop0\": \"value0\",\n        \"prop1\": { \"this\": \"that\" }\n      }\n    }\n  ]\n}"
  },
  {
    "objectID": "Modules/Class 2.html#section-5",
    "href": "Modules/Class 2.html#section-5",
    "title": "First Steps into GIS",
    "section": "",
    "text": "GeoPackage"
  },
  {
    "objectID": "Modules/Class 2.html#csv",
    "href": "Modules/Class 2.html#csv",
    "title": "First Steps into GIS",
    "section": "CSV",
    "text": "CSV\n\nComma-separated values\nNot inherently spatial or…well…anything\nText file\nSpreadsheets!\nYou will find a LOT of information in this format"
  },
  {
    "objectID": "Modules/Class 2.html#csv-1",
    "href": "Modules/Class 2.html#csv-1",
    "title": "First Steps into GIS",
    "section": "CSV",
    "text": "CSV\n ## Data {data-background=“#005c96”}"
  },
  {
    "objectID": "Modules/Class 2.html#where-can-i-get-data",
    "href": "Modules/Class 2.html#where-can-i-get-data",
    "title": "First Steps into GIS",
    "section": "Where can I get data?",
    "text": "Where can I get data?\n\nNatural Earth\nAncient World Mapping Center GeoData\nLos Angeles GeoHub\nArcGIS Living Atlas of the World\nOpenStreetMap"
  },
  {
    "objectID": "Modules/Class 2.html#where-can-i-get-data-1",
    "href": "Modules/Class 2.html#where-can-i-get-data-1",
    "title": "First Steps into GIS",
    "section": "Where can I get data?",
    "text": "Where can I get data?\nExercise Time!\n\nMillion Dollar Hoods"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to LIB 201: Introduction to Digital Studies",
    "section": "",
    "text": "Monday, 6:00 - 8:45 Fant Library Seminar B (unless otherwise instructed)\n\n\n\nName: Hillary A. H. Richardson\nOffice Hours: by appointment at https://libreserves.muw.edu/appointments/hillary email: hhrichardson@muw.edu\n\n\n\nFor this class, we will be using the corpus of letters in the Smith Papers to interrogate various conversations about race, gender, culture, economics, politics, and interpersonal relationships in rural, 20th century Mississippi. Using various digital humanities methods and tools, we will research the impact of these topics in the Smith Papers in a broader context, and we will display our findings in an online space that is visible to the public. In addition to sharing the findings of our research, we will make our research process visible, sharing raw data files, documenting the steps of our process, and detailing our collaborations.\n\n\n\nStudents will create\n\nDefine, digitize, assemble, and create metadata for a circumscribed dataset\nDesign several small research projects that experiment with various digital tools and methods, creating a strategy for the project and documenting progress toward defined goals\nDemystify underpinnings of digital research projects, such as algorithms, formulas, and metadata\nBecome familiar with various ethical, sociocultural, and technological issues associated with digital research and publishing.\nSynthesize the process of a digital project into a reflective paper that pinpoints the goals of the project, the challenges faced, and strategies used to complete those goals.\n\n\n\n\nLinks and PDFs for selected readings will be posted to the schedule prior to each meeting. There are no required textbooks or purchases for this class.\n\n\n\n\n\n\nAssignments\n% of Grade\n\n\n\n\nWeekly Writings, check-ins, and discussion posts\n30%\n\n\nDigital Prepwork (metadata, transcriptions)\n30%\n\n\nMini-projects\n30%\n\n\nPortfolio/Reflection Essay\n10%"
  },
  {
    "objectID": "index.html#meeting-time-place",
    "href": "index.html#meeting-time-place",
    "title": "Welcome to LIB 201: Introduction to Digital Studies",
    "section": "",
    "text": "Monday, 6:00 - 8:45 Fant Library Seminar B (unless otherwise instructed)"
  },
  {
    "objectID": "index.html#instructor-information",
    "href": "index.html#instructor-information",
    "title": "Welcome to LIB 201: Introduction to Digital Studies",
    "section": "",
    "text": "Name: Hillary A. H. Richardson\nOffice Hours: by appointment at https://libreserves.muw.edu/appointments/hillary email: hhrichardson@muw.edu"
  },
  {
    "objectID": "index.html#course-description",
    "href": "index.html#course-description",
    "title": "Welcome to LIB 201: Introduction to Digital Studies",
    "section": "",
    "text": "For this class, we will be using the corpus of letters in the Smith Papers to interrogate various conversations about race, gender, culture, economics, politics, and interpersonal relationships in rural, 20th century Mississippi. Using various digital humanities methods and tools, we will research the impact of these topics in the Smith Papers in a broader context, and we will display our findings in an online space that is visible to the public. In addition to sharing the findings of our research, we will make our research process visible, sharing raw data files, documenting the steps of our process, and detailing our collaborations."
  },
  {
    "objectID": "index.html#objectives-and-goals",
    "href": "index.html#objectives-and-goals",
    "title": "Welcome to LIB 201: Introduction to Digital Studies",
    "section": "",
    "text": "Students will create\n\nDefine, digitize, assemble, and create metadata for a circumscribed dataset\nDesign several small research projects that experiment with various digital tools and methods, creating a strategy for the project and documenting progress toward defined goals\nDemystify underpinnings of digital research projects, such as algorithms, formulas, and metadata\nBecome familiar with various ethical, sociocultural, and technological issues associated with digital research and publishing.\nSynthesize the process of a digital project into a reflective paper that pinpoints the goals of the project, the challenges faced, and strategies used to complete those goals."
  },
  {
    "objectID": "index.html#texts",
    "href": "index.html#texts",
    "title": "Welcome to LIB 201: Introduction to Digital Studies",
    "section": "",
    "text": "Links and PDFs for selected readings will be posted to the schedule prior to each meeting. There are no required textbooks or purchases for this class."
  },
  {
    "objectID": "index.html#grading",
    "href": "index.html#grading",
    "title": "Welcome to LIB 201: Introduction to Digital Studies",
    "section": "",
    "text": "Assignments\n% of Grade\n\n\n\n\nWeekly Writings, check-ins, and discussion posts\n30%\n\n\nDigital Prepwork (metadata, transcriptions)\n30%\n\n\nMini-projects\n30%\n\n\nPortfolio/Reflection Essay\n10%"
  },
  {
    "objectID": "assignments/reflection.html",
    "href": "assignments/reflection.html",
    "title": "Final Reflection Rubric",
    "section": "",
    "text": "You may share these with me in draft form for comments and feedback before the due date.\n\n\n\nThroughout this class, we have focused on how to use different kinds of digital research to interact with archival documents. We have been balancing the line between automating processes with computers, and using computers to do processes manually. Think about how these processes you’ve employed throughout the semester have done this – transcribing texts with an algorithm AND manually, reading letters individually and having a computer read them all, hypothesizing an output of a text analysis mechanism, sorting and classifying data into different groups, finding and creating knowledge and sharing it in an online folder, etc. You have used the computer to process large amounts of data, and for what?\nIn your reflection essay, think about your experiences in each module of the class, and discuss what is gained through these processes, and possibly, what is lost. How do these processes help your research? How do they impede it?\nFinally, what do all of these processes and outputs mean in your interactions with the Smith Papers Collection? What did these digital and manual processes reveal to you about these 100-year-old conversations?\n\n\n\n\nDiscuss the skills, processes, and tools you had to incorporate for each mini-project\nDiscuss the effect these processes had on your projects (feel free to reference the course readings to help make this argument), and what these digital artifacts allow (or disallow) you to do\nDiscuss the impact these digital processes had on your engagement with the archival collection\n\n\n\n\n• 3-4 pages (1200-1500 words) • Include 2-5 links to (or images of) artifacts you reference (e.g. visualizations from mini-projects, cleaning configurations, transcription errors, etc.) • Submit as a word document to Canvas"
  },
  {
    "objectID": "assignments/reflection.html#due-51-in-lieu-of-final-exam",
    "href": "assignments/reflection.html#due-51-in-lieu-of-final-exam",
    "title": "Final Reflection Rubric",
    "section": "",
    "text": "You may share these with me in draft form for comments and feedback before the due date."
  },
  {
    "objectID": "assignments/reflection.html#assignment-objectives",
    "href": "assignments/reflection.html#assignment-objectives",
    "title": "Final Reflection Rubric",
    "section": "",
    "text": "Throughout this class, we have focused on how to use different kinds of digital research to interact with archival documents. We have been balancing the line between automating processes with computers, and using computers to do processes manually. Think about how these processes you’ve employed throughout the semester have done this – transcribing texts with an algorithm AND manually, reading letters individually and having a computer read them all, hypothesizing an output of a text analysis mechanism, sorting and classifying data into different groups, finding and creating knowledge and sharing it in an online folder, etc. You have used the computer to process large amounts of data, and for what?\nIn your reflection essay, think about your experiences in each module of the class, and discuss what is gained through these processes, and possibly, what is lost. How do these processes help your research? How do they impede it?\nFinally, what do all of these processes and outputs mean in your interactions with the Smith Papers Collection? What did these digital and manual processes reveal to you about these 100-year-old conversations?"
  },
  {
    "objectID": "assignments/reflection.html#to-sum-up",
    "href": "assignments/reflection.html#to-sum-up",
    "title": "Final Reflection Rubric",
    "section": "",
    "text": "Discuss the skills, processes, and tools you had to incorporate for each mini-project\nDiscuss the effect these processes had on your projects (feel free to reference the course readings to help make this argument), and what these digital artifacts allow (or disallow) you to do\nDiscuss the impact these digital processes had on your engagement with the archival collection"
  },
  {
    "objectID": "assignments/reflection.html#paper-requirements",
    "href": "assignments/reflection.html#paper-requirements",
    "title": "Final Reflection Rubric",
    "section": "",
    "text": "• 3-4 pages (1200-1500 words) • Include 2-5 links to (or images of) artifacts you reference (e.g. visualizations from mini-projects, cleaning configurations, transcription errors, etc.) • Submit as a word document to Canvas"
  },
  {
    "objectID": "assignments/NA.html",
    "href": "assignments/NA.html",
    "title": "Network Analysis Rubric",
    "section": "",
    "text": "Follow Network Analysis lessons 3 - 4 for a detailed step-by-step for how to model your data for a network analysis. Submit a document with an image file (or several image files) of your network analysis in Palladio, and include an explanation (about 1-2 pages) of what’s going on in the graph, which should give:\n\na brief introduction to the collection of letters, or who/what the network represents, and\na description what the nodes and edges represent within the greater network of the collection of letters.\nany insights you can provide on relationships or nodes to highlight (i.e. notes on who might have the highest degree centrality and why, or which node might serve as a bridge, etc.). If you need to zoom in on parts of the image to do this, include a zoomed-in screenshot of part of the network so that your explanation has a visual aid.\n\n\n\n\nNetwork Analyses that get full credit will be evaluated based on the explanation, the dataset, and the visualization itself.\n\n\n\nAdequately explains how the dataset was modeled and the methods of cleaning data, and links to the original data set\nMakes inferences about the patterns and anomalies in the visualization (e.g. who were the influential players, what the outlying subgroups mean, etc.)\n\n\n\n\n\nis as free from duplicates as possible (keeping “Against Cleaning” in mind!)\nAvoids copy and paste errors. Work done together maintains the “tidiness” of the dataset.\n\n\n\n\n\nContains an images of the network that are clearly visible and demonstrate the analysis from the paragraph\nUses edges and node sizing to show impact of different people"
  },
  {
    "objectID": "assignments/NA.html#submit",
    "href": "assignments/NA.html#submit",
    "title": "Network Analysis Rubric",
    "section": "",
    "text": "Follow Network Analysis lessons 3 - 4 for a detailed step-by-step for how to model your data for a network analysis. Submit a document with an image file (or several image files) of your network analysis in Palladio, and include an explanation (about 1-2 pages) of what’s going on in the graph, which should give:\n\na brief introduction to the collection of letters, or who/what the network represents, and\na description what the nodes and edges represent within the greater network of the collection of letters.\nany insights you can provide on relationships or nodes to highlight (i.e. notes on who might have the highest degree centrality and why, or which node might serve as a bridge, etc.). If you need to zoom in on parts of the image to do this, include a zoomed-in screenshot of part of the network so that your explanation has a visual aid."
  },
  {
    "objectID": "assignments/NA.html#rubric",
    "href": "assignments/NA.html#rubric",
    "title": "Network Analysis Rubric",
    "section": "",
    "text": "Network Analyses that get full credit will be evaluated based on the explanation, the dataset, and the visualization itself.\n\n\n\nAdequately explains how the dataset was modeled and the methods of cleaning data, and links to the original data set\nMakes inferences about the patterns and anomalies in the visualization (e.g. who were the influential players, what the outlying subgroups mean, etc.)\n\n\n\n\n\nis as free from duplicates as possible (keeping “Against Cleaning” in mind!)\nAvoids copy and paste errors. Work done together maintains the “tidiness” of the dataset.\n\n\n\n\n\nContains an images of the network that are clearly visible and demonstrate the analysis from the paragraph\nUses edges and node sizing to show impact of different people"
  },
  {
    "objectID": "assignments/dig-prep.html",
    "href": "assignments/dig-prep.html",
    "title": "Digital Prep Rubric",
    "section": "",
    "text": "Your work with individually assigned letters from the Smith Papers Collection will be the basis for this assigment. You will submit this work across 3 different submissions: - Metadata (worth 50 points) - Transcriptions (worth 100 points) - Tags (worth 50 points)\n\n\nFrom our Google Sheet, copy and paste the full metadata for the letters you were transcribing into a blank spreadsheet, and submit the .xlsx file to Canvas. Metadata will be checked for fullness (all the fields are filled in) and consistency (they are all formatted according to the guidelines).\nMetadata that receives full credit will: - complete all metadata fields, following Smith Paper Guidelines formatting consistently - use Library of Congress subject headings that are relevant and appropriate - Consider and note harmful content in the letters within appropriate fields\n\n\n\nExport and upload the .txt files of your assigned transcriptions. The file names should follow our guidelines (mcj-dp018….txt), and the transcriptions should be complete and thorough, representing the letters as faithfully as possible (see Transcribing lesson 1!\nTranscriptions that receive full credit will: - Maintain structural integrity of the original document through Transkribus’ layout analysis tool - Transcribe the letters fully and as closely to the original letter as possible, keeping original spelling, markings (where appropriate), punctuation, etc.\n\n\n\nUpload the metadata tags (e.g. person, place, organization, etc.) you created for each letter you transcribed. The exported tags will be in an .xls or .xslx format. They should have multiple tabs per sheet for each tag named (e.g. person, place, organization, etc. Each tag category has a separate tab), and there should be an “overview” tab for all tags. It will look something like this:\n\n\n\nScreenshot of example tags exported from Transkribus and displayed in Google Sheets\n\n\nTags that receive full credit will: - tag all words as explained in our Tag Definitions list."
  },
  {
    "objectID": "assignments/dig-prep.html#metadata",
    "href": "assignments/dig-prep.html#metadata",
    "title": "Digital Prep Rubric",
    "section": "",
    "text": "From our Google Sheet, copy and paste the full metadata for the letters you were transcribing into a blank spreadsheet, and submit the .xlsx file to Canvas. Metadata will be checked for fullness (all the fields are filled in) and consistency (they are all formatted according to the guidelines).\nMetadata that receives full credit will: - complete all metadata fields, following Smith Paper Guidelines formatting consistently - use Library of Congress subject headings that are relevant and appropriate - Consider and note harmful content in the letters within appropriate fields"
  },
  {
    "objectID": "assignments/dig-prep.html#transcriptions",
    "href": "assignments/dig-prep.html#transcriptions",
    "title": "Digital Prep Rubric",
    "section": "",
    "text": "Export and upload the .txt files of your assigned transcriptions. The file names should follow our guidelines (mcj-dp018….txt), and the transcriptions should be complete and thorough, representing the letters as faithfully as possible (see Transcribing lesson 1!\nTranscriptions that receive full credit will: - Maintain structural integrity of the original document through Transkribus’ layout analysis tool - Transcribe the letters fully and as closely to the original letter as possible, keeping original spelling, markings (where appropriate), punctuation, etc."
  },
  {
    "objectID": "assignments/dig-prep.html#tags",
    "href": "assignments/dig-prep.html#tags",
    "title": "Digital Prep Rubric",
    "section": "",
    "text": "Upload the metadata tags (e.g. person, place, organization, etc.) you created for each letter you transcribed. The exported tags will be in an .xls or .xslx format. They should have multiple tabs per sheet for each tag named (e.g. person, place, organization, etc. Each tag category has a separate tab), and there should be an “overview” tab for all tags. It will look something like this:\n\n\n\nScreenshot of example tags exported from Transkribus and displayed in Google Sheets\n\n\nTags that receive full credit will: - tag all words as explained in our Tag Definitions list."
  },
  {
    "objectID": "assignments/map-rubric.html",
    "href": "assignments/map-rubric.html",
    "title": "Spatial/Temporal Data Assignment Rubric",
    "section": "",
    "text": "For this module, Maps and Timelines are ways for you to engage with the historical and contextual research of the collection. In groups, you will use either a map or timeline to show the progression of events or ideas, either over time or space, and to make an argument about these occurrences. For this assignment, you’ll identify a topic within the letters of noted historical significance, research the broader impact of this topic, and put the conversations from the letters within this context.\nRemember: Your project is more than a list of things! It’s a visual argument about how/why they occurred."
  },
  {
    "objectID": "assignments/map-rubric.html#research",
    "href": "assignments/map-rubric.html#research",
    "title": "Spatial/Temporal Data Assignment Rubric",
    "section": "Research:",
    "text": "Research:\n\nDefines the research question in both the headline of the timeline and the written accompaniment that goes beyond “a series of events” to explore the change over time/space\nCites Smith Papers at least 5 times, using relevant examples and excerpts\nCites at least 5 secondary sources, using relevant contextual information to ground research question"
  },
  {
    "objectID": "assignments/map-rubric.html#timelinemap-project",
    "href": "assignments/map-rubric.html#timelinemap-project",
    "title": "Spatial/Temporal Data Assignment Rubric",
    "section": "Timeline/Map project:",
    "text": "Timeline/Map project:\n\nContains various media (e.g. images, sound, video, etc.) in a majority of entries\nCredits media (both within the Smith Papers Collection and that which is found online) fully\nContextualizes events with secondary source information within the description text of appropriate entries.\n\nTimeline only:\n\nUses at least 2 “groups” to put events into categories\n\nMap only:\n\nUses additional shapefile data to enrich the individually created entries"
  },
  {
    "objectID": "assignments/narrative-rubric.html",
    "href": "assignments/narrative-rubric.html",
    "title": "Narrative Portfolio Rubric",
    "section": "",
    "text": "Overview\nYou will create a digital exhibit that puts the research we’ve collected and the artifacts we’ve created in class into one project that tells a story about the Smith Papers Collection. You are synthesizing the items in the collection with what we’ve learned so far, and creating a digital story about it.\n\n\nDue:\n\n\nRubric\nSuccessful narrative projects will be evaluated on the following criteria: - Shows improvements on mini-projects from original submission - Uses narrative elements (text, links, and images) to weave together the mini-projects - Uses secondary sources to strengthen the validity of the argument within the narrative and mini-projects - Shares evidence of the digital scholarship (e.g. stop words lists, raw/clean data, explanation of process, etc.) as often and as necessary as possible - Provides ample text and explanation for all required sections - Uses markdown (and/or HTML if applicable) language to create a structured and dynamic digital exhibit"
  },
  {
    "objectID": "assignments/TA.html",
    "href": "assignments/TA.html",
    "title": "Text Analysis Rubric",
    "section": "",
    "text": "The Text Analysis assignment is a way for you to engage with the content (or, the text!) of the collection. The point of this assignment is not to prove anything through text analysis tools, but to ask questions and demonstrate the process you undertook to address those questions using different tools. Think of your text anlysis assignment as a scientific experiment that uses words as its test subject!\n\n\n\n\n\nText Analyses that get full credit will be evaluated based on 4 sections of the assignment: the research question, the methods, the results, and further research.\n\n\n\nClearly defines the purpose of the text analysis in the form of a research question\nCites at least 2 secondary sources to establish a contextual foundation for the question\nCites at least 3 primary sources, using them as evidence to reinforce claims made as a result of the text analysis\n\n\n\n\n\nClearly define the parameters for inclusion and exclusion of the content set\nExplain the choice of 2 text analysis methods with sound justifications\nProvide detailed cleaning methods for the content set\n\n\n\n\n\n\nDemonstrate the experiment’s results with accessibly visual images from text analysis tools\nProvide insight and interpretation into the results of each analysis tool\n\n\n\n\n\nClearly defines the limits of the study and how they affected the current results\nOutlines a concrete path for a future study as a result of the current study’s limitations"
  },
  {
    "objectID": "assignments/TA.html#rubric",
    "href": "assignments/TA.html#rubric",
    "title": "Text Analysis Rubric",
    "section": "",
    "text": "Text Analyses that get full credit will be evaluated based on 4 sections of the assignment: the research question, the methods, the results, and further research.\n\n\n\nClearly defines the purpose of the text analysis in the form of a research question\nCites at least 2 secondary sources to establish a contextual foundation for the question\nCites at least 3 primary sources, using them as evidence to reinforce claims made as a result of the text analysis\n\n\n\n\n\nClearly define the parameters for inclusion and exclusion of the content set\nExplain the choice of 2 text analysis methods with sound justifications\nProvide detailed cleaning methods for the content set"
  },
  {
    "objectID": "assignments/TA.html#the-results",
    "href": "assignments/TA.html#the-results",
    "title": "Text Analysis Rubric",
    "section": "",
    "text": "Demonstrate the experiment’s results with accessibly visual images from text analysis tools\nProvide insight and interpretation into the results of each analysis tool"
  },
  {
    "objectID": "assignments/TA.html#further-research",
    "href": "assignments/TA.html#further-research",
    "title": "Text Analysis Rubric",
    "section": "",
    "text": "Clearly defines the limits of the study and how they affected the current results\nOutlines a concrete path for a future study as a result of the current study’s limitations"
  },
  {
    "objectID": "Modules/Class 1.html#overview",
    "href": "Modules/Class 1.html#overview",
    "title": "DGT HUM 131: Digital Mapping",
    "section": "Overview",
    "text": "Overview\n\nIntroductions\nCourse overview\n\nCourse policies\nCourse resources\nGrading\nProject overview\n\nWhat are we studying and why?\n\nKey concepts"
  },
  {
    "objectID": "Modules/Class 1.html#introductions",
    "href": "Modules/Class 1.html#introductions",
    "title": "DGT HUM 131: Digital Mapping",
    "section": "Introductions",
    "text": "Introductions\n\nDr. Ryan Horne, research consultant in the Office of Advanced Research Computing (OARC) and instructor with the DH program\nInformation Sciences / History double major at Penn State (we had electricity then!)\nSoftware Engineer\nBack to Academia, MA in Ancient History at UCSB, PhD in Ancient History at UNC Chapel Hill\nVarious postdocs and research positions since then\nDo a little bit of all DH; specialties in geospatial, networks, and computational humanities"
  },
  {
    "objectID": "Modules/Class 1.html#introductions-1",
    "href": "Modules/Class 1.html#introductions-1",
    "title": "DGT HUM 131: Digital Mapping",
    "section": "Introductions",
    "text": "Introductions\nPartner up!\n\nWho they are\nMajor\nWhat is a space / place that you feel connected to and why?"
  },
  {
    "objectID": "Modules/Class 1.html#course-policies",
    "href": "Modules/Class 1.html#course-policies",
    "title": "DGT HUM 131: Digital Mapping",
    "section": "Course Policies",
    "text": "Course Policies\n\nNo late projects!\nIf something happens for a reaction or other assignment, let me know\nExpected to be here and attentive\nParticipate in discussions from the reading s and activities\nParticipate in the labs\n\nData for labs is up to you!"
  },
  {
    "objectID": "Modules/Class 1.html#course-policies-1",
    "href": "Modules/Class 1.html#course-policies-1",
    "title": "DGT HUM 131: Digital Mapping",
    "section": "Course Policies",
    "text": "Course Policies\nDiscussion guidelines\n\nListen attentively. Pay attention to what someone is saying, please no interrupting.\nNo name calling. Be respectful of people, and focus on challenging ideas.\nGive and take air time. Who else may be waiting to share their perspective?\nConfidentiality. You should not use what is discussed in this classroom in outside conversations.\nIt is okay to make mistakes!!"
  },
  {
    "objectID": "Modules/Class 1.html#course-resources",
    "href": "Modules/Class 1.html#course-resources",
    "title": "DGT HUM 131: Digital Mapping",
    "section": "Course Resources",
    "text": "Course Resources\n\nCourse materials will be on BruinLearn or linked directly from the schedule\nIf you do not have a laptop, let me know!\n\nYou can e-mail me if you do not want to speak directly about this"
  },
  {
    "objectID": "Modules/Class 1.html#course-overview",
    "href": "Modules/Class 1.html#course-overview",
    "title": "DGT HUM 131: Digital Mapping",
    "section": "Course Overview",
    "text": "Course Overview\n\nIn this course, students will study digital mapping techniques used in humanities and social sciences, with an emphasis on equity and social justice issues. Through project-based learning, students will engage about basic geospatial data types, digital gazetteers, and web-based GIS initiatives. Students will work with georeferencing historical maps and sources, visualizing and querying geospatial data, and crafting narratives that use place-based information."
  },
  {
    "objectID": "Modules/Class 1.html#what-will-we-be-doing",
    "href": "Modules/Class 1.html#what-will-we-be-doing",
    "title": "DGT HUM 131: Digital Mapping",
    "section": "What will we be doing?",
    "text": "What will we be doing?\n\nIn-class exercises and learning\nReadings\nReaction Papers\nProject"
  },
  {
    "objectID": "Modules/Class 1.html#what-does-this-mean",
    "href": "Modules/Class 1.html#what-does-this-mean",
    "title": "DGT HUM 131: Digital Mapping",
    "section": "What does this mean?",
    "text": "What does this mean?\nThink of this course as a humanities lab course\n\nHands on (bring your laptops!)\nApplying technical skills and approaches to social justice\n\nHow do we apply what we are learning to SJ concepts? Your project?\n\nLabs are intended to teach and reinforce GIS skills. We can speed up or slow down\n\nTime for discussion as well"
  },
  {
    "objectID": "Modules/Class 1.html#readings",
    "href": "Modules/Class 1.html#readings",
    "title": "DGT HUM 131: Digital Mapping",
    "section": "Readings",
    "text": "Readings\n\nReadings are for the day they appear on the modules\nNo text to purchase!\nReadings provide perspective, background information, and set the tone for our discussions\nSome of these are controversial, some are challenging, but all relate to GIS and SJ."
  },
  {
    "objectID": "Modules/Class 1.html#slides",
    "href": "Modules/Class 1.html#slides",
    "title": "DGT HUM 131: Digital Mapping",
    "section": "Slides",
    "text": "Slides\n\nIntended as a guide to the discussion and to keep us (well, me) on track\nThere will be more images, maps, and interactions!\nLab slides will be available on BruinLearn after the lab"
  },
  {
    "objectID": "Modules/Class 1.html#grading",
    "href": "Modules/Class 1.html#grading",
    "title": "DGT HUM 131: Digital Mapping",
    "section": "Grading",
    "text": "Grading\n\nScale and breakdown on BruinLearn\nNo written exams\nNo written final\nReaction papers\nProject due 5:00 pm on the last day of finals"
  },
  {
    "objectID": "Modules/Class 1.html#reaction-papers",
    "href": "Modules/Class 1.html#reaction-papers",
    "title": "DGT HUM 131: Digital Mapping",
    "section": "Reaction papers",
    "text": "Reaction papers\n\nAt least 500 words\n4 out of 5 graded\nDue (uploaded to BruinLearn) before the discussion\nThese are very important!"
  },
  {
    "objectID": "Modules/Class 1.html#project",
    "href": "Modules/Class 1.html#project",
    "title": "DGT HUM 131: Digital Mapping",
    "section": "Project",
    "text": "Project\n\nMade with ESRI StoryMaps platform\nThis is not a course where we will be learning Javascript or python\nIncorporate feedback from your presentation\nIntentions for the project:\n\nDemonstrate you can use QGIS to effectively deal with spatial data\nUse StoryMaps to make that data accessible to the public"
  },
  {
    "objectID": "Modules/Class 1.html#presentation",
    "href": "Modules/Class 1.html#presentation",
    "title": "DGT HUM 131: Digital Mapping",
    "section": "Presentation",
    "text": "Presentation\n\n5 minutes to cover the project\nStart a discussion with your summary\nRead and provide feedback to others"
  },
  {
    "objectID": "Modules/Class 1.html#why-space-place-and-social-justice",
    "href": "Modules/Class 1.html#why-space-place-and-social-justice",
    "title": "DGT HUM 131: Digital Mapping",
    "section": "Why space, place, and social justice?",
    "text": "Why space, place, and social justice?\n\nWhat do you think of when you hear these terms?\nWhat about their applicability to UCLA?"
  },
  {
    "objectID": "Modules/Class 1.html#spatial-social-justice",
    "href": "Modules/Class 1.html#spatial-social-justice",
    "title": "DGT HUM 131: Digital Mapping",
    "section": "Spatial / Social Justice",
    "text": "Spatial / Social Justice\n\nContention 1: Inequality is rampant and growing in many areas\n\nIncome bracket, race, etc\n\nContention 2: This type of inequality is not beneficial to society or democratic / representative government\n\nThis is in addition to moral and ethical problems with growing inequality"
  },
  {
    "objectID": "Modules/Class 1.html#spatial-social-justice-1",
    "href": "Modules/Class 1.html#spatial-social-justice-1",
    "title": "DGT HUM 131: Digital Mapping",
    "section": "Spatial / Social Justice",
    "text": "Spatial / Social Justice\nFrom the Pew Research Group"
  },
  {
    "objectID": "Modules/Class 1.html#spatial-social-justice-2",
    "href": "Modules/Class 1.html#spatial-social-justice-2",
    "title": "DGT HUM 131: Digital Mapping",
    "section": "Spatial / Social Justice",
    "text": "Spatial / Social Justice\nFrom the Board of the Governors of the Federal Reserve System"
  },
  {
    "objectID": "Modules/Class 1.html#spatial-social-justice-3",
    "href": "Modules/Class 1.html#spatial-social-justice-3",
    "title": "DGT HUM 131: Digital Mapping",
    "section": "Spatial / Social Justice",
    "text": "Spatial / Social Justice\n\n\nContention 3: Place/space play a critical role in inequality\n\nWhy? What does space and place have to do with social justice?\n\nConcept of Spatial Justice\n\nIn a specific, urban sense focuses on distributive and procedural justice"
  },
  {
    "objectID": "Modules/Class 1.html#distributive-justice",
    "href": "Modules/Class 1.html#distributive-justice",
    "title": "DGT HUM 131: Digital Mapping",
    "section": "Distributive Justice",
    "text": "Distributive Justice\n\n “…the fair distribution of benefits and burdens in spatial development and how is it decided upon.”\n\n\nhttps://spatialjustice.blog/distributive-spatial-justice/, paraphrasing John Rawls"
  },
  {
    "objectID": "Modules/Class 1.html#distributive-justice-1",
    "href": "Modules/Class 1.html#distributive-justice-1",
    "title": "DGT HUM 131: Digital Mapping",
    "section": "Distributive Justice",
    "text": "Distributive Justice\n\nFair distribution of burdens and benefits of social interaction in space\n\nsought through the creation, fair allocation of and access to public goods, resources and services throughout the city.\n\nWhat does this mean for Los Angeles? Can you think of any examples?"
  },
  {
    "objectID": "Modules/Class 1.html#procedural-justice",
    "href": "Modules/Class 1.html#procedural-justice",
    "title": "DGT HUM 131: Digital Mapping",
    "section": "Procedural Justice",
    "text": "Procedural Justice\n\nHow the burdens and benefits of social interactions in space are decided upon\n\nHow cities and communities are negotiated, planned, designed and managed\n\nAre all of these decisions made in a transparent manner?\n\nWhat voices are included in planning and decision making, which are not?\n\nEven if they are heard, can these voices have a real impact on decision making?\n\n\nHow does this apply to the LA area? Can you think of any historical examples?"
  },
  {
    "objectID": "Modules/Class 1.html#los-angeles-and-spatial-justice",
    "href": "Modules/Class 1.html#los-angeles-and-spatial-justice",
    "title": "DGT HUM 131: Digital Mapping",
    "section": "Los Angeles and Spatial Justice",
    "text": "Los Angeles and Spatial Justice\nFood inequality\n\nfrom LA County Department of Public Helath\n\nFood insecurity is more likely to occur among racial and ethnic minorities and low-income communities\nDue to food options, “…are also at increased risk for poorer health in the long run…”\nMay also lack access to public transportation\nFood desert: Majority of the population lives over 1 mile away from an affordable grocery store; in rural areas, the distance is 10 miles.\nhttps://www.ers.usda.gov/data-products/food-access-research-atlas/go-to-the-atlas/"
  },
  {
    "objectID": "Modules/Class 1.html#ucla-and-spatialsocial-justice",
    "href": "Modules/Class 1.html#ucla-and-spatialsocial-justice",
    "title": "DGT HUM 131: Digital Mapping",
    "section": "UCLA and spatial/social justice",
    "text": "UCLA and spatial/social justice\n\nhttps://chancellor.ucla.edu/messages/acknowledging-native-peoples-ucla-events/\nhttps://communityengagement.ucla.edu/programs/los-angeles-data-justice-hub/\nhttps://mila.ss.ucla.edu/\nWe can, and should, do more"
  },
  {
    "objectID": "Modules/Class 3.html#what-is-social-justice",
    "href": "Modules/Class 3.html#what-is-social-justice",
    "title": "Social and Spatial Justice",
    "section": "What is Social Justice?",
    "text": "What is Social Justice?"
  },
  {
    "objectID": "Modules/metadata.html",
    "href": "Modules/metadata.html",
    "title": "Introduction to Metadata",
    "section": "",
    "text": "In this lesson, you will: - Understand what metadata are, and why they’re important - Understand the importance of consistency in metadata through metadata standards - Interact with a set of metadata standards - Locate and choose Library of Congress Subject Headings as a type of metadata - Create your own metadata for items in the Smith Papers Collection\n\n\n\n\nMetadata are the building blocks of digital scholarship! We will be creating, structuring, and using metadata throughout this course to participate in digital research and learn more about our collection.\nAs you transcribe, tag, and create information (aka data!) about the letters in the Smith Papers Collection, you are essentially creating metadata. Metadata is just structured “data about data,” and it’s something archivists and librarians use to describe, preserve, organize, and digitize. Furthermore, metadata makes items in a collection easy to find and discover.Metadata makes the internet work! And for this class, it is one of the building blocks of digital research.\nEach item in the result list above contains several bits of metadata. When you search for something with combinations of keywords, filters, search facets, etc., you are tapping the metadata that someone created in order to return results. This applies to most things in a digital collection. \n[(https://login.libprxy.muw.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&bquery=pizza&cli0=RV&clv0=Y&type=1&searchMode=And&site=eds-live&scope=site)]\nThink about the following digital items, and imagine what kinds of metadata - explicit or hidden - that you might interact with when you use them. Then think about what kinds of data and metadata (titles, descriptions, links, stylistic attributes, etc.) had to be collected to create each of the results in these collections.\n\n\n\nimage of a twitter feed\n\n\n\n\n\nscreenshot of the YouTube homepage\n\n\nNow, think about this as it pertains to a box of letters that have been scanned. In order for someone to digitally interact with these objects, they need metadata.\n\n\n\nscreenshot of an envelope and a letter from the Ellard-Murphree-Pilgreen Smith letters\n\n\n\n\nComplete the 1.Metadata Check-in before moving to the next section.\n\n\n\n\nSimilar to citation guidelines (e.g. MLA, APA, Chicago, etc.), digital items use a schema to organize and standardize their formatting. For instance, websites use HTML and CSS to communicate things to browsers, library databases use specific fields to link similar online articles to each other, and so on. This is a way to classify different kinds of metadata, define the information for each kind, and set rules for each kind (e.g. whether or not the field requires certain information or not).\n\n\n\nFor the Smith Papers collection, we are using a common metadata standard/schema called Dublin Core, which takes its name from Dublin, OH where it was created. It’s used by many libraries and cultural heritage institutions to describe both digital and physical objects. In doing this, we: * give the collection structure * link it to similar items * provide consistency across items within the collection\nIn other words, our metadata standards give guidelines for how to treat the 100+ ways to describe something! Think about the different ways you could write your own name, let alone someone else’s you haven’t met!\n\n\n\ndifferent ways to write the same name\n\n\nOpen our Smith Paper Metadata Guidelines in another tab, and look at the different field names in the column headings.\n\n\nComplete the 2.Metadata Check-in before moving on to the next section.\n\n\n\n\nAs you noticed in the guidelines for the Smith Collection, we have a BUNCH of categories (or “fields”) to describe one letter:\n\n&lt;tr&gt;\n    &lt;td&gt;Creator\n    &lt;td&gt;Date\n    &lt;td&gt;Time Period\n&lt;/tr&gt;\n&lt;tr&gt;\n    &lt;td&gt;Subject\n    &lt;td&gt;Mississippi County\n    &lt;td&gt;Geographic Location\n&lt;/tr&gt;\n&lt;tr&gt;\n    &lt;td&gt;Resource Type\n    &lt;td&gt;Format\n    &lt;td&gt;Publisher\n&lt;/tr&gt;\n&lt;tr&gt;\n    &lt;td&gt;Notes\n    &lt;td&gt;Rights\n    &lt;td&gt;Collection\n&lt;/tr&gt;\n&lt;tr&gt;\n    &lt;td&gt;Source\n    &lt;td&gt;Digital Repository\n    &lt;td&gt;Date Digital\n&lt;tr&gt;\n    &lt;td&gt;Capture Method\n    &lt;td&gt;Master Image\n    &lt;td&gt;Record created by\n&lt;tr&gt;\n    &lt;td&gt;File name\n&lt;/tr&gt;\n\n\nIdentifier\n\nTitle\n\nDescription\n\n\n\n\nWhat do all of these mean? Some of these items capture unique information about the specific letter: * Identifier * Title * Description * Creator * Date * Notes * File name\nSome of them capture information that link them to other items in the collection (and potentially outside the collection): * Time Period * Subject * Mississippi County * Geographic Location * Resource Type * Format * Publisher\nAnd lastly, some of them capture information about the digitization process, in the event that the digital record disappears: * Digital Repository * Date Digital * Capture Method * Master Image * Record created by\nSome of these fields are consistent, and will say one thing, or just be limited to one of select options (e.g. Capture Method, Digital Repository, Resource Type, etc.). And some will require your discretion (e.g. Description, Subject Heading, etc.)\n\n\nUsing this letter and the full guidelines, enter the metadata you’d create for the given fields for the 3. Metadata check-inbefore moving on to the next section.\n\n\n\n\n\n\nYou’ll notice there are a lot of ways we describe each letter. Whew! So why subject headings? Subject Headings, like metadata standards, use “controlled vocabulary,” meaning you can group things together by category, so they’ll be  linked by topic. For instance, if I used the subject heading “Dating (Social Customs)” – and we do! – all of the letters with that designation can be grouped together. Books in the library are sorted and grouped by these subject headings, so we can explore information on the same topic in one physical location. What kind of books do you think are next to the one below? What kind of books do you think are next to this one?\n\n\n\nimage of a book record in the catalog for Ellen S. Woodward: New Deal Advocate for women by Martha Swain, with subject headings highlighted\n\n\n\n\n\nThere are a number of pre-determined subject headings defined by the Library of Congress, but how do we decide which subject heading to use when we are the ones coming up with them on our own? Subject headings can be confusing because there are so many of them, and just like words, you can use any number of words to describe something! In order to help with this for the Smith Papers collection, we have gathered several subject headings that are common to the collection. So your first step to choosing a subject heading will be to check this list:\nSmith Papers Authority headings\nIf the contents of your letter do not quite go with the suggested headings, then your next step is to look something up with similar topics, and choose a subject heading from there. Here’s what that looks like:\n\nGo to the MUW Library Online Catalog and search for a book using keywords that describe the topic. We’ll use the example “movies,” since the Smiths often talk about going to see a movie in many of their letters. Type movies in the search box.\nScroll through the results, and find something that is most similar to your concept. Keep it as simple as possible, and go with what seems most prevalent. Note all of the different ways people have categorized movies, and how these terms range in meaning and specificity:\n\nFilm\nMotion pictures\nFilm & Video\nDrama\nComedy\nCinematography\n\n\nIn this example, Motion pictures is broadly used enough in this list of results, and it captures the meaning that the people in the letters are using, since they are talking about going to a movie theatre to see a motion picture, so we’ll go with that!\n\n\n\nRemember: people create metadata, and even though they have mostly the best intentions, they also have biases, implicit or explicit. Metadata and metadata standards have been created by people with a notably biased frame of reference. This includes subject headings.\nFor many years, the Library of Congress classified books on homosexuality under “sexual deviance.” Recently, an undergraduate student at Dartmouth College began a campaign to change the “illegal alien” subject heading to “noncitizen,” resulting in a resolution from the American Library Association that has ended its use. Another example is the subject heading “Minorities” in our catalog. Not only is it not very descriptive, it’s also a loaded term that de-humanizes a group of people.\nYour job in choosing a subject heading is to create a helpful category for users who are interacting with items in a collection. If you came across a folder of photographs labeled “minorities,” what would you expect to find? Even if you didn’t know what was in there, what would be more specific, more helpful, or more appropriate? As you find and create metadata, consider the words you use and the different connotations of their meanings.\n\n\n\nComplete the 4.Metadata check-in before completing the final section.\n\n\n\n\nCreating metadata on your own can be a lot of work! But we are working as a group! Let’s discuss what we’ll need to do as a group in class, and consider these parts of project and data management that are embedded in creating digital scholarship.\n\n\nHow do you document your progress so someone could pick up where you leave off and won’t duplicate your work? It is important to document your progress for yourself (your future self!), but also for those who are working in the same collection alongside you. What usually works for you (e.g. notes in a journal, putting dates on a folder on your computer, etc.) doesn’t always work for others!\n\n\n\n\nThere are lots of ways to keep up with the progress of group projects. Often, a project manager will keep track of progress by checking in with people, going over updated tasks, and reporting progress notes at regular meetings. For keeping up with tasks and due dates, there are several free, online tools, like: - Asana - Google Sheets - Trello\nFor this class, we will use Asana as a group, and it will be up to you to keep up with your progress as an individual. I can’t recommend note-taking enough!\n\n\n\nscreenshot of project management application, asana, showing complete and incomplete tasks\n\n\n\n\nEssentially, we are creating a high-quality digital archival record for a physical item. There’s a lot that goes into that! Consider the following: * How do you preserve access so other people can access them? * How do you preserve the different types and formats of data you’re working with?\nSome data types that will come from our project include: * Non-digital text (handwritten notes, sketches) * Digital texts or digital copies of text (.txt, .pdf, .md, .doc, .otf, .rtf, .tei, etc.) * Data visualizations (.png, .jpg, .svg) * Spreadsheets (e.g. .xlsx, .csv) * Geographic Information Systems (GIS) and spatial data (e.g. .shp, .dbf, .shx, .kml) * Digital copies of images (e.g. .png, .jpeg, .tiff) * Web files (e.g. .html, .css, .md, .yml)\nOther questions to consider: * How and where do you save backup copies? * How do you ensure your work gets credit? * How do you ensure quality control (i.e. consistency in the metadata, image quality, etc.)? * How do you recognize and protect the integrity of a document with potentially harmful or sensitive information in it? * Are we publishing the documents somewhere? Do we have persmission to do that?\nMany projects consider these questions (and more) as a part of a Data Management Plan. If a project is grant funded, sometimes the grant will require a DMP. Here are some examples: * Digital Library Federation Sample Plans (Download the .doc file to see the actual plan) * ICSPR Guidelines for Effective Data Management Plans (Pages 1-10)\n\n\n\nYou won’t always be working on the collection at the same time, so how do you talk to each other outside of class time? How do you leave messages, and what are the expectations for responding? We will discuss how to do this as a class, and set up a channel for communication.\n\n\n\nscreenshot of Microsoft Teams, showing a conversation about a the Smith Papers\n\n\n\n\n\nConsider the principals under which you’ll operate as a cohesive group. Many projects are defined at the outset by practices that its members adopt and value as a group. How we collaborate together will be driven by specific values. Consider some values statements from the following projects: - Colored Conventions Project Values Statement - Smith Papers Project and Data Management - “Pledges,” from Scott Weingart’s blog\n\n\n\nUse these 4 sections - Progress, Preservation, Communication, and Principles - to answer the questions in the Data and Project Management Plan in Canvas to complete the metadata module."
  },
  {
    "objectID": "Modules/metadata.html#what-is-metadata",
    "href": "Modules/metadata.html#what-is-metadata",
    "title": "Introduction to Metadata",
    "section": "",
    "text": "Metadata are the building blocks of digital scholarship! We will be creating, structuring, and using metadata throughout this course to participate in digital research and learn more about our collection.\nAs you transcribe, tag, and create information (aka data!) about the letters in the Smith Papers Collection, you are essentially creating metadata. Metadata is just structured “data about data,” and it’s something archivists and librarians use to describe, preserve, organize, and digitize. Furthermore, metadata makes items in a collection easy to find and discover.Metadata makes the internet work! And for this class, it is one of the building blocks of digital research.\nEach item in the result list above contains several bits of metadata. When you search for something with combinations of keywords, filters, search facets, etc., you are tapping the metadata that someone created in order to return results. This applies to most things in a digital collection. \n[(https://login.libprxy.muw.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&bquery=pizza&cli0=RV&clv0=Y&type=1&searchMode=And&site=eds-live&scope=site)]\nThink about the following digital items, and imagine what kinds of metadata - explicit or hidden - that you might interact with when you use them. Then think about what kinds of data and metadata (titles, descriptions, links, stylistic attributes, etc.) had to be collected to create each of the results in these collections.\n\n\n\nimage of a twitter feed\n\n\n\n\n\nscreenshot of the YouTube homepage\n\n\nNow, think about this as it pertains to a box of letters that have been scanned. In order for someone to digitally interact with these objects, they need metadata.\n\n\n\nscreenshot of an envelope and a letter from the Ellard-Murphree-Pilgreen Smith letters\n\n\n\n\nComplete the 1.Metadata Check-in before moving to the next section."
  },
  {
    "objectID": "Modules/metadata.html#what-are-metadata-standards",
    "href": "Modules/metadata.html#what-are-metadata-standards",
    "title": "Introduction to Metadata",
    "section": "",
    "text": "Similar to citation guidelines (e.g. MLA, APA, Chicago, etc.), digital items use a schema to organize and standardize their formatting. For instance, websites use HTML and CSS to communicate things to browsers, library databases use specific fields to link similar online articles to each other, and so on. This is a way to classify different kinds of metadata, define the information for each kind, and set rules for each kind (e.g. whether or not the field requires certain information or not)."
  },
  {
    "objectID": "Modules/metadata.html#smith-paper-metadata-schema",
    "href": "Modules/metadata.html#smith-paper-metadata-schema",
    "title": "Introduction to Metadata",
    "section": "",
    "text": "For the Smith Papers collection, we are using a common metadata standard/schema called Dublin Core, which takes its name from Dublin, OH where it was created. It’s used by many libraries and cultural heritage institutions to describe both digital and physical objects. In doing this, we: * give the collection structure * link it to similar items * provide consistency across items within the collection\nIn other words, our metadata standards give guidelines for how to treat the 100+ ways to describe something! Think about the different ways you could write your own name, let alone someone else’s you haven’t met!\n\n\n\ndifferent ways to write the same name\n\n\nOpen our Smith Paper Metadata Guidelines in another tab, and look at the different field names in the column headings.\n\n\nComplete the 2.Metadata Check-in before moving on to the next section."
  },
  {
    "objectID": "Modules/metadata.html#smith-paper-metadata-explained",
    "href": "Modules/metadata.html#smith-paper-metadata-explained",
    "title": "Introduction to Metadata",
    "section": "",
    "text": "As you noticed in the guidelines for the Smith Collection, we have a BUNCH of categories (or “fields”) to describe one letter:\n\n&lt;tr&gt;\n    &lt;td&gt;Creator\n    &lt;td&gt;Date\n    &lt;td&gt;Time Period\n&lt;/tr&gt;\n&lt;tr&gt;\n    &lt;td&gt;Subject\n    &lt;td&gt;Mississippi County\n    &lt;td&gt;Geographic Location\n&lt;/tr&gt;\n&lt;tr&gt;\n    &lt;td&gt;Resource Type\n    &lt;td&gt;Format\n    &lt;td&gt;Publisher\n&lt;/tr&gt;\n&lt;tr&gt;\n    &lt;td&gt;Notes\n    &lt;td&gt;Rights\n    &lt;td&gt;Collection\n&lt;/tr&gt;\n&lt;tr&gt;\n    &lt;td&gt;Source\n    &lt;td&gt;Digital Repository\n    &lt;td&gt;Date Digital\n&lt;tr&gt;\n    &lt;td&gt;Capture Method\n    &lt;td&gt;Master Image\n    &lt;td&gt;Record created by\n&lt;tr&gt;\n    &lt;td&gt;File name\n&lt;/tr&gt;\n\n\nIdentifier\n\nTitle\n\nDescription\n\n\n\n\nWhat do all of these mean? Some of these items capture unique information about the specific letter: * Identifier * Title * Description * Creator * Date * Notes * File name\nSome of them capture information that link them to other items in the collection (and potentially outside the collection): * Time Period * Subject * Mississippi County * Geographic Location * Resource Type * Format * Publisher\nAnd lastly, some of them capture information about the digitization process, in the event that the digital record disappears: * Digital Repository * Date Digital * Capture Method * Master Image * Record created by\nSome of these fields are consistent, and will say one thing, or just be limited to one of select options (e.g. Capture Method, Digital Repository, Resource Type, etc.). And some will require your discretion (e.g. Description, Subject Heading, etc.)\n\n\nUsing this letter and the full guidelines, enter the metadata you’d create for the given fields for the 3. Metadata check-inbefore moving on to the next section."
  },
  {
    "objectID": "Modules/metadata.html#subject-headings",
    "href": "Modules/metadata.html#subject-headings",
    "title": "Introduction to Metadata",
    "section": "",
    "text": "You’ll notice there are a lot of ways we describe each letter. Whew! So why subject headings? Subject Headings, like metadata standards, use “controlled vocabulary,” meaning you can group things together by category, so they’ll be  linked by topic. For instance, if I used the subject heading “Dating (Social Customs)” – and we do! – all of the letters with that designation can be grouped together. Books in the library are sorted and grouped by these subject headings, so we can explore information on the same topic in one physical location. What kind of books do you think are next to the one below? What kind of books do you think are next to this one?\n\n\n\nimage of a book record in the catalog for Ellen S. Woodward: New Deal Advocate for women by Martha Swain, with subject headings highlighted\n\n\n\n\n\nThere are a number of pre-determined subject headings defined by the Library of Congress, but how do we decide which subject heading to use when we are the ones coming up with them on our own? Subject headings can be confusing because there are so many of them, and just like words, you can use any number of words to describe something! In order to help with this for the Smith Papers collection, we have gathered several subject headings that are common to the collection. So your first step to choosing a subject heading will be to check this list:\nSmith Papers Authority headings\nIf the contents of your letter do not quite go with the suggested headings, then your next step is to look something up with similar topics, and choose a subject heading from there. Here’s what that looks like:\n\nGo to the MUW Library Online Catalog and search for a book using keywords that describe the topic. We’ll use the example “movies,” since the Smiths often talk about going to see a movie in many of their letters. Type movies in the search box.\nScroll through the results, and find something that is most similar to your concept. Keep it as simple as possible, and go with what seems most prevalent. Note all of the different ways people have categorized movies, and how these terms range in meaning and specificity:\n\nFilm\nMotion pictures\nFilm & Video\nDrama\nComedy\nCinematography\n\n\nIn this example, Motion pictures is broadly used enough in this list of results, and it captures the meaning that the people in the letters are using, since they are talking about going to a movie theatre to see a motion picture, so we’ll go with that!\n\n\n\nRemember: people create metadata, and even though they have mostly the best intentions, they also have biases, implicit or explicit. Metadata and metadata standards have been created by people with a notably biased frame of reference. This includes subject headings.\nFor many years, the Library of Congress classified books on homosexuality under “sexual deviance.” Recently, an undergraduate student at Dartmouth College began a campaign to change the “illegal alien” subject heading to “noncitizen,” resulting in a resolution from the American Library Association that has ended its use. Another example is the subject heading “Minorities” in our catalog. Not only is it not very descriptive, it’s also a loaded term that de-humanizes a group of people.\nYour job in choosing a subject heading is to create a helpful category for users who are interacting with items in a collection. If you came across a folder of photographs labeled “minorities,” what would you expect to find? Even if you didn’t know what was in there, what would be more specific, more helpful, or more appropriate? As you find and create metadata, consider the words you use and the different connotations of their meanings.\n\n\n\nComplete the 4.Metadata check-in before completing the final section."
  },
  {
    "objectID": "Modules/metadata.html#project-and-data-management",
    "href": "Modules/metadata.html#project-and-data-management",
    "title": "Introduction to Metadata",
    "section": "",
    "text": "Creating metadata on your own can be a lot of work! But we are working as a group! Let’s discuss what we’ll need to do as a group in class, and consider these parts of project and data management that are embedded in creating digital scholarship.\n\n\nHow do you document your progress so someone could pick up where you leave off and won’t duplicate your work? It is important to document your progress for yourself (your future self!), but also for those who are working in the same collection alongside you. What usually works for you (e.g. notes in a journal, putting dates on a folder on your computer, etc.) doesn’t always work for others!"
  },
  {
    "objectID": "Modules/metadata.html#progress-1",
    "href": "Modules/metadata.html#progress-1",
    "title": "Introduction to Metadata",
    "section": "",
    "text": "There are lots of ways to keep up with the progress of group projects. Often, a project manager will keep track of progress by checking in with people, going over updated tasks, and reporting progress notes at regular meetings. For keeping up with tasks and due dates, there are several free, online tools, like: - Asana - Google Sheets - Trello\nFor this class, we will use Asana as a group, and it will be up to you to keep up with your progress as an individual. I can’t recommend note-taking enough!\n\n\n\nscreenshot of project management application, asana, showing complete and incomplete tasks\n\n\n\n\nEssentially, we are creating a high-quality digital archival record for a physical item. There’s a lot that goes into that! Consider the following: * How do you preserve access so other people can access them? * How do you preserve the different types and formats of data you’re working with?\nSome data types that will come from our project include: * Non-digital text (handwritten notes, sketches) * Digital texts or digital copies of text (.txt, .pdf, .md, .doc, .otf, .rtf, .tei, etc.) * Data visualizations (.png, .jpg, .svg) * Spreadsheets (e.g. .xlsx, .csv) * Geographic Information Systems (GIS) and spatial data (e.g. .shp, .dbf, .shx, .kml) * Digital copies of images (e.g. .png, .jpeg, .tiff) * Web files (e.g. .html, .css, .md, .yml)\nOther questions to consider: * How and where do you save backup copies? * How do you ensure your work gets credit? * How do you ensure quality control (i.e. consistency in the metadata, image quality, etc.)? * How do you recognize and protect the integrity of a document with potentially harmful or sensitive information in it? * Are we publishing the documents somewhere? Do we have persmission to do that?\nMany projects consider these questions (and more) as a part of a Data Management Plan. If a project is grant funded, sometimes the grant will require a DMP. Here are some examples: * Digital Library Federation Sample Plans (Download the .doc file to see the actual plan) * ICSPR Guidelines for Effective Data Management Plans (Pages 1-10)\n\n\n\nYou won’t always be working on the collection at the same time, so how do you talk to each other outside of class time? How do you leave messages, and what are the expectations for responding? We will discuss how to do this as a class, and set up a channel for communication.\n\n\n\nscreenshot of Microsoft Teams, showing a conversation about a the Smith Papers\n\n\n\n\n\nConsider the principals under which you’ll operate as a cohesive group. Many projects are defined at the outset by practices that its members adopt and value as a group. How we collaborate together will be driven by specific values. Consider some values statements from the following projects: - Colored Conventions Project Values Statement - Smith Papers Project and Data Management - “Pledges,” from Scott Weingart’s blog\n\n\n\nUse these 4 sections - Progress, Preservation, Communication, and Principles - to answer the questions in the Data and Project Management Plan in Canvas to complete the metadata module."
  },
  {
    "objectID": "Modules/network-analysis.html",
    "href": "Modules/network-analysis.html",
    "title": "Introduction to Network Analysis",
    "section": "",
    "text": "Understand how network analyses help answer research questions with quantitative and visual components.\nIdentify parts of a network\nAnalyze relational data using different modes of centrality to determine the influence of individual nodes within the network\nClean and model data according to a specific question\nUse Palladio to customize and interrogate the network\nExplain the existing relationships in the network\n\n\n\nA Network Analysis is simply a study of relationships among a group of connected things. The “things” could be people, cities, cells in the body, etc.! The idea of a network analysis is to visualize these relationships, so you can see the patterns (or lack thereof) in connections within the network.\n\nWe use network analysis to ask questions about the players in the network, like * Who is the most/least influential? * What are some of the roles of the different things in this network? * How connected are the things in this network? * Are there distinct subgroups or “neighborhoods” in this network?\nFor the Smith Papers collection, we are asking these questions, more or less!\n\n\n\n\n\n\n\nQuestion\nSmith Papers Translation\n\n\n\n\nWho is the most influential?\nWhich person wrote the most letters to the most people?\n\n\nHow connected is this network?\nDid everyone write letters to each other, or just some people?\n\n\nWhat are the distinct groups?\nWho wrote to each other most often?\n\n\n\n\nNetwork analysis visualizations are excellent ways to show what’s going on in a complex network, but they cannot stand alone! Even the most clearly labeled network needs context. When you are creating a networked graph, include some writing that gives context to the network, and shows the things you chose to highlight. Then include that explanation alongside the graph in an accessible way.\nAnother thing to consider when making highly visual graphs, like the one above, is how accessible your visualization may or may not be to everyone, including those with visual impairments. Consider the following when designing a graph: * Are your labels large enough to read? * Are the colors in high enough contrast  to distinguish from each other? * Is there alternate text or description somewhere that explains the graph with as clearly worded text as possible? * Do the density of clusters and lines prevent someone from reading what’s going on in your graph?\nMany thanks to Dr. Katayoun Torabi’s 2020 Programming4Humanists Presentation “Introduction to Gephi” for modeling network analysis and providing the basis for this lesson.\n\n\nComplete the 1.Network Analysis check-in before moving on to the next section.\n\n\n\n\n\nMost softwares that facilitate Network Analyses use a different vocabulary. You’ll learn about what each part of a network is in this section, and in the next section, we’ll talk about different ways analyze those parts.\n\n\n\n\n\nexample of a simple network with 6 green and pink nodes and edges\n\n\nA network is a visual representation of relationships between entities. The network alone does not always share the context of the relationships (e.g. “These are people who talk to each other in this book,” or “These are the flight paths among major airlines,”), but they do highlight relationships. In doing so, they show how different players dominate (or not!) those relationships.\n\n\n\n\n\n\nexample of a simple network the nodes circled\n\n\nA node represents someone or something in the network. For the Smith Papers project, it will most likely be a person. Nodes can be different colors, depending on how influential they are, what subgroups within the network they belong to, etc.\n\n\n\n\n\n\nexample of a simple network the edges highlighted\n\n\nEdges represent the relationships between nodes. They usually indicate some kind of interaction in a relationship (e.g. conversation, travel, transactions, etc.), and depending on that interaction, an edge can be one of two kinds:\n\nUndirected - an undirected edge indicates that the interaction is reciprocal, like a two-way street, a friendship, a characteristic, etc.\nDirected - a directed edge indicates that the interaction is not reciprocal, like a one-way trip, a letter sent to someone, a payment for something, etc. In directed edges, there are those that give, and those that receive (just like in a real relationship!):\n\nSource - the source is the originator of the interaction, or the giver. Sometimes this is called “out-degree.”\nTarget - the target is where the interaction is directed, or the receiver. Sometimes this is called “in-degree.”\n\n\n\n\n\nexample of 2 simple networks with directed edges, indicated by arrows, and undirected edges, indicated by straight lines\n\n\n\n\n\n\n\n\nexample of a simple network with 6 nodes connected by 7 edges\n\n\nPaths are the length from one node to the next. In this image, the path length from Ben to Anna is 1, but the path length from Ben to Cara is 2, and so on. The average length of each path will tell us how dense the network is, or in other words, how connected everyone is to each other. If everyone is connected to everyone by 1 path, the network is 100% dense!\n…just a few more terms! Stick with me!…\n\n\n\nLogin to Canvas and complete the 1.Network Analysis check-in before moving on to the next section.\n\n\n\n\nDid that phrase make your stomach turn? Don’t worry! We are not actually doing math in this class, but we will be using mathematic concepts. We analyze networks by calculating the ways nodes influence each other. Let’s break it down.\n\n\n\nexample of a simple network with different sized nodes\n\n\n\n\nCentrality is the relative influence of individual nodes within the network. In the image above, the nodes have different sizes and colors to indicate their influence. Notice that Pauline has several direct connections, so they are the largest node. Other nodes, like Martha, have fewer connections, and therefore appear smaller. From this, we infer that Pauline has a more influential role in the network.\nFor this class, we will measure centrality with 3 different metrics: * Degree * Closeness * Betweenness \n\n\nThe degree of a node is how connected or influential a node is within the network, so degree centrality counts the number of direct connections a node has. In the previous image, Edith Pauline Smith has the highest degree centrality because she has the most immediate connections. To indicate this, we have made the size of node labels correspond to degree, which for Pauline, is the largest.\n\n\n\nAnother simple metric is called closeness centrality, which measures which node has the shortest average path to the rest of the nodes in the network.\n\n\n\nexample of a simple network with subgroups\n\n\nUnlike degree, which measures direct connections, closeness measures the proximity of a node to all the other nodes through average path. There are 8 total edges in the network above. If we wanted to calculate the closeness centrality of B, we’d count the shortest path from B to every other node, and take the average.\nB -&gt; A = 2 B -&gt; C = 1 B -&gt; D = 1 B -&gt; E = 3 B -&gt; F = 4 B -&gt; G = 4 B -&gt; H = 0 (there are no paths to H from B!) B -&gt; I = 0 (there are no paths to I from B!)\nB has a closeness centrality of 1.875. (There are 15 total paths to every other node, and 8 total edges, so 15/8=1.875). So this node has more direct connections (2), but is less “close” in terms of the entire network (1.875).\n\n\n\n\n\nDegree and Closeness measure influence through connections and paths. Betweenness centrality indicates nodes that are “bridges” between different groups in a network. In other words, they glue pieces of a less dense network together, or they serve as a “go-between” for 2 distinct parts of the network.\n\n\n\nexample of a simple network that is loosely connected, but bridged together by a node in the center\n\n\n\n\n\nLog in to Canvas and complete the 2.Network Analysis check in before moving on to the next section.\n\n\n\n\nIn order to tell the software (which we go over in the next section) how to recognize which nodes are connected, and by what paths, we are going to create a spreadsheet with “tidy data.” Each column will have a variable, and each row will be an observation of that variable.\nThe most important thing to remember when structuring data for a network analysis is this: what question are you trying to answer with a network analysis graph? Are you trying to see who someone is writing to in a letter? Are you trying to find out who has written the most letters and who has received the most? Are you trying to figure out who gets mentioned more times by one person than another? All of this matters. For instance, if we wanted to see where the letters were coming from and who is writing to whom, we could structure the data like this:\n\n\n\nSource\nTarget\nAddress\nReturn Address\n\n\n\n\nPauline Smith\nSam H. Smith\nPittsboro\nJackson\n\n\nChristine Smith\nPauline Smith\nStarkville\nPittsboro\n\n\nPauline Smith\nMartha Smith\nPittsboro\nMeridian\n\n\n\nIn this graph, each column heading (the variables) has one observation (the information in one letter). That’s tidy data!\nSay, for instance, though, that we want to see all of the people mentioned in several letters to get a better idea of the network of people in the lives of the Smith family members and their friends. We could structure our data so that each source (the letter writer) has a target (the person they write to and the people they mention), and each target has either a reciprocal (undirected) or non-reciprocal (directed) edge.\n\n\nUsing a pencil and paper, sketch what you think the second example’s graph might look like. Upload that sketch in Canvas on the 3. Network Analysis check-in before moving on to the next section.\n\n\n\n\nWouldn’t it be nice if you could just copy and paste some names in a list into a spreadsheet, and click a button to make a nice network graph? IT NEVER WORKS THAT WAY! But that’s part of the work of the digital scholar - is to structure data through a process of cleaning and testing it, in order to see if they can answer a question through visualization. Hadley Wickham, author of Tidy Data says:\n\nIt is often said that 80% of data analysis is spent on the process of cleaning and preparing the data (Dasu and Johnson 2003). Data preparation is not just a first step, but must be repeated many times over the course of analysis as new problems come to light or new data is collected.\n\nLuckily, we have done a lot of that work already, and in this class, you won’t be starting from scratch, but let’s look at the labor that comes with cleaning and preparing data from the beginning.\nWhen you export tags from Transkribus, you get a folder that corresponds to each item, and in that folder, a spreadsheet of tags.\n\n\n\nscreenshot of a folder containing 346 folders\n\n\nThese files, individually, do not help us answer our research question! So we have to combine them and clean them in order to see all the senders, recipients, and people mentioned in the letters. Rather than open and inspect each file, copy all of its contents, and paste it into a new file, we are using automated functions in Excel to combine the data before we clean and examine our data. Assuming all the data in these files is uniform (and in several cases, they aren’t!), Excel should combine the sheets into one file, going from this…\n\n\n\nscreenshot of the ‘get data from folder’ function in Excel\n\n\n…to this…\n\n\n\nscreenshot of an Excel spreadsheet with combined data from the tags export\n\n\nAfter combining these tags, we then add a layer of structure, so that our data will model the question we’re asking: how can we use the letters to get an idea of the people in the Smiths’ network? Using the combined tags sheet, we created the structured tags sheet. Now we can start adding newly transcribed tags and cleaning them.\nThere are lots of ways to clean a spreadsheet - some complex and some simple. Here the recommended steps you can take in order to get from names and data you tagged in the letters, to structured data that you can put into a network graph software. With each step is a video that shows you how to do it!\n\nExport the Tags metadata from Transkribus. Watch this in a video here. Hint: you have already done this step in the data prep assignment\nCopy and past the names of the senders, recipients, and people from your letters to the “combined tags” sheet.\nOpen the “structured tags” sheet and add your tags here in the appropriate columns.\nNow it’s time to clean! I recommend doing these steps in a separate sheet, so you don’t accidentally lose any data. Remember: Clean the names that you can (Watch this in a video here) by:\n\nsorting names A-Z, and reconciling names that are extremely similar in spelling (i.e. Bro Breland and Brother Breland) for consistency.\nreplacing known names with their authority control names (i.e. “mother” with “Smith, Edith Pauline). We have identified some people by first name and context, and listed them in this Google Sheet. You can do this by Find and Replace (ctrl+f), but careful not to do blanket replacements! These tend to muck up your data.\nde-duplicating names that appear more than once per letter, so you include only one representation of that person. (To measure our network, we are not including each time a person is mentioned in a letter…just that they are mentioned at all!). To do this, copy the column into another sheet, and click on Remove Duplicates under Data. Paste your de-duplicated list back into the rows associated with that letter, and delete the empty rows.\n\n\n\n\n\nspreadsheet screenshot of similar names, including Bro. Brealand and Bro Breland\n\n\nIf you don’t know the names, that’s ok! Remember, we aren’t trying to erase anyone’s name or presence by assuming who they are or are not(see “Against Cleaning” again!). Kate and Katherine might not be the same person. We don’t know! Embrace the chaos and keep assumptions to a minimum! This process will take a while and feel repetitive, but remember, that is normal. Give yourself time, and take plenty of breaks.\n\n\nWrite a detailed list of things that you did to clean up the data you exported. Did you de-duplicate names? Did you make assumptions for similar names? If so, what assumptions did you make? Write these down as precisely as you can so that if someone else were to follow your directions, they’d get similar results.\nComplete the 3a.Network Analysis Check-in before moving on. For an example of detailing this process, see the tags README file.\n\n\n\n\nWe have prepared our data (it won’t be the last time!), so now it’s time to see what that looks like in a network analysis software’s visualization. There are several different softwares for visualizing a network, and they all serve different purposes. We are going to use Palladio because it is free, has a relatively low accessibility bar, will work in your browser, and most importantly, will allow us to use the visualization to answer our questions: What does the network of people in the lives of the Smith family members look like? Where are the connections and who facilitates them?\n\n\n\n\nYou can either copy and paste the cells from your spreadsheet (columnn headings included), or you can upload your file as a .csv file - .csv, or comma separated value files are more flexible among different platforms, and is a simpler, less formatted version of tabular (or structured) data.\nPalladio will not support an .xlsx file!\nCopying your data will look like this:\n\n\n\nscreenshot showing how to copy data for use in Palladio\n\n\nSaving your data to upload will look like this:\n\n\n\nscreenshot showing how to save data as .csv for use in Palladio\n\n\n\n\n\nPalladio flags inconsistencies in data, like the use of special characters or multiple observations in one variable (i.e. more than one value in a cell. Commas are tricky to this program!). Use Palladio’s verification tools, sorting options, and searching option to take another thorough look at your data. \nClicking on the little red dots will allow you to verify and look further at your data. \nThis is also an opportunity to evaluate if you need to go back and clean the original spreadsheet! For instance, if you are reviewing the issues, and you find that:\n\na person’s name recorded in 2 different ways,\nspelling errors, or\nthe type of data (i.e. text, number, date, etc.) is displayed incorrectly,\n\nyou want to take this opportunity to clean your data again. This is part of the process!\n\n\n\nscreenshot showing sorted data that needs further cleaning\n\n\n\n\n\n\nOnce you’ve verified the data and gone back to clean any leftover issues, it’s time to visualize it. (Reality check: this isn’t necessarily the end of cleaning data! A visualization can also make inconsistencies and cleaning issues apparent!). For the question of how to visualize the letter writer and their network of people (the person who received the letter and those mentioned within), click the Graph tab, then choose the following * Source dimension - this is the sender of the letter * Target dimension - these will be both the recipient and the people mentioned\n\n\n\n\nAdd facets to your data\nSize the nodes according to different dimensions (ours will just have one dimension - number of times they appear).\nFilter what appears in the network (Directed v. Undirected)\nDrag nodes manually to see different connections among the network. Who is connected to whom? Who bridges the network?\n\nHere is a brief video of what this process looks like.\nThere are also Palladio’s own Tutorials and FAQs for loading data and customizing a graph. Both of which are helpful!\n\n\n\n\nEyeball test - what can you start to answer with the image you see? What looks like it needs fixing?\nComplete the last 4.Network Analysis check-in before turning in the image of your final visualization for this mini-project."
  },
  {
    "objectID": "Modules/network-analysis.html#what-is-a-network-analysis",
    "href": "Modules/network-analysis.html#what-is-a-network-analysis",
    "title": "Introduction to Network Analysis",
    "section": "",
    "text": "A Network Analysis is simply a study of relationships among a group of connected things. The “things” could be people, cities, cells in the body, etc.! The idea of a network analysis is to visualize these relationships, so you can see the patterns (or lack thereof) in connections within the network.\n\nWe use network analysis to ask questions about the players in the network, like * Who is the most/least influential? * What are some of the roles of the different things in this network? * How connected are the things in this network? * Are there distinct subgroups or “neighborhoods” in this network?\nFor the Smith Papers collection, we are asking these questions, more or less!\n\n\n\n\n\n\n\nQuestion\nSmith Papers Translation\n\n\n\n\nWho is the most influential?\nWhich person wrote the most letters to the most people?\n\n\nHow connected is this network?\nDid everyone write letters to each other, or just some people?\n\n\nWhat are the distinct groups?\nWho wrote to each other most often?\n\n\n\n\nNetwork analysis visualizations are excellent ways to show what’s going on in a complex network, but they cannot stand alone! Even the most clearly labeled network needs context. When you are creating a networked graph, include some writing that gives context to the network, and shows the things you chose to highlight. Then include that explanation alongside the graph in an accessible way.\nAnother thing to consider when making highly visual graphs, like the one above, is how accessible your visualization may or may not be to everyone, including those with visual impairments. Consider the following when designing a graph: * Are your labels large enough to read? * Are the colors in high enough contrast  to distinguish from each other? * Is there alternate text or description somewhere that explains the graph with as clearly worded text as possible? * Do the density of clusters and lines prevent someone from reading what’s going on in your graph?\nMany thanks to Dr. Katayoun Torabi’s 2020 Programming4Humanists Presentation “Introduction to Gephi” for modeling network analysis and providing the basis for this lesson.\n\n\nComplete the 1.Network Analysis check-in before moving on to the next section."
  },
  {
    "objectID": "Modules/network-analysis.html#network-analysis-terms",
    "href": "Modules/network-analysis.html#network-analysis-terms",
    "title": "Introduction to Network Analysis",
    "section": "",
    "text": "Most softwares that facilitate Network Analyses use a different vocabulary. You’ll learn about what each part of a network is in this section, and in the next section, we’ll talk about different ways analyze those parts.\n\n\n\n\n\nexample of a simple network with 6 green and pink nodes and edges\n\n\nA network is a visual representation of relationships between entities. The network alone does not always share the context of the relationships (e.g. “These are people who talk to each other in this book,” or “These are the flight paths among major airlines,”), but they do highlight relationships. In doing so, they show how different players dominate (or not!) those relationships.\n\n\n\n\n\n\nexample of a simple network the nodes circled\n\n\nA node represents someone or something in the network. For the Smith Papers project, it will most likely be a person. Nodes can be different colors, depending on how influential they are, what subgroups within the network they belong to, etc.\n\n\n\n\n\n\nexample of a simple network the edges highlighted\n\n\nEdges represent the relationships between nodes. They usually indicate some kind of interaction in a relationship (e.g. conversation, travel, transactions, etc.), and depending on that interaction, an edge can be one of two kinds:\n\nUndirected - an undirected edge indicates that the interaction is reciprocal, like a two-way street, a friendship, a characteristic, etc.\nDirected - a directed edge indicates that the interaction is not reciprocal, like a one-way trip, a letter sent to someone, a payment for something, etc. In directed edges, there are those that give, and those that receive (just like in a real relationship!):\n\nSource - the source is the originator of the interaction, or the giver. Sometimes this is called “out-degree.”\nTarget - the target is where the interaction is directed, or the receiver. Sometimes this is called “in-degree.”\n\n\n\n\n\nexample of 2 simple networks with directed edges, indicated by arrows, and undirected edges, indicated by straight lines\n\n\n\n\n\n\n\n\nexample of a simple network with 6 nodes connected by 7 edges\n\n\nPaths are the length from one node to the next. In this image, the path length from Ben to Anna is 1, but the path length from Ben to Cara is 2, and so on. The average length of each path will tell us how dense the network is, or in other words, how connected everyone is to each other. If everyone is connected to everyone by 1 path, the network is 100% dense!\n…just a few more terms! Stick with me!…\n\n\n\nLogin to Canvas and complete the 1.Network Analysis check-in before moving on to the next section."
  },
  {
    "objectID": "Modules/network-analysis.html#analyzing-relational-data",
    "href": "Modules/network-analysis.html#analyzing-relational-data",
    "title": "Introduction to Network Analysis",
    "section": "",
    "text": "Did that phrase make your stomach turn? Don’t worry! We are not actually doing math in this class, but we will be using mathematic concepts. We analyze networks by calculating the ways nodes influence each other. Let’s break it down.\n\n\n\nexample of a simple network with different sized nodes\n\n\n\n\nCentrality is the relative influence of individual nodes within the network. In the image above, the nodes have different sizes and colors to indicate their influence. Notice that Pauline has several direct connections, so they are the largest node. Other nodes, like Martha, have fewer connections, and therefore appear smaller. From this, we infer that Pauline has a more influential role in the network.\nFor this class, we will measure centrality with 3 different metrics: * Degree * Closeness * Betweenness \n\n\nThe degree of a node is how connected or influential a node is within the network, so degree centrality counts the number of direct connections a node has. In the previous image, Edith Pauline Smith has the highest degree centrality because she has the most immediate connections. To indicate this, we have made the size of node labels correspond to degree, which for Pauline, is the largest.\n\n\n\nAnother simple metric is called closeness centrality, which measures which node has the shortest average path to the rest of the nodes in the network.\n\n\n\nexample of a simple network with subgroups\n\n\nUnlike degree, which measures direct connections, closeness measures the proximity of a node to all the other nodes through average path. There are 8 total edges in the network above. If we wanted to calculate the closeness centrality of B, we’d count the shortest path from B to every other node, and take the average.\nB -&gt; A = 2 B -&gt; C = 1 B -&gt; D = 1 B -&gt; E = 3 B -&gt; F = 4 B -&gt; G = 4 B -&gt; H = 0 (there are no paths to H from B!) B -&gt; I = 0 (there are no paths to I from B!)\nB has a closeness centrality of 1.875. (There are 15 total paths to every other node, and 8 total edges, so 15/8=1.875). So this node has more direct connections (2), but is less “close” in terms of the entire network (1.875)."
  },
  {
    "objectID": "Modules/network-analysis.html#betweenness-centrality",
    "href": "Modules/network-analysis.html#betweenness-centrality",
    "title": "Introduction to Network Analysis",
    "section": "",
    "text": "Degree and Closeness measure influence through connections and paths. Betweenness centrality indicates nodes that are “bridges” between different groups in a network. In other words, they glue pieces of a less dense network together, or they serve as a “go-between” for 2 distinct parts of the network.\n\n\n\nexample of a simple network that is loosely connected, but bridged together by a node in the center\n\n\n\n\n\nLog in to Canvas and complete the 2.Network Analysis check in before moving on to the next section."
  },
  {
    "objectID": "Modules/network-analysis.html#data-modeling-for-visualizing-a-network",
    "href": "Modules/network-analysis.html#data-modeling-for-visualizing-a-network",
    "title": "Introduction to Network Analysis",
    "section": "",
    "text": "In order to tell the software (which we go over in the next section) how to recognize which nodes are connected, and by what paths, we are going to create a spreadsheet with “tidy data.” Each column will have a variable, and each row will be an observation of that variable.\nThe most important thing to remember when structuring data for a network analysis is this: what question are you trying to answer with a network analysis graph? Are you trying to see who someone is writing to in a letter? Are you trying to find out who has written the most letters and who has received the most? Are you trying to figure out who gets mentioned more times by one person than another? All of this matters. For instance, if we wanted to see where the letters were coming from and who is writing to whom, we could structure the data like this:\n\n\n\nSource\nTarget\nAddress\nReturn Address\n\n\n\n\nPauline Smith\nSam H. Smith\nPittsboro\nJackson\n\n\nChristine Smith\nPauline Smith\nStarkville\nPittsboro\n\n\nPauline Smith\nMartha Smith\nPittsboro\nMeridian\n\n\n\nIn this graph, each column heading (the variables) has one observation (the information in one letter). That’s tidy data!\nSay, for instance, though, that we want to see all of the people mentioned in several letters to get a better idea of the network of people in the lives of the Smith family members and their friends. We could structure our data so that each source (the letter writer) has a target (the person they write to and the people they mention), and each target has either a reciprocal (undirected) or non-reciprocal (directed) edge.\n\n\nUsing a pencil and paper, sketch what you think the second example’s graph might look like. Upload that sketch in Canvas on the 3. Network Analysis check-in before moving on to the next section."
  },
  {
    "objectID": "Modules/network-analysis.html#cleaning-data",
    "href": "Modules/network-analysis.html#cleaning-data",
    "title": "Introduction to Network Analysis",
    "section": "",
    "text": "Wouldn’t it be nice if you could just copy and paste some names in a list into a spreadsheet, and click a button to make a nice network graph? IT NEVER WORKS THAT WAY! But that’s part of the work of the digital scholar - is to structure data through a process of cleaning and testing it, in order to see if they can answer a question through visualization. Hadley Wickham, author of Tidy Data says:\n\nIt is often said that 80% of data analysis is spent on the process of cleaning and preparing the data (Dasu and Johnson 2003). Data preparation is not just a first step, but must be repeated many times over the course of analysis as new problems come to light or new data is collected.\n\nLuckily, we have done a lot of that work already, and in this class, you won’t be starting from scratch, but let’s look at the labor that comes with cleaning and preparing data from the beginning.\nWhen you export tags from Transkribus, you get a folder that corresponds to each item, and in that folder, a spreadsheet of tags.\n\n\n\nscreenshot of a folder containing 346 folders\n\n\nThese files, individually, do not help us answer our research question! So we have to combine them and clean them in order to see all the senders, recipients, and people mentioned in the letters. Rather than open and inspect each file, copy all of its contents, and paste it into a new file, we are using automated functions in Excel to combine the data before we clean and examine our data. Assuming all the data in these files is uniform (and in several cases, they aren’t!), Excel should combine the sheets into one file, going from this…\n\n\n\nscreenshot of the ‘get data from folder’ function in Excel\n\n\n…to this…\n\n\n\nscreenshot of an Excel spreadsheet with combined data from the tags export\n\n\nAfter combining these tags, we then add a layer of structure, so that our data will model the question we’re asking: how can we use the letters to get an idea of the people in the Smiths’ network? Using the combined tags sheet, we created the structured tags sheet. Now we can start adding newly transcribed tags and cleaning them.\nThere are lots of ways to clean a spreadsheet - some complex and some simple. Here the recommended steps you can take in order to get from names and data you tagged in the letters, to structured data that you can put into a network graph software. With each step is a video that shows you how to do it!\n\nExport the Tags metadata from Transkribus. Watch this in a video here. Hint: you have already done this step in the data prep assignment\nCopy and past the names of the senders, recipients, and people from your letters to the “combined tags” sheet.\nOpen the “structured tags” sheet and add your tags here in the appropriate columns.\nNow it’s time to clean! I recommend doing these steps in a separate sheet, so you don’t accidentally lose any data. Remember: Clean the names that you can (Watch this in a video here) by:\n\nsorting names A-Z, and reconciling names that are extremely similar in spelling (i.e. Bro Breland and Brother Breland) for consistency.\nreplacing known names with their authority control names (i.e. “mother” with “Smith, Edith Pauline). We have identified some people by first name and context, and listed them in this Google Sheet. You can do this by Find and Replace (ctrl+f), but careful not to do blanket replacements! These tend to muck up your data.\nde-duplicating names that appear more than once per letter, so you include only one representation of that person. (To measure our network, we are not including each time a person is mentioned in a letter…just that they are mentioned at all!). To do this, copy the column into another sheet, and click on Remove Duplicates under Data. Paste your de-duplicated list back into the rows associated with that letter, and delete the empty rows.\n\n\n\n\n\nspreadsheet screenshot of similar names, including Bro. Brealand and Bro Breland\n\n\nIf you don’t know the names, that’s ok! Remember, we aren’t trying to erase anyone’s name or presence by assuming who they are or are not(see “Against Cleaning” again!). Kate and Katherine might not be the same person. We don’t know! Embrace the chaos and keep assumptions to a minimum! This process will take a while and feel repetitive, but remember, that is normal. Give yourself time, and take plenty of breaks.\n\n\nWrite a detailed list of things that you did to clean up the data you exported. Did you de-duplicate names? Did you make assumptions for similar names? If so, what assumptions did you make? Write these down as precisely as you can so that if someone else were to follow your directions, they’d get similar results.\nComplete the 3a.Network Analysis Check-in before moving on. For an example of detailing this process, see the tags README file."
  },
  {
    "objectID": "Modules/network-analysis.html#software-for-visualizing-a-network-analysis",
    "href": "Modules/network-analysis.html#software-for-visualizing-a-network-analysis",
    "title": "Introduction to Network Analysis",
    "section": "",
    "text": "We have prepared our data (it won’t be the last time!), so now it’s time to see what that looks like in a network analysis software’s visualization. There are several different softwares for visualizing a network, and they all serve different purposes. We are going to use Palladio because it is free, has a relatively low accessibility bar, will work in your browser, and most importantly, will allow us to use the visualization to answer our questions: What does the network of people in the lives of the Smith family members look like? Where are the connections and who facilitates them?\n\n\n\n\nYou can either copy and paste the cells from your spreadsheet (columnn headings included), or you can upload your file as a .csv file - .csv, or comma separated value files are more flexible among different platforms, and is a simpler, less formatted version of tabular (or structured) data.\nPalladio will not support an .xlsx file!\nCopying your data will look like this:\n\n\n\nscreenshot showing how to copy data for use in Palladio\n\n\nSaving your data to upload will look like this:\n\n\n\nscreenshot showing how to save data as .csv for use in Palladio\n\n\n\n\n\nPalladio flags inconsistencies in data, like the use of special characters or multiple observations in one variable (i.e. more than one value in a cell. Commas are tricky to this program!). Use Palladio’s verification tools, sorting options, and searching option to take another thorough look at your data. \nClicking on the little red dots will allow you to verify and look further at your data. \nThis is also an opportunity to evaluate if you need to go back and clean the original spreadsheet! For instance, if you are reviewing the issues, and you find that:\n\na person’s name recorded in 2 different ways,\nspelling errors, or\nthe type of data (i.e. text, number, date, etc.) is displayed incorrectly,\n\nyou want to take this opportunity to clean your data again. This is part of the process!\n\n\n\nscreenshot showing sorted data that needs further cleaning\n\n\n\n\n\n\nOnce you’ve verified the data and gone back to clean any leftover issues, it’s time to visualize it. (Reality check: this isn’t necessarily the end of cleaning data! A visualization can also make inconsistencies and cleaning issues apparent!). For the question of how to visualize the letter writer and their network of people (the person who received the letter and those mentioned within), click the Graph tab, then choose the following * Source dimension - this is the sender of the letter * Target dimension - these will be both the recipient and the people mentioned\n\n\n\n\nAdd facets to your data\nSize the nodes according to different dimensions (ours will just have one dimension - number of times they appear).\nFilter what appears in the network (Directed v. Undirected)\nDrag nodes manually to see different connections among the network. Who is connected to whom? Who bridges the network?\n\nHere is a brief video of what this process looks like.\nThere are also Palladio’s own Tutorials and FAQs for loading data and customizing a graph. Both of which are helpful!\n\n\n\n\nEyeball test - what can you start to answer with the image you see? What looks like it needs fixing?\nComplete the last 4.Network Analysis check-in before turning in the image of your final visualization for this mini-project."
  },
  {
    "objectID": "Modules/transcribing.html",
    "href": "Modules/transcribing.html",
    "title": "Introduction to Transcribing",
    "section": "",
    "text": "In this module, you will learn: * Basic transcribing guidelines * How to create and edit text regions in Transkribus * How to use style and metadata tags in Transkribus\nWhy do we transcribe documents? On one level, transcribing something allows you to become intimately familiar with the contents of a document. By the end of this semester, you may be invested in a family that isn’t your own! \nFurthermore, transcribing makes physical documents, especially handwritten documents, machine readable. In other words, it allows computers to recognize text. Think about it – scanned letters are pictures of text, which is something a machine cannot translate. Even though there are programs that can attempt to read handwriting (like Transkribus) and even typed text, computers aren’t smart enough to read things humans are!\nTranscribing something bridges the gap between reading a document, and storing a picture of one.\n\n\n\nIn order to maintain the integrity of your original document while you are translating it for the computer, consider a few things while you’re transcribing.\n\nStay as true to the document as you can!\n\n\nInclude all text as it is written. This includes errors in spelling and grammar, and words you might not be familiar with. Transcribing is NOT editing!\nIf there are images in the text, describe them in brackets (e.g. “[doodle of a curly-haired smiling face]”).\nKeep it simple! You want people to be able to read the transcription without looking at the original letter. Your transcriptions will be plain text files (e.g. files with no formatting), so, without editorializing, keep the text readable as possible.\n\n\nIf you have trouble reading the text…\n\n\nTake a break and come back. Fresh eyes make a difference!\nTry to read the words in context. The word might look like “fhe,” but it doesn’t make sense to read “And then fhe called her sister.” You would realize the word is “she.”\nAsk for help. Send a screenshot to your classmates, or post it in the Teams channel.\nIf you’ve tried all these things, and it’s just not possible to figure out, write [illegible] in brackets and tag that text with the “gap” tag (more on tags in section IV!).\n\n\nTake notes while you are transcribing\n\n\nMake note of things you don’t understand, and Google them to see if you can make sense of the reference.\nYou might find something interesting that you want to come back to! Take notes of the letters you transcribe, and what you find interesting about them.\n\n\n\nPractice transcribing a few lines, staying true to the original text with the 0.Transcribing Check-in.\n\n\n\n\nDownload Transkribus before starting this section\nBefore we get started actually transcribing, we have to tell Transkribus what parts of the page have text on them, and what order to read that text! (Remember, computers aren’t smart enough to read like humans!)\n\n\nThe page (or “text region”), where text is: \nThe line of text, as in the full, left-to-write line: \nAnd the baseline, which underlines the words themselves: \n\n\n\nTranskribus does a pretty good job of identifying these text regions for us. The quickest way to do this is to run a layout analysis on the document, then correct any mistakes that Transkribus makes. Here’s how you run the analysis:\n\nDouble click the document you’ve uploaded, and click on the Tools tab.\nUnder Layout Analysis, make sure you’ve selected all pages.\nClick Run\n\n\n\n\nscreenshot of layout analysis\n\n\n\nA notification that the layout analysis is in progress will pop up. After a few moments (between 3 and 10 seconds), the layout (green squares, blue rectangles, and dark blue lines) will appear over the letter’s text.\n\n\n\n\n\nCorrect errors in the layout (e.g. delete text regions or lines that don’t cover text, resize regions that cut off text, reorder line numbers, or merge lines that have been split).\n\n\nText regions that don’t actually contain any transcribe-able text. \nThis layout analysis split a line in two, and needs merging. (Sometimes, though less frequently, it will merge something that should be two lines. For this, use the scissor tools above the merge tool.) Holding control, click each region that needs joining (usually the baseline and the line) and then click merge. \nSometimes the software will number things in a different order than we’d actually read them. Click on the eye in the top toolbar, and select show lines reading order. If the lines are out of order, click on the numbers to change them. \n\n\n\n\nComplete 2.Transcribing Check-in before moving on to the next section.\n\n\n\n\nTranskribus allows you to create Handwritten Text Recognition (HTR) models. Over the past 2 years that we’ve worked on this project, we have transcribed over 100 letters and trained the program to begin to recognize Pauline Smith’s handwriting. This allows us to run the model on letters in the collection and start with a more complete transcription to correct, rather than having to transcribe the letter from scratch.\n\nTo transcribe with the HTR model, click on the Tools tab, and under Text Recognition, select the model called “Pauline Smith 2.0.” You might have to navigate to the next page to find the Pauline Smith model.\n\n\n\n\nscreenshot of the Text recognition tool\n\n\n\n\n\nscreenshot of Pauline Smith’s HTR model\n\n\n\nClick Run. The HTR analysis may take anywhere from 20 seconds to 2 minutes. A notification that the “job” is done will pop up when analysis is complete. From there, correct the transcription.\nTranscribe each page as you see it, following Smithsonian and Transkribus transcription guidelines. You may have a few errors on a page or there may be significant errors. It depends on several things - the quality of the scan, the quality of the document, the handwriting and what utensil they used, etc.!\n\n\n\n\nscreenshot of HTR errors\n\n\n\nAfter you’ve completed editing the transcription, go back over the letter, making sure you didn’t miss typos, and you can simultaneously begin tagging the document, which will be explained in the next lesson!\n\n\n\nComplete the Transcribing Check-in before moving on to the next section.\n\n\n\n\nTagging, annotating, or marking-up a document is a common practice for text analysis. Why is this? Think about the things someone might talk to you about in a letter. They might mention people they talked about, places they went, or a book they read. Now think about the contents of 6,000 letters, and all of this information together! Tagging will allow us to use the information in the collection of thousands of letters to track these trends over time, explore topics, and find people.\nThink of tagging the letter like highlighting categories of information with a different color highlighter. All of the people’s names are blue, the places are purple, etc. Transkribus does the same thing.\n\n\nIt is tempting to tag everything, but not necessary! You only need to tag: 1. Important words (e.g. people and places) 2. When you add something for clarity that isn’t there originally (e.g. “[illegible]”). 3. If you want to provide further information as context.\n\n\n\nHere are the tags we have, to this point, decided to use for the Smith Collection, and their definitions. (For the info below in a Google Doc, click here. And for more info about how Transkribus feels about tags, click here.)\n\nperson - when any person’s name is mentioned within the body of the letter (e.g. “Daddy,” Martha, Gilbert, etc.), or when a person is referred to (e.g. “Martha’s friend boy,” or “Jewish soldier”).\nplace - when a place name is mentioned within the body of the letter. This does not usually include general places like Sunday school or hospital. Examples would be town names like Bruce or Memphis. sender - The person who sent the letter, either in the return address, or in the letter’s closing.\nrecipient - The person the letter is addressed to, either on the envelope, or in the salutation.\nsender - The person who wrote the letter. They usually put their name in the return address and in the signature.\naddress - The address the letter was sent to.\nreturn-address - the address the letter came from (not always included)\norganization - a specific group, corporation, or entity (e.g. the Senate, the T.V.A., or Ole Miss), not a generic place like hospital or school.\nsic - for an error in the original letter (i.e. spelling, grammar, etc.) that you include it in the transcript for accuracy, tag it with “sic,” showing that it’s “as is,” and you didn’t make the error in transcribing.\ntitle - the title of a book, play, radio show, government decision, etc. Anything that is published, for example. Specifications may include full title, author, and publication date.\ndate - a specific date (e.g. January 15th, 1948). This does not include general references to days of the week or years (e.g. “Saturday nite” or “next year.”)\nunclear - when you write something in the transcription, but aren’t sure of what it actually says in the letter: a guess.\ngap (formerly supplied) - when you include something (i.e. [illegible], a note, etc.) that isn’t in the original letter to give clarity, but to prevent the HTR from picking up the note.\n\n\n\n\n\nTo tag something, highlight the text you want to tag, right-click, and from “All tags,” select the relevant tag.\n\n\n\n\nScreenshot of how to tag something\n\n\n\nIf the tag isn’t in the list, go to the metadata menu, then the Textual menu, and click Customize.\n\n\n\n\nScreenshot of customize tag\n\n\n\nClick “Create new tag,” then type in the name of the tag, lowercase, like it’s shown in the Tag definitions document.\n\n\n\n\nScreenshot of creating a new tag\n\n\n\n\n\nTags are sort of tricky! Let’s go over the different scenarios in the next 4.Transcribing Check-in. Complete this before moving on to the next section.\nAs a reminder, there is a video of transcribing a letter from beginning to end, which you can watch as a refresher: https://youtu.be/-cDD9P0rnLw.\nAt the end of this lesson, you should be able to fill out the full transcription and tags in your letters, and turn them in in Canvas.\n\n\n\n\nAfter you have transcribed (or corrected a transcription of) a letter, tagged it, and proofed it, it has been digitally transformed, and we can now do fun things with the transcriptions and metadata. But before you can do that, you have to export the files.\nBefore the step-by-step, I recommend watching this 3-minute video on this process.\nClick the export button (a manila folder with a green, right-facing arrow), and you will choose the following files to export: * .docx - to give you a formatted version of the transcript * .txt - to give a non-formatted version of the transcript * .xlsx (tag export) - to give you structured lists of the tags you created\nIf you would like to export more than one document, select the radio button that says “Current Collection,” and choose which letters to export.\n\n\n\nScreenshot of Export Document option\n\n\n\n\nWhen you click OK, Transkribus emails you a temporary link where you can access the compressed files you just created. Compressing them allows them to send large amounts of data over email. When you click the link, it will ask you to save the compressed files (.zip).\n\n\n\nScreenshot of saving compressed folder\n\n\nSaving them as compressed files stores them to your computer, but does not let you access them until you extract them. Right-click your compressed folder and click Extract All... then tell your computer where you want to extract them to. I recommend saving them to a folder where you’re keeping files for this class. (We will also store them on a cloud server, e.g. Google Drive or our Institutional Repository at some point.)\n\n\n\nScreenshot of extracting files to a destination\n\n\nAfter you extract the files, you should notice that your folder no longer has a zipper on it, and you have 3 files that represent digital versions of the letter you prepared. You are now ready to start playing with these digital artifacts!"
  },
  {
    "objectID": "Modules/transcribing.html#transcribing-guidelines",
    "href": "Modules/transcribing.html#transcribing-guidelines",
    "title": "Introduction to Transcribing",
    "section": "",
    "text": "In order to maintain the integrity of your original document while you are translating it for the computer, consider a few things while you’re transcribing.\n\nStay as true to the document as you can!\n\n\nInclude all text as it is written. This includes errors in spelling and grammar, and words you might not be familiar with. Transcribing is NOT editing!\nIf there are images in the text, describe them in brackets (e.g. “[doodle of a curly-haired smiling face]”).\nKeep it simple! You want people to be able to read the transcription without looking at the original letter. Your transcriptions will be plain text files (e.g. files with no formatting), so, without editorializing, keep the text readable as possible.\n\n\nIf you have trouble reading the text…\n\n\nTake a break and come back. Fresh eyes make a difference!\nTry to read the words in context. The word might look like “fhe,” but it doesn’t make sense to read “And then fhe called her sister.” You would realize the word is “she.”\nAsk for help. Send a screenshot to your classmates, or post it in the Teams channel.\nIf you’ve tried all these things, and it’s just not possible to figure out, write [illegible] in brackets and tag that text with the “gap” tag (more on tags in section IV!).\n\n\nTake notes while you are transcribing\n\n\nMake note of things you don’t understand, and Google them to see if you can make sense of the reference.\nYou might find something interesting that you want to come back to! Take notes of the letters you transcribe, and what you find interesting about them.\n\n\n\nPractice transcribing a few lines, staying true to the original text with the 0.Transcribing Check-in."
  },
  {
    "objectID": "Modules/transcribing.html#text-regions-in-transkribus",
    "href": "Modules/transcribing.html#text-regions-in-transkribus",
    "title": "Introduction to Transcribing",
    "section": "",
    "text": "Download Transkribus before starting this section\nBefore we get started actually transcribing, we have to tell Transkribus what parts of the page have text on them, and what order to read that text! (Remember, computers aren’t smart enough to read like humans!)\n\n\nThe page (or “text region”), where text is: \nThe line of text, as in the full, left-to-write line: \nAnd the baseline, which underlines the words themselves: \n\n\n\nTranskribus does a pretty good job of identifying these text regions for us. The quickest way to do this is to run a layout analysis on the document, then correct any mistakes that Transkribus makes. Here’s how you run the analysis:\n\nDouble click the document you’ve uploaded, and click on the Tools tab.\nUnder Layout Analysis, make sure you’ve selected all pages.\nClick Run\n\n\n\n\nscreenshot of layout analysis\n\n\n\nA notification that the layout analysis is in progress will pop up. After a few moments (between 3 and 10 seconds), the layout (green squares, blue rectangles, and dark blue lines) will appear over the letter’s text.\n\n\n\n\n\nCorrect errors in the layout (e.g. delete text regions or lines that don’t cover text, resize regions that cut off text, reorder line numbers, or merge lines that have been split).\n\n\nText regions that don’t actually contain any transcribe-able text. \nThis layout analysis split a line in two, and needs merging. (Sometimes, though less frequently, it will merge something that should be two lines. For this, use the scissor tools above the merge tool.) Holding control, click each region that needs joining (usually the baseline and the line) and then click merge. \nSometimes the software will number things in a different order than we’d actually read them. Click on the eye in the top toolbar, and select show lines reading order. If the lines are out of order, click on the numbers to change them. \n\n\n\n\nComplete 2.Transcribing Check-in before moving on to the next section."
  },
  {
    "objectID": "Modules/transcribing.html#handwritten-text-recognition",
    "href": "Modules/transcribing.html#handwritten-text-recognition",
    "title": "Introduction to Transcribing",
    "section": "",
    "text": "Transkribus allows you to create Handwritten Text Recognition (HTR) models. Over the past 2 years that we’ve worked on this project, we have transcribed over 100 letters and trained the program to begin to recognize Pauline Smith’s handwriting. This allows us to run the model on letters in the collection and start with a more complete transcription to correct, rather than having to transcribe the letter from scratch.\n\nTo transcribe with the HTR model, click on the Tools tab, and under Text Recognition, select the model called “Pauline Smith 2.0.” You might have to navigate to the next page to find the Pauline Smith model.\n\n\n\n\nscreenshot of the Text recognition tool\n\n\n\n\n\nscreenshot of Pauline Smith’s HTR model\n\n\n\nClick Run. The HTR analysis may take anywhere from 20 seconds to 2 minutes. A notification that the “job” is done will pop up when analysis is complete. From there, correct the transcription.\nTranscribe each page as you see it, following Smithsonian and Transkribus transcription guidelines. You may have a few errors on a page or there may be significant errors. It depends on several things - the quality of the scan, the quality of the document, the handwriting and what utensil they used, etc.!\n\n\n\n\nscreenshot of HTR errors\n\n\n\nAfter you’ve completed editing the transcription, go back over the letter, making sure you didn’t miss typos, and you can simultaneously begin tagging the document, which will be explained in the next lesson!\n\n\n\nComplete the Transcribing Check-in before moving on to the next section."
  },
  {
    "objectID": "Modules/transcribing.html#metadata-tags",
    "href": "Modules/transcribing.html#metadata-tags",
    "title": "Introduction to Transcribing",
    "section": "",
    "text": "Tagging, annotating, or marking-up a document is a common practice for text analysis. Why is this? Think about the things someone might talk to you about in a letter. They might mention people they talked about, places they went, or a book they read. Now think about the contents of 6,000 letters, and all of this information together! Tagging will allow us to use the information in the collection of thousands of letters to track these trends over time, explore topics, and find people.\nThink of tagging the letter like highlighting categories of information with a different color highlighter. All of the people’s names are blue, the places are purple, etc. Transkribus does the same thing.\n\n\nIt is tempting to tag everything, but not necessary! You only need to tag: 1. Important words (e.g. people and places) 2. When you add something for clarity that isn’t there originally (e.g. “[illegible]”). 3. If you want to provide further information as context.\n\n\n\nHere are the tags we have, to this point, decided to use for the Smith Collection, and their definitions. (For the info below in a Google Doc, click here. And for more info about how Transkribus feels about tags, click here.)\n\nperson - when any person’s name is mentioned within the body of the letter (e.g. “Daddy,” Martha, Gilbert, etc.), or when a person is referred to (e.g. “Martha’s friend boy,” or “Jewish soldier”).\nplace - when a place name is mentioned within the body of the letter. This does not usually include general places like Sunday school or hospital. Examples would be town names like Bruce or Memphis. sender - The person who sent the letter, either in the return address, or in the letter’s closing.\nrecipient - The person the letter is addressed to, either on the envelope, or in the salutation.\nsender - The person who wrote the letter. They usually put their name in the return address and in the signature.\naddress - The address the letter was sent to.\nreturn-address - the address the letter came from (not always included)\norganization - a specific group, corporation, or entity (e.g. the Senate, the T.V.A., or Ole Miss), not a generic place like hospital or school.\nsic - for an error in the original letter (i.e. spelling, grammar, etc.) that you include it in the transcript for accuracy, tag it with “sic,” showing that it’s “as is,” and you didn’t make the error in transcribing.\ntitle - the title of a book, play, radio show, government decision, etc. Anything that is published, for example. Specifications may include full title, author, and publication date.\ndate - a specific date (e.g. January 15th, 1948). This does not include general references to days of the week or years (e.g. “Saturday nite” or “next year.”)\nunclear - when you write something in the transcription, but aren’t sure of what it actually says in the letter: a guess.\ngap (formerly supplied) - when you include something (i.e. [illegible], a note, etc.) that isn’t in the original letter to give clarity, but to prevent the HTR from picking up the note.\n\n\n\n\n\nTo tag something, highlight the text you want to tag, right-click, and from “All tags,” select the relevant tag.\n\n\n\n\nScreenshot of how to tag something\n\n\n\nIf the tag isn’t in the list, go to the metadata menu, then the Textual menu, and click Customize.\n\n\n\n\nScreenshot of customize tag\n\n\n\nClick “Create new tag,” then type in the name of the tag, lowercase, like it’s shown in the Tag definitions document.\n\n\n\n\nScreenshot of creating a new tag\n\n\n\n\n\nTags are sort of tricky! Let’s go over the different scenarios in the next 4.Transcribing Check-in. Complete this before moving on to the next section.\nAs a reminder, there is a video of transcribing a letter from beginning to end, which you can watch as a refresher: https://youtu.be/-cDD9P0rnLw.\nAt the end of this lesson, you should be able to fill out the full transcription and tags in your letters, and turn them in in Canvas."
  },
  {
    "objectID": "Modules/transcribing.html#exporting-files-in-transkribus",
    "href": "Modules/transcribing.html#exporting-files-in-transkribus",
    "title": "Introduction to Transcribing",
    "section": "",
    "text": "After you have transcribed (or corrected a transcription of) a letter, tagged it, and proofed it, it has been digitally transformed, and we can now do fun things with the transcriptions and metadata. But before you can do that, you have to export the files.\nBefore the step-by-step, I recommend watching this 3-minute video on this process.\nClick the export button (a manila folder with a green, right-facing arrow), and you will choose the following files to export: * .docx - to give you a formatted version of the transcript * .txt - to give a non-formatted version of the transcript * .xlsx (tag export) - to give you structured lists of the tags you created\nIf you would like to export more than one document, select the radio button that says “Current Collection,” and choose which letters to export.\n\n\n\nScreenshot of Export Document option\n\n\n\n\nWhen you click OK, Transkribus emails you a temporary link where you can access the compressed files you just created. Compressing them allows them to send large amounts of data over email. When you click the link, it will ask you to save the compressed files (.zip).\n\n\n\nScreenshot of saving compressed folder\n\n\nSaving them as compressed files stores them to your computer, but does not let you access them until you extract them. Right-click your compressed folder and click Extract All... then tell your computer where you want to extract them to. I recommend saving them to a folder where you’re keeping files for this class. (We will also store them on a cloud server, e.g. Google Drive or our Institutional Repository at some point.)\n\n\n\nScreenshot of extracting files to a destination\n\n\nAfter you extract the files, you should notice that your folder no longer has a zipper on it, and you have 3 files that represent digital versions of the letter you prepared. You are now ready to start playing with these digital artifacts!"
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Explore the course roadmap.\nWatch the Introduction to Digital Studies recorded lecture\n\nSlides for this lecture are available here.\n\nExplore example digital scholarship projects:\n\nLetters of Maynooth - 1916\nThe Digital Welty Lab\nGoin’ North: Stories of the Great Migration to Philadelphia\nColored Conventions Project\nSalem Witch Trials Documentary Archive and Transcription Project -Lincoln Logarithms: Finding Meaning in Sermons\n\n\n\n\n\n\n\nAnswer the Roadmap Questionnaire\nComplete the “Evaluating Digital Scholarship” check-in on Canvas\n\n\n\n\n\n\n\n\nDrucker, Johanna. “Ontologies and Metadata Standards,” Introduction to Digital Humanities 2014, pp 24-27. (Available for LIB 201 students in Canvas).\nSchöch, Christof, 2013. “Big? Smart? Clean? Messy? Data in the Humanities,” The Dragonfly’s Gaze (revised and published in Journal of Digital Humanities 2(3).) https://dragonfly.hypotheses.org/443\nHanna, Laurel, 2021. “Pitts Staff Works to Counteract Racism in Library Catalog.” Candler News and Events, Emory, Candler School of Theology, https://candler.emory.edu/news/releases/2021/01/pitts-staff-works-to-counteract-racism-in-library-catalog.html\nKetchley, Sarah. “Planning & Managing Digital Projects,” DH in Practice Coursebook. pp 46-55.(Available for LIB 201 students in Canvas).\n\n\n\n\n\nErin Baucom, 2018. “An exploration into archival descriptions of LGBTQ Materials. The American Archivist, 81(1): 65-83. doi: https://doi.org/10.17723/0360-9081-81.1.65\n\n\n\n\n\nComplete Metadata module and the check-ins in each section\n\n\n\n\n\n\n\n\nWoodford, Chris. 2021. “Optical Character Recognition,” Explain that Stuff, https://www.explainthatstuff.com/how-ocr-works.html.\n“About the Smith Papers Collection.” This document gives context for the letters and describes the main contributors. Read this before you begin transcribing.\nTranscribing from Start to Finish\nTranskribus Transcribing Conventions, 2021. Read Co-op, https://readcoop.eu/transkribus/howto/transkribus-transcription-conventions/.\nSmithsonian, “General Instructions for Transcription and Review,” https://transcription.si.edu/instructions\nYouTube video of Hillary introducing Transkribus: https://youtu.be/szGPn0K_WWE\nYouTube video of Hillary transcribing a letter from beginning to end: https://youtu.be/-cDD9P0rnLw\nInterview with Bridget and Steve Pieschel about the contents of the Smith Papers letters: https://youtu.be/zPynkw9V-Q8\n\n\n\n\n\nTranskribus: https://readcoop.eu/transkribus/?sc=Transkribus (Installation Instructions here. Be sure you have Java 8 installed before installing Transkribus. This is in the instructions, but not in the first step. If you run into an issue, email your instructor!)\n\n\n\n\n\nResearch Discussion 1 - Research Topics in the Smith Papers\nResearch Discussion 2 - Research topics, continued\nData Prep Work Assignment:\n\nMetadata\nTranscriptions\nTags\n\n\n\n\n\n\n\nWhat is a network analysis, and how does it help me answer questions about a group of people?\nStructuring Data\nSpreadsheets!\n\n\n\n\nRawson and Muñoz, “Against Cleaning.”, 2016. Curating Menus\nScott Weingart, 2011. “Demystifying Networks,” Journal of Digital Humanities 1(1).\n\nOptional readings for more help with NA:\n\nThomas Padilla and Brandon Locke. “Introduction to Network Analysis.”\nMiriam Posner, “Network Analysis.”\nScott Weingart, 2015, “Networks Demystified 9: Bimodal networks.”\nPalladio Tutorial and FAQs.\nMiriam Posner, “Creating a Network Graph with Gephi.” \n\n\n\n\n\nNo installations necessary for this module! We will use Google Sheets and Palladio\n\n\n\n\nCleaned data tags\nNetwork Analysis Visualization\n1-2 page Write-up\nExtra credit: “Fun with Spreadsheets”\n\n\n\n\n\n\nWhat does a map or a timeline help me answer?\nWhat kind of data does a map or a timeline use?\nCollaborating on a class project\n\n\n\n\nWhite, Phil. (2014). “Story map blog” https://blogs.library.duke.edu/data/2014/10/28/story-maps/\nManuel Gimond, 2021, “Introduction to GIS,” Intro to GIS and Spatial Analysis. https://mgimond.github.io/Spatial/introGIS.html\nExplore mapping and timeline projects:\n\n“Torn Apart/Separados,” http://xpmethod.columbia.edu/torn-apart/volume/2/index\n“Native Land” https://native-land.ca/\nMaps of the Starkville Civil Rights Project: https://starkvillecivilrights.msstate.edu/wordpress/the-place/\n“How Wine Colonized the World,” TimelineJS by VinePair, Inc.\n\n\n\n\n\n\nLisa Charlotte Rost, 2021, “Which color scale to use when visualizing data” Datawrapper, https://blog.datawrapper.de/which-color-scale-to-use-in-data-vis/\nJ. B. Harley, 1989, “Deconstructing the Map,” Cartographica, 26(2), http://hdl.handle.net/2027/spo.4761530.0003.008\nShannon Mattern, 2015, “Gaps in the Map: Why we’re mapping everything, and why not everything can, or should, be mapped.” https://wordsinspace.net/2015/09/18/gaps-in-the-map-why-were-mapping-everything-and-why-not-everything-can-or-should-be-mapped/\nOlivia Ildefonso, 2021, “Top Mapping Mistakes,” CUNY Academic Commons, https://digitalfellows.commons.gc.cuny.edu/2021/05/12/top-mapping-mistakes/\n\n\n\n\n\nCreate a free, public account with ArcGIS. Directions and links for doing so are here.\nSpreadsheet template for Timeline JS (by Knightlab). Step by step instructions are here, and a video introduction is here.\n\n\n\n\n\nResearch Discussion 3 - Sharing Timeline Sources\nCollaborative Notes\nMap OR Timeline mini-project\n\n\n\n\n\n\nWhat are distant and close reading?\nHow do different distant reading methods help me make sense of what I’m reading?\n\n\n\n\nCaulfield, Jack. “A quick guide to textual analysis,” Scribbr.com, 2020.\nSculley and Pasanek, “Meaning and mining: the impact of implicit assumptions in data mining for the humanities” Literary and Linguistic Computing, 23(4), 2008. This link will take you to MUW Library databases. Sign in with your 950# if you are off campus!\n\n\n\n\n\nCorpus Thomisticum\nThe Willa Cather Archive\nPowerhouse Annotated: A digital edition\nTopic Modeling Martha Ballard’s Diary\n\n\n\n\nThere are no installations for this module. We will use tools that are available through MUW Library and openly online:\nRequired Tools - Gale Digital Scholar Lab (No download required)\nOptional tools - Lexos - web-based text-cleaning app - Voyant - web-based text analysis app - jsLDA: In-browser Topic Modeling\n- dataBASIC - various web-based text analysis tools for different purposes\n\n\n\n\nContent Set Brainstorm\nRough Draft of text analysis mini-project\nText analysis mini-project\n\n\n\n\n\n\n\n\nBrannock, Jennifer. “Creating an Exhibit in Special Collections and Using it to Promote Collections and Educate Users,” Mississippi Libraries 73(2), 2009. pp. 32-34.\nOvadia, Steven. (2014). “Markdown for Librarians and Academics,” Behavioral and Social Sciences Librarian 33, pp. 120-124.\n“Markdown Syntax” - A cheatsheet for markdown syntax\n\n\n\n\n\nText Editor software (comes installed on most computers). Suggestions for text editors:\n\nVisual Studio Code is the Microsoft Office equivalent of Notepad or TextEdit. VS Code is more user friendly, and works within your browser (by typing the period . in a github markdown page). For further direction see VS Code’s Installation instructions for MAC or PC users.\nPC users: Notepad comes already installed\nMac users: TextEdit comes already installed\nIf you are using an already-installed text editor, preview your markdown using something like https://dillinger.io/ or https://markdownlivepreview.com/\n\n\n\n\n\n\nDraft outline of Digital Exhibit\nCollaborative Notes Document\nFinal narrative portfolio\n\n\n\n\n\nIn lieu of a final exam, you will turn in a Reflection Essay, due at the time of the university-scheduled exam time."
  },
  {
    "objectID": "schedule.html#week-1---introduction-to-the-course-and-digital-scholarship",
    "href": "schedule.html#week-1---introduction-to-the-course-and-digital-scholarship",
    "title": "Schedule",
    "section": "",
    "text": "Explore the course roadmap.\nWatch the Introduction to Digital Studies recorded lecture\n\nSlides for this lecture are available here.\n\nExplore example digital scholarship projects:\n\nLetters of Maynooth - 1916\nThe Digital Welty Lab\nGoin’ North: Stories of the Great Migration to Philadelphia\nColored Conventions Project\nSalem Witch Trials Documentary Archive and Transcription Project -Lincoln Logarithms: Finding Meaning in Sermons\n\n\n\n\n\n\n\nAnswer the Roadmap Questionnaire\nComplete the “Evaluating Digital Scholarship” check-in on Canvas"
  },
  {
    "objectID": "schedule.html#module-1-metadata-and-project-management",
    "href": "schedule.html#module-1-metadata-and-project-management",
    "title": "Schedule",
    "section": "",
    "text": "Drucker, Johanna. “Ontologies and Metadata Standards,” Introduction to Digital Humanities 2014, pp 24-27. (Available for LIB 201 students in Canvas).\nSchöch, Christof, 2013. “Big? Smart? Clean? Messy? Data in the Humanities,” The Dragonfly’s Gaze (revised and published in Journal of Digital Humanities 2(3).) https://dragonfly.hypotheses.org/443\nHanna, Laurel, 2021. “Pitts Staff Works to Counteract Racism in Library Catalog.” Candler News and Events, Emory, Candler School of Theology, https://candler.emory.edu/news/releases/2021/01/pitts-staff-works-to-counteract-racism-in-library-catalog.html\nKetchley, Sarah. “Planning & Managing Digital Projects,” DH in Practice Coursebook. pp 46-55.(Available for LIB 201 students in Canvas).\n\n\n\n\n\nErin Baucom, 2018. “An exploration into archival descriptions of LGBTQ Materials. The American Archivist, 81(1): 65-83. doi: https://doi.org/10.17723/0360-9081-81.1.65\n\n\n\n\n\nComplete Metadata module and the check-ins in each section"
  },
  {
    "objectID": "schedule.html#module-2-transcribing",
    "href": "schedule.html#module-2-transcribing",
    "title": "Schedule",
    "section": "",
    "text": "Woodford, Chris. 2021. “Optical Character Recognition,” Explain that Stuff, https://www.explainthatstuff.com/how-ocr-works.html.\n“About the Smith Papers Collection.” This document gives context for the letters and describes the main contributors. Read this before you begin transcribing.\nTranscribing from Start to Finish\nTranskribus Transcribing Conventions, 2021. Read Co-op, https://readcoop.eu/transkribus/howto/transkribus-transcription-conventions/.\nSmithsonian, “General Instructions for Transcription and Review,” https://transcription.si.edu/instructions\nYouTube video of Hillary introducing Transkribus: https://youtu.be/szGPn0K_WWE\nYouTube video of Hillary transcribing a letter from beginning to end: https://youtu.be/-cDD9P0rnLw\nInterview with Bridget and Steve Pieschel about the contents of the Smith Papers letters: https://youtu.be/zPynkw9V-Q8\n\n\n\n\n\nTranskribus: https://readcoop.eu/transkribus/?sc=Transkribus (Installation Instructions here. Be sure you have Java 8 installed before installing Transkribus. This is in the instructions, but not in the first step. If you run into an issue, email your instructor!)\n\n\n\n\n\nResearch Discussion 1 - Research Topics in the Smith Papers\nResearch Discussion 2 - Research topics, continued\nData Prep Work Assignment:\n\nMetadata\nTranscriptions\nTags"
  },
  {
    "objectID": "schedule.html#module-3-network-analysis",
    "href": "schedule.html#module-3-network-analysis",
    "title": "Schedule",
    "section": "",
    "text": "What is a network analysis, and how does it help me answer questions about a group of people?\nStructuring Data\nSpreadsheets!\n\n\n\n\nRawson and Muñoz, “Against Cleaning.”, 2016. Curating Menus\nScott Weingart, 2011. “Demystifying Networks,” Journal of Digital Humanities 1(1).\n\nOptional readings for more help with NA:\n\nThomas Padilla and Brandon Locke. “Introduction to Network Analysis.”\nMiriam Posner, “Network Analysis.”\nScott Weingart, 2015, “Networks Demystified 9: Bimodal networks.”\nPalladio Tutorial and FAQs.\nMiriam Posner, “Creating a Network Graph with Gephi.” \n\n\n\n\n\nNo installations necessary for this module! We will use Google Sheets and Palladio\n\n\n\n\nCleaned data tags\nNetwork Analysis Visualization\n1-2 page Write-up\nExtra credit: “Fun with Spreadsheets”"
  },
  {
    "objectID": "schedule.html#module-4-maps-and-timelines",
    "href": "schedule.html#module-4-maps-and-timelines",
    "title": "Schedule",
    "section": "",
    "text": "What does a map or a timeline help me answer?\nWhat kind of data does a map or a timeline use?\nCollaborating on a class project\n\n\n\n\nWhite, Phil. (2014). “Story map blog” https://blogs.library.duke.edu/data/2014/10/28/story-maps/\nManuel Gimond, 2021, “Introduction to GIS,” Intro to GIS and Spatial Analysis. https://mgimond.github.io/Spatial/introGIS.html\nExplore mapping and timeline projects:\n\n“Torn Apart/Separados,” http://xpmethod.columbia.edu/torn-apart/volume/2/index\n“Native Land” https://native-land.ca/\nMaps of the Starkville Civil Rights Project: https://starkvillecivilrights.msstate.edu/wordpress/the-place/\n“How Wine Colonized the World,” TimelineJS by VinePair, Inc.\n\n\n\n\n\n\nLisa Charlotte Rost, 2021, “Which color scale to use when visualizing data” Datawrapper, https://blog.datawrapper.de/which-color-scale-to-use-in-data-vis/\nJ. B. Harley, 1989, “Deconstructing the Map,” Cartographica, 26(2), http://hdl.handle.net/2027/spo.4761530.0003.008\nShannon Mattern, 2015, “Gaps in the Map: Why we’re mapping everything, and why not everything can, or should, be mapped.” https://wordsinspace.net/2015/09/18/gaps-in-the-map-why-were-mapping-everything-and-why-not-everything-can-or-should-be-mapped/\nOlivia Ildefonso, 2021, “Top Mapping Mistakes,” CUNY Academic Commons, https://digitalfellows.commons.gc.cuny.edu/2021/05/12/top-mapping-mistakes/\n\n\n\n\n\nCreate a free, public account with ArcGIS. Directions and links for doing so are here.\nSpreadsheet template for Timeline JS (by Knightlab). Step by step instructions are here, and a video introduction is here.\n\n\n\n\n\nResearch Discussion 3 - Sharing Timeline Sources\nCollaborative Notes\nMap OR Timeline mini-project"
  },
  {
    "objectID": "schedule.html#module-5-text-analysis",
    "href": "schedule.html#module-5-text-analysis",
    "title": "Schedule",
    "section": "",
    "text": "What are distant and close reading?\nHow do different distant reading methods help me make sense of what I’m reading?\n\n\n\n\nCaulfield, Jack. “A quick guide to textual analysis,” Scribbr.com, 2020.\nSculley and Pasanek, “Meaning and mining: the impact of implicit assumptions in data mining for the humanities” Literary and Linguistic Computing, 23(4), 2008. This link will take you to MUW Library databases. Sign in with your 950# if you are off campus!\n\n\n\n\n\nCorpus Thomisticum\nThe Willa Cather Archive\nPowerhouse Annotated: A digital edition\nTopic Modeling Martha Ballard’s Diary\n\n\n\n\nThere are no installations for this module. We will use tools that are available through MUW Library and openly online:\nRequired Tools - Gale Digital Scholar Lab (No download required)\nOptional tools - Lexos - web-based text-cleaning app - Voyant - web-based text analysis app - jsLDA: In-browser Topic Modeling\n- dataBASIC - various web-based text analysis tools for different purposes\n\n\n\n\nContent Set Brainstorm\nRough Draft of text analysis mini-project\nText analysis mini-project"
  },
  {
    "objectID": "schedule.html#module-6-digital-exhibits",
    "href": "schedule.html#module-6-digital-exhibits",
    "title": "Schedule",
    "section": "",
    "text": "Brannock, Jennifer. “Creating an Exhibit in Special Collections and Using it to Promote Collections and Educate Users,” Mississippi Libraries 73(2), 2009. pp. 32-34.\nOvadia, Steven. (2014). “Markdown for Librarians and Academics,” Behavioral and Social Sciences Librarian 33, pp. 120-124.\n“Markdown Syntax” - A cheatsheet for markdown syntax\n\n\n\n\n\nText Editor software (comes installed on most computers). Suggestions for text editors:\n\nVisual Studio Code is the Microsoft Office equivalent of Notepad or TextEdit. VS Code is more user friendly, and works within your browser (by typing the period . in a github markdown page). For further direction see VS Code’s Installation instructions for MAC or PC users.\nPC users: Notepad comes already installed\nMac users: TextEdit comes already installed\nIf you are using an already-installed text editor, preview your markdown using something like https://dillinger.io/ or https://markdownlivepreview.com/\n\n\n\n\n\n\nDraft outline of Digital Exhibit\nCollaborative Notes Document\nFinal narrative portfolio"
  },
  {
    "objectID": "schedule.html#final-exam",
    "href": "schedule.html#final-exam",
    "title": "Schedule",
    "section": "",
    "text": "In lieu of a final exam, you will turn in a Reflection Essay, due at the time of the university-scheduled exam time."
  },
  {
    "objectID": "modules.html",
    "href": "modules.html",
    "title": "Modules",
    "section": "",
    "text": "Modules\nWe will work our way through 6 modules in this class. Each module will include readings, weekly writings and check-ins, and will conclude with a mini-project assignment.\nSee the full schedule for a list of readings and links to turn in assignments. Below are the links to each full module.\nModule 1: Metadata\nModule 2: Transcribing\nModule 3: Network Analysis\nModule 4: Spatial and Temporal Data\nModule 5: Text Analysis\nModule 6: Narrative"
  }
]